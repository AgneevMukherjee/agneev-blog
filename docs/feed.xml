<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://agneevmukherjee.github.io/agneev-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://agneevmukherjee.github.io/agneev-blog/" rel="alternate" type="text/html" /><updated>2022-07-04T23:21:14+02:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/feed.xml</id><title type="html">Agneev’s DS/ML lab book</title><subtitle>I mostly write about my experiments in data science/machine learning here,
 sometimes about other things...</subtitle><entry><title type="html">11. ACMC - Class weighting and MCC</title><link href="https://agneevmukherjee.github.io/agneev-blog/ACMC-class-wt/" rel="alternate" type="text/html" title="11. ACMC - Class weighting and MCC" /><published>2022-07-02T13:00:00+02:00</published><updated>2022-07-02T13:00:00+02:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/ACMC-class-wt</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/ACMC-class-wt/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/ACMC-undersampling/">&#8672;&nbsp;10. ACMC - Undersampling and F-scores</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Metric">Metric</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
TensorFlow 2, Keras, Matplotlib, Seaborn, Pandas, Scikit-learn, Matthews correlation coefficient
</p>
<p><br /></p>

<p style="text-align: justify">Hello! <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/">In the previous post</a>, we looked at the pros and cons of using undersampling for dealing with the imbalance inherent in the <a href="https://www.kaggle.com/datasets/agneev/archie-comics-multi-class">Archie Comics Multiclass (ACMC) dataset</a>. The next thing to explore, logically, would be the opposite tactic - oversampling to increase the number of samples in the minority classes. That’s <em>not</em> what we are going to do today. As I mentioned last time, things are a bit busy with me at the moment, and as oversampling deserves a fairly detailed look, I will defer that till next time. Instead, we are going to look at another important method of dealing with class imbalance - class weighting.</p>

<h2 id="class-weights-">Class weights <a id="wts"></a></h2>

<p style="text-align: justify">Class weighting as a means of handling class imbalance is as simple as assigning a higher weight to the minority classes, so that the classifier places a greater emphasis on these classes while training - that’s it. This makes intuitive sense - if we want our model to do as well on a class having 50 samples as on one having 500, then the classifier better pay a lot more attention to the minority class, right?</p>

<p style="text-align: justify">More technically, during model training, the loss is calculated at the end of training each batch, and the model parameters are then updated in an attempt to reduce this loss. Without class weights, every sample in a batch contributes equally to the loss. On the other hand, if class weights are provided, then the contribution of a particular sample to the loss becomes proportional to the class weight. Thus, if a minor class has a weight ten times higher than a majority class, then every minor class sample will contribute ten times more to the loss than a majority class sample. This can make the model a little slower to train, as the training loss declines more slowly than in an unweighted model, but the end product is more even-handed in classifying unbalanced classes.</p>

<p style="text-align: justify">Let us jump right into the code. As always, the full code is <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-imagenet-class-weights-20.ipynb">available on Github</a>, and we will look at the important bits here. By now, the procedure for creating and running the model must be quite familiar, and so I will not go over that. The only difference is the insertion of class weights:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="n">class_weights</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="n">class_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">max_samples</span><span class="o">/</span><span class="n">samples_per_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">class_weights</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span> <span class="p">(</span> <span class="n">key</span><span class="p">,</span> <span class="s">' : '</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="c1"># Based on https://stackoverflow.com/questions/66501676/how-to-set-class-weights-by-dictionary-for-imbalanced-classes-in-keras-or-tensor</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p style="text-align: justify">Step-by-step, the procedure is like this: first, we create an empty dictionary called class_weights (Line 1). We then run a for-loop (Lines 2 and 3) to assign the weights to each class. In Line 3, max_samples refers to a pre-calculated value, the maximum number of samples in any one class. In our case, it happens to be 1284, which is the number of samples in the ‘Archie’ class. This value is divided by the number of samples in each class to obtain the class weight for that class. As an example, the ‘Kleats’ class has 41 samples, and so its class weight is 1284 / 41 = 31.3. The ‘Jughead’ class has a far greater number of samples, and so its weight is 1284 / 962 = 1.3.</p>

<p style="text-align: justify">That’s essentially all that we need to do to get the class weights. If we want to check if we have assigned the weights correctly, we can print out the dictionary (Lines 4 and 5) to get:</p>

<p><img src="/agneev-blog/assets/img/img_11_1.png?raw=true" alt="Image_1" width="200&quot;, height=&quot;100" /></p>

<p style="text-align: justify">The ‘key’ numbers have been assigned sequentially in the for-loop - note the ‘class_weights[i]’ in the code above. We can check which number corresponds to which class via the ‘inv_index_classes_dict’ created earlier in the notebook, which has for its content:</p>

<p><img src="/agneev-blog/assets/img/img_11_2.png?raw=true" alt="Image_2" width="150&quot;, height=&quot;100" /></p>

<p style="text-align: justify"><br />
The only thing that now needs to be done is to pass the class_weights dictionary as an argument while fitting the model:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">train</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="n">valid</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stopping</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">],</span>
        <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span>
<span class="p">)</span></code></pre></figure>

<p style="text-align: justify">Et voilà! We obtain a validation accuracy of around 48%, which is lower than that obtained without class weights, around 56%. Something more interesting is the change in the shape of the training curves:</p>

<p><img src="/agneev-blog/assets/img/img_11_3.png?raw=true" alt="Image_3" width="750&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Unweighted model curves on top, weighted bottom
</p>

<p style="text-align: justify"><br />
Looking at the model loss curves, we see that for the unweighted model (top), the training loss starts at around 2.75 and drops to around 0.75 after 20 epochs. The weighted training loss (bottom), however, starts at around 15 and remains around 4 at the end of the run. This reflects in a difference in the accuracy curves as well, with a training accuracy of around 78% obtained for the original model after 20 epochs and only 56% for the weighted model. The validation loss, meanwhile, has a similar pattern for both the models, and so the drop in validation accuracy is less pronounced. All this adds up to a far lower degree of overfitting in the weighted model as compared to the original. In other words, class weighting here is also acting as a regularisation technique!</p>

<p style="text-align: justify">As we are not overfitting after 20 epochs and the validation accuracy appears to have room to rise further, I also created a notebook <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-imagenet-class-weights-50.ipynb">looking at a run of 50 epochs</a>. We will see the results at the end of this post.</p>
<p style="text-align: justify">.</p>
<p style="text-align: justify">.</p>
<p style="text-align: justify">So our original intention was to produce a model which would be roughly equally accurate on the majority and minority classes. How does class weighting fare in that respect? We will come to that, but first a detour.</p>

<p style="text-align: justify">Last time around, I had <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/#f-score">described F-scores</a> and how they can be used to quantitatively compare classification models. I may, however, have mistakenly given the impression that F-scores are the only way of doing this, or that they are infallible. Neither is true, and there are in fact plenty of statistical measures that may be used depending on the circumstances. Perhaps one day I will write a blog post going into several of these, but today I will focus on one that is relevant to balanced classification on the ACMC - the Matthews correlation coefficient (MCC).</p>

<h2 id="matthews-correlation-coefficient-">Matthews correlation coefficient <a id="mcc"></a></h2>

<p style="text-align: justify">The Matthews correlation coefficient (MCC), more generally known as the <a href="https://en.wikipedia.org/wiki/Phi_coefficient">phi coefficient</a>, is calculated for binary classification as:</p>

<p><img src="/agneev-blog/assets/img/img_11_4.png?raw=true" alt="Image_4" width="350&quot;, height=&quot;200" /></p>

<p style="text-align: justify">While the  F-score ranges from 0 (completely useless model) to 1 (perfection), the MCC ranges from -1 to 1, with a higher score better, and 0 indicating a random model. We can see that, like the F-score, its calculation includes the true positives (TP), false positives (FP) and false negatives (FN). However, unlike the F-score, we also take into account the true negatives (TN). Therefore, a high MCC score is only obtained if a model does well on all four confusion matrix categories. There has been <a href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7">some</a>… <a href="https://biodatamining.biomedcentral.com/articles/10.1186/s13040-021-00244-z">work</a>… <a href="https://clevertap.com/blog/the-best-metric-to-measure-accuracy-of-classification-models/">published</a>… suggesting that the MCC is more reliable and robust than the F-score and other metrics for binary classification, especially for unbalanced datasets, while others <a href="https://www.sciencedirect.com/science/article/abs/pii/S016786552030115X">contest this</a>. I am not competent enough to arbitrate on this, but at the very least the MCC should be a significant addition to our toolbox, either in addition to or in place of the F-score.</p>

<p style="text-align: justify">While the MCC can be adapted to multiclass classification via micro- and macro-averaging like the F<sub>1</sub> score, it also has a generalised equation that is given <a href="https://en.wikipedia.org/wiki/Phi_coefficient#Multiclass_case">in Wikipedia</a> (as well as articles like <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882">this</a>). The formula is rather indimidating, and while it is certainly possible to code it from scratch, a better option may be to use Scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html">in-built method</a> that can be used for either binary or multiclass classification. The code for this is simplicity itself - we simply import <em>matthews_corrcoef</em> from <em>sklearn.metrics</em> and then pass it the true labels and the predictions:</p>

<p><img src="/agneev-blog/assets/img/img_11_5.png?raw=true" alt="Image_5" width="500&quot;, height=&quot;300" /></p>

<h2 id="results-">Results <a id="result"></a></h2>

<p style="text-align: justify">All right then, let us have a look at the results. The table below compares 3 approaches - using the whole dataset, undersampling, and class weighting - on 7 different criteria. You may want to refresh your memory on the details of the <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/#min_sample">undersampling-min</a> and <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/#arb_thresh">undersampling-200</a> approaches, otherwise let’s go!</p>

<p style="text-align: justify">Before we discuss the results, though, a couple of notes: first, unlike in the previous post, the undersampling-200 approach code looked at here does not use training set augmentations, so as to enable a fairer comparison with the other approaches, none of which use augmentations either. Second, I have bolded the best results for each criterion in the table, but given the stochastic nature of the models, close results may often be flipped, and that is why I have also highlighted values that are close to the best in the table.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Approach</th>
      <th style="text-align: center">Epochs</th>
      <th style="text-align: center">RT (s)<sup>a</sup></th>
      <th style="text-align: center">Train   acc. (%)</th>
      <th style="text-align: center">Val. acc. (%)</th>
      <th style="text-align: center">Macro-F<sub>1</sub></th>
      <th style="text-align: center">Micro-F<sub>1</sub></th>
      <th style="text-align: center">MCC</th>
      <th style="text-align: left">Code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Whole dataset</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">614</td>
      <td style="text-align: center">78</td>
      <td style="text-align: center">57</td>
      <td style="text-align: center">0.37</td>
      <td style="text-align: center">0.57</td>
      <td style="text-align: center">0.51</td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-simple-imagenet-f-mcc.ipynb">1</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Whole dataset</td>
      <td style="text-align: center">50</td>
      <td style="text-align: center">1535</td>
      <td style="text-align: center"><strong>96</strong></td>
      <td style="text-align: center"><strong>60</strong></td>
      <td style="text-align: center"><strong>0.45</strong></td>
      <td style="text-align: center"><strong>0.6</strong></td>
      <td style="text-align: center"><strong>0.55</strong></td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-simple-imagenet-50-f-mcc.ipynb">2</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Undersampling-min</td>
      <td style="text-align: center">50</td>
      <td style="text-align: center"><strong>218</strong></td>
      <td style="text-align: center"><strong>98</strong></td>
      <td style="text-align: center">31</td>
      <td style="text-align: center">0.3</td>
      <td style="text-align: center">0.31</td>
      <td style="text-align: center">0.28</td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/undersampling-acmc-f-mcc.ipynb">3</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Undersampling-200</td>
      <td style="text-align: center">50</td>
      <td style="text-align: center">735</td>
      <td style="text-align: center"><strong>97</strong></td>
      <td style="text-align: center">46</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: center">0.46</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/undersampling-acmc-200-f-mcc.ipynb">4</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Class weighting</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">618</td>
      <td style="text-align: center">56</td>
      <td style="text-align: center">48</td>
      <td style="text-align: center">0.39</td>
      <td style="text-align: center">0.48</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-imagenet-class-weights-20.ipynb">5</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Class weighting</td>
      <td style="text-align: center">50</td>
      <td style="text-align: center">1534</td>
      <td style="text-align: center">83</td>
      <td style="text-align: center"><strong>58</strong></td>
      <td style="text-align: center"><strong>0.47</strong></td>
      <td style="text-align: center"><strong>0.58</strong></td>
      <td style="text-align: center"><strong>0.53</strong></td>
      <td style="text-align: left"><a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-imagenet-class-weights-50.ipynb">6</a></td>
    </tr>
  </tbody>
</table>

<p><sup>a</sup>: Approx run time in 2022 on Kaggle Notebooks using GPU</p>

<p style="text-align: justify">Now, looking at the table, a few things stand out. Firstly, for the whole dataset, overfitting is evident even after 20 epochs, and this only increases further after 50. However, the macroaverage F<sub>1</sub> score does show considerable improvement under further training, far more than the micro-average F<sub>1</sub> score does. Intuitively, this would suggest that the model first learns mostly on the majority classes before turning its attention towards the minority classes in an effort to raise the accuracy further.</p>

<p style="text-align: justify">In case of undersampling, using the minimum number of samples per class makes for very fast training due to the much-reduced number of samples. That’s all that it has going for it, though - while training accuracy approaches 100%, the other parameters are very bad indeed. The undersampling-200 approach fares much better, although compared with using the whole dataset, again the only area it does better is the shorter run time. The MCC is considerably lower than for the whole dataset, and the macro-F<sub>1</sub> score slightly inferior, which means that our hope of building a more equitable model hasn’t really come to pass using undersampling.</p>

<p style="text-align: justify">Class weighting, as we saw earlier, narrows the overfitting considerably, which is a plus. The remaining results, however, are merely comparable with using the full dataset, rather than an improvement. The fact that the training accuracy still has upward room may mean that further training will improve the other values, but this would of course be at the cost of increased training time.</p>

<p style="text-align: justify">A final comment. We see that the MCC tend to lie between the micro- and macro-F<sub>1</sub> scores, and rise and fall in tandem with them. For this dataset, therefore, it does not appear to provide any novel insights. It would be unfair to draw any conclusions based on this alone, however, and so I will continue using the MCC in conjunction with the F<sub>1</sub> score, and see if they diverge in future studies, and what such divergence could mean.</p>

<h2 id="conclusion-">Conclusion <a id="conc"></a></h2>

<p style="text-align: justify">So far we have tested two approaches for developing a classifier that is similarly accurate on the different classes of the ACMC dataset. Unfortunately, neither undersampling nor class weighting were able to significantly improve upon simply using the original dataset. Perhaps oversampling will do the trick? Or maybe a combination of different options? We shall see…for now, bye!</p>

<p><br /></p>
<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/ACMC-undersampling/">&#8672;&nbsp;10. ACMC - Undersampling and F-scores</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;, &quot;Metric&quot;]" /><category term="TensorFlow 2" /><category term="Keras" /><category term="Matplotlib" /><category term="Seaborn" /><category term="Pandas" /><category term="Scikit-learn" /><category term="Matthews correlation coefficient" /><summary type="html"><![CDATA[&#8672;&nbsp;10. ACMC - Undersampling and F-scores]]></summary></entry><entry><title type="html">10. ACMC - Undersampling and F-scores</title><link href="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/" rel="alternate" type="text/html" title="10. ACMC - Undersampling and F-scores" /><published>2022-06-30T13:00:00+02:00</published><updated>2022-06-30T13:00:00+02:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/ACMC-undersampling/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/ACMC-simple-model/">&#8672;&nbsp;9. Diving into the Archie Comics Multiclass dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-class-wt/">11. ACMC - Class weighting and MCC&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Metric">Metric</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
TensorFlow 2, Keras, Matplotlib, Seaborn, Pandas, Scikit-learn, F-score
</p>
<p><br /></p>

<p><em>[Edit: This post expands on a previously published version, with a description of F-scores and an alternative undersampling approach added.]</em></p>

<p style="text-align: justify">Hello! It’s been a long time since my previous post, as I have had plenty to do, professionally and personally, since then. This also means that this post will be shorter than I had originally planned. <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/">Last time</a> I left off saying that we would be looking at different approaches for handling unbalanced datasets, but today we will only look at one such approach - the rest (hopefully) coming soon!</p>

<p style="text-align: justify">So, as a brief reminder, we are dealing with the Archie Comics Multiclass (ACMC) dataset that I created and placed <a href="https://www.kaggle.com/datasets/agneev/archie-comics-multi-class">here</a>. We <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/">have already seen</a> how a simple model can get a decent validation accuracy of around 56%, at the cost of a) overfitting, and b) having a far higher true positive rate (or recall) for the classes having more samples (i.e. the majority classes) in the unbalanced dataset. The overfitting we leave aside for the moment, but what if we need a model that is approximately equally accurate for every class? For this, we need to overcome the class imbalance, such as via…</p>

<h2 id="undersampling-">Undersampling <a id="undersample"></a></h2>

<p style="text-align: justify">Since some of the classes having more samples than others is our issue, the first way of handling it would be to simply make the number of samples the same. This could be done either by reducing the number of samples considered in the majority classes (undersampling) or increasing the number of samples in the minority classes via duplication or synthetic sampling (oversampling). We will look at oversampling in a later post, but how do we go about undersampling?</p>

<p style="text-align: justify">The simplest thing to do is to look at the minimum number of samples that are present in every class and just take that many images from each class. In other words, we reduce all the columns in the figure below to the red line showing the level of the minimum samples class (Svenson). This method ensures that every class has the same number of samples, at the cost of discarding a large number of samples from the majority classes.</p>

<p><img src="/agneev-blog/assets/img/img_10_1.png?raw=true" alt="Image_1" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">An alternative is to fix a particular threshold, say 200 samples, and remove the excess samples for the classes that have a higher number of samples. This is denoted by the yellow line in the figure. In this case, a degree of imbalance persists among the classes, but to a much lesser extent than in the original dataset, and we get to train with more samples than using the minimum samples approach.</p>

<p>Let’s look at both approaches in turn.</p>

<h3 id="minimum-samples-approach-">Minimum samples approach <a id="min_sample"></a></h3>

<p style="text-align: justify">Now, I mentioned the ‘simplest thing to do’ above, but I found it surprisingly hard to actually carry it out in TensorFlow. After trying different methods, I finally decided that using the <a href="https://vijayabhaskar96.medium.com/tutorial-on-keras-flow-from-dataframe-1fd4493d237c">flow_from_dataframe</a> function of Keras is the best way to go about things. It’s not perfect - it is a function of Keras’ <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">ImageDataGenerator</a>, which is deprecated. Still, this method allows us to quickly look at the effect of undersampling on the results without having to actually create new directories containing fewer images, etc., and so we’ll stick with this for now. The full code is on Github <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/undersampling-acmc.ipynb">here</a> - let’s look at the important parts.</p>

<p style="text-align: justify">First, let’s create and populate lists with the names of the classes, number of samples per class, and the image filenames:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">samples_per_class</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">file_names</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">directory</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="s">'../input/archie-comics-multi-class/Multi-class/'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">directory</span><span class="p">:</span>
    <span class="n">currentFolder</span> <span class="o">=</span> <span class="s">'../input/archie-comics-multi-class/Multi-class/'</span> <span class="o">+</span> <span class="n">each</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">files</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">walk</span><span class="p">(</span><span class="n">currentFolder</span><span class="p">))</span>
    <span class="n">samples_per_class</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="n">classes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">each</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">currentFolder</span><span class="p">)):</span>
            <span class="n">fullpath</span> <span class="o">=</span> <span class="n">currentFolder</span><span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="nb">file</span>
            <span class="n">file_names</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">fullpath</span><span class="p">)</span>

<span class="nb">min</span><span class="p">(</span><span class="n">samples_per_class</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">The line at the end of the above code will show that the minimum number of samples in each class is 33. Now to go about undersampling. To do this, first let’s create a list of 33 filenames per class. There are various ways of doing this, but the code below is what I have gone for.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="n">small_ds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">each_class</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
    <span class="n">trial_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">re</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">each_class</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="n">trial_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">small_ds</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">trial_list</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">samples_per_class</span><span class="p">)))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p style="text-align: justify">This code deserves a bit of explanation, and so I have added line numbers to the code block above to make it easier to understand. Essentially, after creating an empty small_ds list (Line 1), we run a for-loop for each class populating this list (Line 3). The for-loop itself works like this: consider any class, for instance ‘Jughead’. We create a new empty trial_list (Line 4), then, using a nested for-loop (Line 5), scan through the file_names list we created earlier. Using a regular expression (Line 6), we select the filenames containing ‘Jughead’. These file names are added to trial_list (Line 7). At the end of the nested for-loop, trial_list contains all the filenames containing ‘Jughead’ in them. Exiting the nested for-loop, we add 33 randomly selected ‘Jughead’ filenames from trial_list to the small_ds list (Line 8). We then move to the next class in the outer loop (Line 3), with a new empty trial_list being created (Line 4), and so on. At the end, therefore, we get a list containing 33 randomly selected filenames for each of the 23 classes.</p>

<p style="text-align: justify">Let us see whether we have created the small_ds properly. First, the length:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span></code></pre></figure>

<p style="text-align: justify">The results of the above print statements are 23, 33, and 759. In other words, we have created a 2D list with dimensions 23*33. Handling this list might be easier if it’s 1D, so let’s flatten it.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">small_ds</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">small_ds</span><span class="p">).</span><span class="n">flat</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">))</span></code></pre></figure>

<p style="text-align: justify">The length is now 759, indicating we have successfully obtained a 1D list containing all the filenames. We can peek into this list and see:</p>

<p><img src="/agneev-blog/assets/img/img_10_2.png?raw=true" alt="Image_2" width="600&quot;, height=&quot;400" /></p>

<p style="text-align: justify">The list, we see, contains the complete filenames selected randomly. Let us now make a Pandas dataframe out of this list.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="n">files_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">)),</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>

<span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">samples_per_class</span><span class="p">)</span>
<span class="k">for</span> <span class="n">each_class</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
    <span class="n">files_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_class</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">end</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">samples_per_class</span><span class="p">)</span>

<span class="n">files_df</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">files_df</span><span class="p">[</span><span class="s">'Class'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'str'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p style="text-align: justify">Above, we first create an empty Pandas dataframe with an index between 0 and 759 and a solitary ‘Class’ column (Line 1). We now need to fill up this ‘Class’ column - I have used a quick and dirty method in the code above, but this, as we will see, is not suitable for a general case. This method is simply using a for-loop (Lines 5-8) so that rows 0-32 containing the first class name, 33-65 the second, and so on.</p>

<p><img src="/agneev-blog/assets/img/img_10_3.png?raw=true" alt="Image_3" width="400&quot;, height=&quot;200" /></p>

<p>The small_ds dataset is then added as a new column ‘Files’:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">files_df</span><span class="p">[</span><span class="s">'Files'</span><span class="p">]</span> <span class="o">=</span> <span class="n">small_ds</span></code></pre></figure>

<p>We now see the files_df dataframe has the class names and the file names:</p>

<p><img src="/agneev-blog/assets/img/img_10_4.png?raw=true" alt="Image_4" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">Now, during my first run, I left things like this and moved on to modelling - this is an error! Try and figure out why, and we will return to the question a little later :grin:</p>

<h3 id="model-run-">Model run <a id="run"></a></h3>

<p style="text-align: justify">So the next step is to create training and validation generators with a 80:20 training-validation split. We can as usual apply a range of image augmentations to the training set here (as explained in <a href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-2/#aug">my earlier post</a>).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">datagen</span><span class="o">=</span><span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">train_generator</span><span class="o">=</span><span class="n">datagen</span><span class="p">.</span><span class="n">flow_from_dataframe</span><span class="p">(</span>
<span class="n">dataframe</span><span class="o">=</span><span class="n">files_df</span><span class="p">,</span>
<span class="n">directory</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">x_col</span><span class="o">=</span><span class="s">'Files'</span><span class="p">,</span>
<span class="n">y_col</span><span class="o">=</span><span class="s">'Class'</span><span class="p">,</span>
<span class="n">subset</span><span class="o">=</span><span class="s">"training"</span><span class="p">,</span>
<span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="n">class_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
<span class="c1"># rotation_range=30,
# width_shift_range=0.2,
# height_shift_range=0.2,
# brightness_range=(0.5,1.5),
# shear_range=0.2,
# zoom_range=0.2,
# channel_shift_range=30.0,
# fill_mode='nearest',
# horizontal_flip=True,
# vertical_flip=False,
</span><span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>

<span class="n">valid_generator</span><span class="o">=</span><span class="n">datagen</span><span class="p">.</span><span class="n">flow_from_dataframe</span><span class="p">(</span>
<span class="n">dataframe</span><span class="o">=</span><span class="n">files_df</span><span class="p">,</span>
<span class="n">directory</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">x_col</span><span class="o">=</span><span class="s">'Files'</span><span class="p">,</span>
<span class="n">y_col</span><span class="o">=</span><span class="s">'Class'</span><span class="p">,</span>
<span class="n">subset</span><span class="o">=</span><span class="s">"validation"</span><span class="p">,</span>
<span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="n">class_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
<span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span></code></pre></figure>

<p style="text-align: justify">We can then run the model, with the <a href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-2/#stop">usual callbacks</a>. Initially, let us have a patience value of 15 epochs for the early stopping, and run it for 50 epochs. The image sizes I used were 256 x 256, the base value used last time. The first results look like this:</p>

<p><img src="/agneev-blog/assets/img/img_10_5.png?raw=true" alt="Image_5" width="350&quot;, height=&quot;200" />
<img src="/agneev-blog/assets/img/img_10_6.png?raw=true" alt="Image_6" width="350&quot;, height=&quot;200" /></p>

<p style="text-align: justify">Hmm…strange. The training accuracy and loss are fine, but the validation accuracy remains virtually at zero, while the validation loss actually rises relentlessly. Clearly something is wrong…</p>
<p style="text-align: justify">.</p>
<p style="text-align: justify">.</p>

<p style="text-align: justify">Remember I said earlier that creating the dataframe and moving directly to modelling was an error? Well, what this has resulted in is a complete mismatch between the training and validation sets. In Keras, “validation_split = 0.2” <a href="https://github.com/keras-team/keras/issues/597">leads to</a> the bottom 20% of the input data being designated as the validation set while the upper 80% is the training set. This means that here, the training set consists of all the classes that appear in the upper portion of the files_df dataframe (Kleats, Midge, Dilton, etc.), while the bottom (Beazley, Hiram Lodge, Others…) becomes the validation set. No wonder then that the results are so poor, since the model is being trained on one set of classes and validated on another set!</p>

<p>We therefore add a crucial line of code prior to creating the generators:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">files_df</span> <span class="o">=</span> <span class="n">files_df</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reset_index</span><span class="p">()</span></code></pre></figure>

<p style="text-align: justify">The above line of code is explained well <a href="https://datagy.io/pandas-shuffle-dataframe/">here</a> and so I won’t go into the details, but in short, it shuffles the rows of the dataframe so that our training and validation sets now receive a mix of classes. If we see the last 10 rows of the dataframe, we now see:</p>

<p><img src="/agneev-blog/assets/img/img_10_7.png?raw=true" alt="Image_7" width="450&quot;, height=&quot;250" /></p>

<p><br /></p>

<p style="text-align: justify">That’s better…the different classes are now randomly arranged. We see that the original indices have been added as a column, which we can drop if we choose. Anyway, if we run the model again, we now get:</p>

<p><img src="/agneev-blog/assets/img/img_10_8.png?raw=true" alt="Image_8" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">We see that the validation accuracy and loss now look more familiar (and appropriate). Let us recap a sentence from the start of this post, however.</p>

<blockquote>
  <p>A simple model can get a decent validation accuracy of around 56%, at the cost of a) overfitting, and b) having a far higher true positive rate (or recall) for the classes having more samples (i.e. the majority classes) in the unbalanced dataset.</p>
</blockquote>

<p style="text-align: justify">In the simple model, we got the validation accuracy of 56% after 20 epochs. Here, after 50 epochs we managed around 33%. A cursory glance at the training curves shows that the overfitting problem has got worse. However, these things are not really surprising - we are using far fewer samples in this run, so understandably the accuracy figures suffer and overfitting occurs. The reason we experimented with undersampling in the first place was not to improve the validation accuracy or solve overfitting, but to make the results more equitable across the classes. Have we at least achieved that? Let us look at the <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/#elephant">confusion matrix</a>. <em>(Note: the image looks much better on Firefox than Chrome, due to a well-known problem Chrome has with downscaling images. If needed, you can open the image in a new tab and zoom it to make it easier to read.)</em></p>

<p><img src="/agneev-blog/assets/img/img_10_9.png?raw=true" alt="Image_9" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">Viewed one way, the problem of the majority classes getting higher true positive rates has been resolved - the results for Archie, Jughead, etc. are pretty average now. On the other hand, several classes still perform much better or worse than average. Why is this? Well, one reason can be divined from looking at the absolute numbers in the confusion matrix:</p>
<p><br />
<img src="/agneev-blog/assets/img/img_10_10.png?raw=true" alt="Image_10" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">We see that some classes have as few as 4 samples in the validation set, and in general, the number of samples are too few to make a reasonable judgement about how good the model is for any class, considering both the heterogeneity of the dataset and the <a href="https://machinelearningmastery.com/stochastic-in-machine-learning/">stochastic nature</a> of ML models. Other factors for why some classes show a higher true positive rate might be some characters being easier to identify, the validation images more closely resembling the training figures for these classes, etc. In any case, our stated goal of achieving an even recognition of all the classes thus remains unaccomplished…</p>

<p><br /></p>
<h3 id="arbitrary-threshold-approach-">Arbitrary threshold approach <a id="arb_thresh"></a></h3>
<p style="text-align: justify">As I said near the start of this post, an alternative to the minimum samples approach would be to fix a particular threshold and discard the excess samples for the majority classes. This threshold is somewhat arbitrary, but looking at Image 1, we see that 200 samples seems to be a pretty good value to pick. If we choose a smaller number, say 100, then we will be discarding a lot of images, since most of the classes have more than 100 images. On the other hand, a threshold of 300 or above will again result in an excess of samples from the majority classes, since only these have more than about 250 images. 200 seems to be a good compromise number, and so let’s go with this (complete code <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/undersampling-acmc-200.ipynb">here</a>).</p>

<p style="text-align: justify">The procedure is similar to that we saw earlier, with one critical difference. Remember that we had used a for-loop to fill the ‘Class’ column with 33 rows of each samples in files_df? So that we got something like this?</p>

<p><img src="/agneev-blog/assets/img/img_10_3.png?raw=true" alt="Image_3" width="350&quot;, height=&quot;200" /></p>

<p>We can’t use this now, since we don’t have the same number of samples for each class. Instead, we will have to do things the hard way and get the class names from the file names. The code for this is:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre><span class="n">files_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">small_ds</span><span class="p">)),</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Files'</span><span class="p">])</span>

<span class="n">files_df</span><span class="p">[</span><span class="s">'Files'</span><span class="p">]</span> <span class="o">=</span> <span class="n">small_ds</span>

<span class="n">files_df</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">files_df</span><span class="p">.</span><span class="n">Files</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="sa">r</span><span class="s">'Multi-class/(.*?)/'</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">files_df</span> <span class="o">=</span> <span class="n">files_df</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pd</span><span class="p">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_colwidth'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">files_df</span><span class="p">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p style="text-align: justify">In Line 1 above, we create the files_df dataframe, but use ‘Files’ as the original column now. We then populate the column with the small_ds list as earlier (Line 3). After this, we use a regular expression to extract the class name from the file name (see <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html">this</a>) and store this in a newly created ‘Class’ column (Line 5). Line 7 randomly shuffles the rows as before, only this time I am dropping the original indices instead of adding them as a new column. Line 9 is optional, but important here. Pandas typically limits column width and only displays the first few characters of every row in the column. However, if we want to check whether the class names have been correctly selected, we need to see the end of the file names, which we can do by setting the maximum column width to ‘None’. Finally, looking at the last 10 rows of the dataframe (Line 11), we see:</p>

<p><img src="/agneev-blog/assets/img/img_10_11.png?raw=true" alt="Image_11" width="600&quot;, height=&quot;400" /></p>

<p style="text-align: justify">Yes, this looks fine. We also see that the number of rows has now increased to 2807 (starting from 0), so we are now using a much greater proportion of our original dataset. We can then go on with the modelling as usual, finally getting a validation accuracy of around 50% from 50 epochs (after applying some augmentations to the training set, check out the complete code for that).</p>

<p><img src="/agneev-blog/assets/img/img_10_12.png?raw=true" alt="Image_12" width="800&quot;, height=&quot;400" /></p>

<p style="text-align: justify">Once again we see the overfitting persisting, but once again, that’s not our concern here. What does the confusion matrix look like?</p>

<p><img src="/agneev-blog/assets/img/img_10_13.png?raw=true" alt="Image_13" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">Hmm. This certainly appears better than the minimum samples approach. Is it better than <a href="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/#elephant">that of the original dataset</a>, however? If so, by how much? Better enough to justify the loss in validation accuracy?</p>

<p style="text-align: justify">So far we have been handling these things qualitatively. If we are to answer the above questions, however, we need to be more quantitative. Using just accuracy won’t work - we need a metric that can compare models taking into account not only the predictions that the models got correct but also those they got wrong. Enter the…</p>
<p><br /></p>

<h2 id="f-score-">F-score <a id="f-score"></a></h2>

<p style="text-align: justify"><a href="https://en.wikipedia.org/wiki/F-score">As Wikipedia says</a>, the F-score is a measure of accuracy calculated using <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a>. What are these terms?</p>

<p style="text-align: justify">Precision (also called <a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values">positive predictive value</a>) is the ratio of true positives (TP) to all positive predictions (true positives + false positives (FP)). Recall (also called <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">true positive rate or sensitivity</a>) is the ratio of true positives to actual positives (true positives + false negatives (FN)). They are calculated as per:</p>

<p><img src="/agneev-blog/assets/img/img_10_14.png?raw=true" alt="Image_14" width="300&quot;, height=&quot;200" /></p>

<p style="text-align: justify">We can better understand the above equations with a concrete example. Let us look at the absolute numbers obtained for the Archie class in the confusion matrix for the 200 sample threshold undersampling.</p>

<p><img src="/agneev-blog/assets/img/img_10_15.png?raw=true" alt="Image_15" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">Adding up the numbers for ‘True Values’, we see that there are a total of 30 Archie images in the validation set. Of these, our model has correctly predicted only 11 (True Positives), meaning that 19 images were incorrectly classified as belonging to other classes (False Negatives). The recall, therefore, is 11/(11+19) = <strong>0.37</strong>.</p>

<p style="text-align: justify">What about the precision? We see that no fewer than 35 images had been identified as Archie (adding up the ‘Predicted Value’ numbers), of which, again, only 11 are correct predictions (True Positives). The remaining 24 are therefore False Positives. The precision is  11/(11+24) = <strong>0.31</strong>.</p>

<p style="text-align: justify">OK, so now that we have the precision and recall values, how do we get the F-score? There are actually various ways of doing this, giving different importances to precision and recall, but the most widely used is the F<sub>1</sub> score. This is defined as the harmonic mean of precision and recall:</p>

<p><img src="/agneev-blog/assets/img/img_10_16.png?raw=true" alt="Image_16" width="600&quot;, height=&quot;400" /></p>

<p style="text-align: justify">The F<sub>1</sub> score for the Archie class is thus 2 * (0.31 * 0.37) / (0.31 + 0.37) = <strong>0.34</strong>.</p>

<p style="text-align: justify">Is this a good performance? No! F-scores range between 0 and 1, with a higher number indicating a better performing model. At least for the Archie class, therefore, our model hasn’t been brilliant. What about its overall performance, though?</p>

<h3 id="f-score-averaging-">F-score averaging <a id="avg"></a></h3>

<p style="text-align: justify">One thing that I have not mentioned yet is that the F-score is defined for binary classification. The example we looked at above was essentially ‘Archie’ versus ‘non-Archie’, a binary classification problem. How do we adapt the F-score to multiclass classification then? Simply by taking the average of the F-scores for all the classes…but how do we take the average?</p>

<p style="text-align: justify">The two most widely used F-score averages are the micro- and the macro-averages. These can again be better understood by concrete numbers, so let us first tabulate the TP, FP, TN (true negative), FN and F1 values for all the classes.</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th style="text-align: left">Class</th>
        <th style="text-align: right">True positives</th>
        <th style="text-align: right">False positives</th>
        <th style="text-align: right">True negatives</th>
        <th style="text-align: right">False negatives</th>
        <th style="text-align: right">F<sub>1</sub> score</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left">Kleats</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">20</td>
        <td style="text-align: right">506</td>
        <td style="text-align: right">19</td>
        <td style="text-align: right">0.45</td>
      </tr>
      <tr>
        <td style="text-align: left">Midge</td>
        <td style="text-align: right">6</td>
        <td style="text-align: right">0</td>
        <td style="text-align: right">553</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">0.86</td>
      </tr>
      <tr>
        <td style="text-align: left">Dilton</td>
        <td style="text-align: right">30</td>
        <td style="text-align: right">21</td>
        <td style="text-align: right">500</td>
        <td style="text-align: right">10</td>
        <td style="text-align: right">0.66</td>
      </tr>
      <tr>
        <td style="text-align: left">Pop Tate</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">0</td>
        <td style="text-align: right">539</td>
        <td style="text-align: right">6</td>
        <td style="text-align: right">0.84</td>
      </tr>
      <tr>
        <td style="text-align: left">Reggie</td>
        <td style="text-align: right">14</td>
        <td style="text-align: right">20</td>
        <td style="text-align: right">511</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">0.44</td>
      </tr>
      <tr>
        <td style="text-align: left">Jughead</td>
        <td style="text-align: right">4</td>
        <td style="text-align: right">8</td>
        <td style="text-align: right">540</td>
        <td style="text-align: right">9</td>
        <td style="text-align: right">0.32</td>
      </tr>
      <tr>
        <td style="text-align: left">Grundy</td>
        <td style="text-align: right">4</td>
        <td style="text-align: right">5</td>
        <td style="text-align: right">547</td>
        <td style="text-align: right">5</td>
        <td style="text-align: right">0.44</td>
      </tr>
      <tr>
        <td style="text-align: left">Ethel</td>
        <td style="text-align: right">15</td>
        <td style="text-align: right">20</td>
        <td style="text-align: right">511</td>
        <td style="text-align: right">15</td>
        <td style="text-align: right">0.46</td>
      </tr>
      <tr>
        <td style="text-align: left">Weatherbee</td>
        <td style="text-align: right">21</td>
        <td style="text-align: right">23</td>
        <td style="text-align: right">503</td>
        <td style="text-align: right">14</td>
        <td style="text-align: right">0.53</td>
      </tr>
      <tr>
        <td style="text-align: left">Harry Clayton</td>
        <td style="text-align: right">4</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">553</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">0.67</td>
      </tr>
      <tr>
        <td style="text-align: left">Smithers</td>
        <td style="text-align: right">21</td>
        <td style="text-align: right">32</td>
        <td style="text-align: right">483</td>
        <td style="text-align: right">25</td>
        <td style="text-align: right">0.42</td>
      </tr>
      <tr>
        <td style="text-align: left">Svenson</td>
        <td style="text-align: right">30</td>
        <td style="text-align: right">21</td>
        <td style="text-align: right">494</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">0.62</td>
      </tr>
      <tr>
        <td style="text-align: left">Moose</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">3</td>
        <td style="text-align: right">551</td>
        <td style="text-align: right">5</td>
        <td style="text-align: right">0.33</td>
      </tr>
      <tr>
        <td style="text-align: left">Chuck</td>
        <td style="text-align: right">3</td>
        <td style="text-align: right">3</td>
        <td style="text-align: right">542</td>
        <td style="text-align: right">13</td>
        <td style="text-align: right">0.27</td>
      </tr>
      <tr>
        <td style="text-align: left">Nancy</td>
        <td style="text-align: right">14</td>
        <td style="text-align: right">11</td>
        <td style="text-align: right">521</td>
        <td style="text-align: right">15</td>
        <td style="text-align: right">0.52</td>
      </tr>
      <tr>
        <td style="text-align: left">Veronica</td>
        <td style="text-align: right">5</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">549</td>
        <td style="text-align: right">5</td>
        <td style="text-align: right">0.59</td>
      </tr>
      <tr>
        <td style="text-align: left">Flutesnoot</td>
        <td style="text-align: right">1</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">513</td>
        <td style="text-align: right">31</td>
        <td style="text-align: right">0.04</td>
      </tr>
      <tr>
        <td style="text-align: left">Fred Andrews</td>
        <td style="text-align: right">8</td>
        <td style="text-align: right">10</td>
        <td style="text-align: right">533</td>
        <td style="text-align: right">10</td>
        <td style="text-align: right">0.44</td>
      </tr>
      <tr>
        <td style="text-align: left">Archie</td>
        <td style="text-align: right">11</td>
        <td style="text-align: right">24</td>
        <td style="text-align: right">507</td>
        <td style="text-align: right">19</td>
        <td style="text-align: right">0.34</td>
      </tr>
      <tr>
        <td style="text-align: left">Betty</td>
        <td style="text-align: right">1</td>
        <td style="text-align: right">3</td>
        <td style="text-align: right">544</td>
        <td style="text-align: right">13</td>
        <td style="text-align: right">0.11</td>
      </tr>
      <tr>
        <td style="text-align: left">Beazley</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">2</td>
        <td style="text-align: right">553</td>
        <td style="text-align: right">4</td>
        <td style="text-align: right">0.4</td>
      </tr>
      <tr>
        <td style="text-align: left">Hiram Lodge</td>
        <td style="text-align: right">27</td>
        <td style="text-align: right">20</td>
        <td style="text-align: right">499</td>
        <td style="text-align: right">15</td>
        <td style="text-align: right">0.61</td>
      </tr>
      <tr>
        <td style="text-align: left">Others</td>
        <td style="text-align: right">21</td>
        <td style="text-align: right">19</td>
        <td style="text-align: right">505</td>
        <td style="text-align: right">16</td>
        <td style="text-align: right">0.55</td>
      </tr>
      <tr>
        <td style="text-align: left">Total</td>
        <td style="text-align: right">276</td>
        <td style="text-align: right">285</td>
        <td style="text-align: right">12057</td>
        <td style="text-align: right">285</td>
        <td style="text-align: right">nan</td>
      </tr>
    </tbody>
  </table>

</div>

<p style="text-align: justify">The macroaverage F<sub>1</sub> is simply the mean of the F<sub>1</sub> values for all the classes. In this case, the mean of the F<sub>1</sub> score column comes to <strong>0.47</strong>, and this is the macroaverage F<sub>1</sub> score.</p>

<p style="text-align: justify">The microaverage F<sub>1</sub> score, on the other hand, is calculated using the sum of the TP, FP and FN values. Here, the microaverage F<sub>1</sub> = 276 / (276 + 1/2 * (285 + 285)) = <strong>0.49</strong>.</p>

<p style="text-align: justify">We can see that the macroaverage F<sub>1</sub> score gives equal weightage to every class, regardless of the class sample size. The microaverage F<sub>1</sub> score, on the other hand, uses the cumulative TP, FP and FN values, and as the majority classes contribute more to these than the minority classes, the microaverage F<sub>1</sub> score is biased towards performance on the majority classes. For evaluating a model we want to see perform equally well on different classes of an imbalanced dataset, therefore, the macroaverage F<sub>1</sub> score is the more suitable metric.</p>

<p style="text-align: justify">If we compare the macro- and microaverage F<sub>1</sub> scores for the two undersampling approaches, we get:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">Macroaverage F<sub>1</sub></th>
      <th style="text-align: right">Microaverage F<sub>1</sub></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Undersampling-min</td>
      <td style="text-align: right">0.32</td>
      <td style="text-align: right">0.33</td>
    </tr>
    <tr>
      <td style="text-align: left">Undersampling-200</td>
      <td style="text-align: right">0.47</td>
      <td style="text-align: right">0.49</td>
    </tr>
  </tbody>
</table>

<p style="text-align: justify">We see that undersampling with 200 samples per class has a much better macroaverage F<sub>1</sub> score than undersampling using the minimum number of samples. We will discuss in detail how the two approaches compare with using the whole dataset next time. Teaser!</p>

<p style="text-align: justify">One thing to notice is that as the dataset gets more balanced, the difference between the macro- and microaverage F<sub>1</sub> scores reduces. This is axiomatically true - for a perfectly balanced dataset, there is no difference between the two averaging methods.</p>

<h2 id="conclusion-">Conclusion <a id="conc"></a></h2>
<p style="text-align: justify">Undersampling is a straightforward method of dealing with unbalanced data, but taken to an extreme for very unbalanced datasets, it is not very useful. Here, in our first attempt, we discarded almost 90% of the original images in an attempt to balance the classes, but ended up with both a poor validation accuracy and an enduring disparity in the performance across classes, as confirmed by the macroaverage F<sub>1</sub> score.</p>

<p style="text-align: justify">However, if we are willing to be a little more flexible and choose a higher threshold for the number of images per class, we can achieve much better results. It must be remembered, though, that the training and testing will remain less stable than for the original dataset due to the fewer images used. The main takeaway so far- undersampling may be a useful tool in redressing class imbalance, but needs to be used carefully!</p>

<p style="text-align: justify">This post has expanded to be much longer than I had anticipated, and so I will stop here. Next time, we will look at another option for handling unbalanced datasets. So long!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/ACMC-simple-model/">&#8672;&nbsp;9. Diving into the Archie Comics Multiclass dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-class-wt/">11. ACMC - Class weighting and MCC&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;, &quot;Metric&quot;]" /><category term="TensorFlow 2" /><category term="Keras" /><category term="Matplotlib" /><category term="Seaborn" /><category term="Pandas" /><category term="Scikit-learn" /><category term="F-score" /><summary type="html"><![CDATA[&#8672;&nbsp;9. Diving into the Archie Comics Multiclass dataset 11. ACMC - Class weighting and MCC&nbsp;&#8674;]]></summary></entry><entry><title type="html">9. Diving into the Archie Comics Multiclass dataset</title><link href="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/" rel="alternate" type="text/html" title="9. Diving into the Archie Comics Multiclass dataset" /><published>2022-04-25T13:00:00+02:00</published><updated>2022-04-25T13:00:00+02:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/ACMC-simple-model/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Archie-comics-multiclass-dataset-intro/">&#8672;&nbsp;8. Introducing the Archie Comics Multiclass dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-undersampling/">10. ACMC - Undersampling and F-scores&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
TensorFlow 2, Keras, Matplotlib, Seaborn, PIL, Pandas, Scikit-learn, Confusion Matrix
</p>
<p><br /></p>

<p style="text-align: justify">Hi there! <a href="https://agneevmukherjee.github.io/agneev-blog/Archie-comics-multiclass-dataset-intro/">Last time</a> we had a brief look at the Archie Comics Multiclass (ACMC) dataset that I created, and which can be found <a href="https://www.kaggle.com/datasets/agneev/archie-comics-multi-class">here</a>. In this post, let us look at the dataset in a little more detail, and then do some simple modelling on it.</p>

<h2 id="a-look-at-the-images-and-their-distribution-">A look at the images and their distribution <a id="look"></a></h2>

<p style="text-align: justify">You can find a notebook looking at the images <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/acmc-images.ipynb">here</a>. The first part of the notebook looks at two images from each of the folders (as a reminder, there are 23 folders, for each of the 22 characters + ‘Others’), but as we already saw <a href="https://agneevmukherjee.github.io/agneev-blog/Archie-comics-multiclass-dataset-intro/#brief">something similar</a> in the previous post, let us move on to one of the aspects that I think makes this dataset really interesting- the class distribution. The code below first creates and populates two lists, one for the class names and the other for the number of samples per class, by walking through all the image folders. We can then create a <a href="https://pandas.pydata.org/">Pandas</a> dataframe from these lists, and plot them in a  bar plot using <a href="https://seaborn.pydata.org/">Seaborn</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">samples_per_class</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">directory</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="s">'../input/archie-comics-multi-class/Multi-class/'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">directory</span><span class="p">:</span>
    <span class="n">currentFolder</span> <span class="o">=</span> <span class="s">'../input/archie-comics-multi-class/Multi-class/'</span> <span class="o">+</span> <span class="n">each</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">files</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">walk</span><span class="p">(</span><span class="n">currentFolder</span><span class="p">))</span>
    <span class="n">samples_per_class</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="n">classes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">each</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">samples_per_class</span><span class="p">)),</span>
                   <span class="n">columns</span> <span class="o">=</span><span class="p">[</span><span class="s">'Classes'</span><span class="p">,</span> <span class="s">'Samples per class'</span><span class="p">])</span>

<span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span><span class="mi">8</span>

<span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s">'Classes'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">'Samples per class'</span><span class="p">)</span>
<span class="n">ticks</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify"><br />
All right, so let us see what the class distribution looks like…</p>

<p><img src="/agneev-blog/assets/img/img_9_1.png?raw=true" alt="Image_1" width="750&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Image 1: Class distribution of ACMC images
</p>

<p style="text-align: justify">Wow, that’s quite something, right? We see that there are nearly 1300 Archie images, while the other major characters, and ‘Others’ have plenty of samples as well (&gt;500 each). On the other hand, some of the minor characters have less than 50 samples apiece. This imbalance is a challenge, but I think is quite representative of the real world, where data is unlikely to be neatly balanced.</p>

<p style="text-align: justify">But the imbalance is not restricted just to the number of images per class - let us see what the image size distribution looks like. We can use <a href="https://pillow.readthedocs.io/en/stable/">Pillow</a> to get the size of every image, store these in a list of tuples, and then make a scatter plot. Along the way, we also check the median and mean of the tuples.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sizes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">directory</span><span class="p">:</span>
    <span class="n">currentFolder</span> <span class="o">=</span> <span class="s">'../input/archie-comics-multi-class/Multi-class/'</span> <span class="o">+</span> <span class="n">each</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">currentFolder</span><span class="p">)):</span>
        <span class="n">fullpath</span> <span class="o">=</span> <span class="n">currentFolder</span><span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="nb">file</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">fullpath</span><span class="p">)</span>
        <span class="n">sizes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">sizes</span><span class="p">).</span><span class="n">values</span><span class="p">()))</span>
<span class="c1"># from https://stackoverflow.com/questions/31836655/using-numpy-to-find-median-of-second-element-of-list-of-tuples
</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">sizes</span><span class="p">).</span><span class="n">values</span><span class="p">()))</span>

<span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span><span class="mi">6</span>

<span class="c1"># https://stackoverflow.com/questions/47032283/how-to-scatter-plot-a-two-dimensional-list-in-python
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><br />
This plot looks like this:</p>

<p><img src="/agneev-blog/assets/img/img_9_2.png?raw=true" alt="Image_2" width="700&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Image 2: Size distribution of ACMC images
</p>

<p style="text-align: justify">There are two things noticeable here. Firstly, the images have a wide size distribution, with the smallest being smaller than 100 pixel x 100 pixel, and the largest exceeding 1000 pixel by 1000 pixel. The code above gives us a median of 437 and a mean of 470.5. The second thing is that the images are far from being square. As computer vision (CV) models like to be fed images that are similarly sized and square-shaped, it is clear that image processing may be key to maximising image recognition accuracy on this dataset.</p>

<p style="text-align: justify">We can wind up this section with a look at some random images. The exact images displayed will differ every time the notebook is run, but will in general look like this:</p>

<p><img src="/agneev-blog/assets/img/img_9_3.png?raw=true" alt="Image_3" width="750&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Image 3: A look at random ACMC images
</p>

<p style="text-align: justify">The difference in the image quality, shapes, centering, etc. is obvious from these images. Also, as expected, the main characters and ‘Others’ dominate the images. Less obvious in this subplot is the difference in the image sizes, unless we look at the axes, some of which go beyond 800 while others stop below 100.</p>

<p style="text-align: justify">All right then, let’s get our hands dirty by doing some modelling on the data!</p>

<h2 id="simple-resnet-models-">Simple ResNet models <a id="simple"></a></h2>

<p style="text-align: justify">We will do some more intricate modelling on this data in later posts, but for now, let us make a simple ResNet50-based model on it. I have published the code for this <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-simple-orig.ipynb">here</a>. The most important parts are given below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">path</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="s">"inferred"</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
        <span class="n">class_names</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">subset</span><span class="o">=</span><span class="s">"training"</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="p">)</span>

<span class="n">valid</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">path</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="s">"inferred"</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
        <span class="n">class_names</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">subset</span><span class="o">=</span><span class="s">"validation"</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="p">)</span>

<span class="n">stopping</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
            <span class="n">monitor</span><span class="o">=</span><span class="s">"val_accuracy"</span><span class="p">,</span>
            <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s">"auto"</span><span class="p">,</span>
            <span class="n">baseline</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
            <span class="s">"best_model"</span><span class="p">,</span>
            <span class="n">monitor</span><span class="o">=</span><span class="s">"val_accuracy"</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s">"max"</span><span class="p">,</span>
            <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">ResNet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">resnet</span><span class="p">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
                <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(),</span><span class="c1">#from_logits=True),
</span>                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">]</span>
            <span class="p">)</span>

<span class="n">loss_0</span><span class="p">,</span> <span class="n">acc_0</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"loss </span><span class="si">{</span><span class="n">loss_0</span><span class="si">}</span><span class="s">, acc </span><span class="si">{</span><span class="n">acc_0</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">train</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="n">valid</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stopping</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
        <span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">"best_model"</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"final loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s">, final acc </span><span class="si">{</span><span class="n">acc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>        

<span class="c1"># summarize history for accuracy
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'model accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># summarize history for loss
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'model loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p style="text-align: justify">Briefly, what the above code does first is generate training and validation TensorFlow datasets with a 80:20 split. Note that since we do not have a pre-existing training-validation split, i.e. separate folders for training and validation, we use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory">image_dataset_from_directory</a> util instead of the more customary <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">flow_from_directory ImageDataGenerator</a>. Next, we define and fit a ResNet50 model, saving the model with the best validation accuracy via a checkpoint callback. We also provide an early stopping callback, with a ‘Patience’ of 5, i.e., the model will stop running if the validation accuracy doesn’t improve over 5 epochs. The remaining parameters are fairly standard. After the run, we plot the model accuracy and loss over the epochs, which look like this:</p>

<p><img src="/agneev-blog/assets/img/img_9_4.png?raw=true" alt="Image_4" width="400&quot;, height=&quot;200" /></p>

<p style="color:grey;font-size:100%;">
 Image 4: Accuracy and loss of ResNet50 model without pre-trained weights
</p>

<p style="text-align: justify">We can see that the plots show definite overfitting, with a divergence between the training and the validation curves. The other noteworthy feature is the jaggedness of the curves, one reason for which may be the small batch size (8) employed.</p>

<p style="text-align: justify">The maximum validation accuracy obtained in the above run is around 57%. This made me wonder - what if I started with Imagenet weights for the ResNet instead of no weights? I made another notebook on that <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/fork-of-tf-acmc-simple-imagenet.ipynb">here</a>. The main change is that for using the Imagenet weights, the model head needs to be changed, which I did as per <a href="https://pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/">this</a> - the modified portion of the code is shown below, while the rest remains unchanged.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># construct the head of the model that will be placed on top of the
# the base model
</span><span class="n">headModel</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">.</span><span class="n">output</span>
<span class="n">headModel</span> <span class="o">=</span> <span class="n">AveragePooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))(</span><span class="n">headModel</span><span class="p">)</span>
<span class="n">headModel</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"flatten"</span><span class="p">)(</span><span class="n">headModel</span><span class="p">)</span>
<span class="n">headModel</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">headModel</span><span class="p">)</span>
<span class="n">headModel</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">headModel</span><span class="p">)</span>
<span class="n">headModel</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)(</span><span class="n">headModel</span><span class="p">)</span>
<span class="c1"># place the head FC model on top of the base model (this will become
# the actual model we will train)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">base_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">headModel</span><span class="p">)</span>
<span class="c1"># loop over all layers in the base model and freeze them so they will
# *not* be updated during the training process
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">base_model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span></code></pre></figure>

<p style="text-align: justify">And we obtain a maximum validation accuracy of…56%…in other words, it appears to make no difference whether we use the Imagenet weights or not. This, however, is only true if we look just at the highest validation accuracy. Looking at the training curves gives a slightly different picture.</p>

<p><img src="/agneev-blog/assets/img/img_9_5.png?raw=true" alt="Image_5" width="400&quot;, height=&quot;200" /></p>

<p style="color:grey;font-size:100%;">
 Image 5: Accuracy and loss of ResNet50 model with Imagenet weights
</p>

<p style="text-align: justify">We can see that the curves are much smoother, implying that the training is a lot more stable - a feature not to be underestimated while training a small unbalanced dataset - or any dataset for that matter. On the other hand, the issue of overfitting remains, but that is understandable given the class imbalance. We will see later if we can do something about this.</p>

<h2 id="effect-of-image-size-on-validation-accuracy-">Effect of image size on validation accuracy <a id="size"></a></h2>

<p style="text-align: justify">One point that I did not comment on earlier was the image size we fed into the ResNets. If you see the code above, we used an input size of 256 x 256. I selected this size because, looking at Image 2, it is clear that the vast majority of images are larger than this, and it is <a href="https://blog.roboflow.com/you-might-be-resizing-your-images-incorrectly/">generally preferable</a> to downscale images to smaller sizes than upscale them by stretching the pixels.</p>

<p style="text-align: justify">Nevertheless, it is worth taking a look at what effect changing the image size has on accuracy, which is what I did in <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/fork-of-tf-acmc-simple-sizes.ipynb">this notebook</a>. Essentially, I made a list of three numbers - 128, 256 and 512 - and ran these as image sizes in a for-loop. The maximum validation accuracies I got were revealing - 0.43 for 128 x 128, 0.51 for 256 x 256, and 0.63 for 512 x 512. Why is the 256 x 256 accuracy lower than what we got earlier? Mainly because the early stopping kicked in on this particular run and halted the run after only 10 epochs, which is why the absolute numbers must be taken with a pinch of salt. The trend, however, is what is interesting, as we see a clear increase in validation accuracy with image size. Why would this be the case? Well quite simply, all other things equal, larger images have higher resolution, making it possible for the CV model to learn from details that are lost when the image is shrunk. This is in line with results obtained, for instance, in <a href="https://pubmed.ncbi.nlm.nih.gov/34943421/">this paper</a>, which looked at the effect of varying image resolutions from 32 x 32 to 512 x 512 and found the best results at the highest resolution.</p>

<p style="text-align: justify">If this is the case, then why did I not try an even higher resolution? Actually, I <em>did</em> try running the model at 1024 x 1024, and ran into a <a href="https://www.tensorflow.org/api_docs/python/tf/errors/ResourceExhaustedError">‘Resource Exhausted Error’</a>. Essentially, trying to load images of this size into memory results in a memory error. Resolving that would require making other changes in the code. One option would be reducing the batch size - but that is already just 8, and reducing it further would <a href="https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/">further increase</a> the volatility that we have already seen in the training curves. Another would be employing a smaller model - but then the comparison with the other image sizes modelled with ResNet50 would not be apples and apples. A third would be to upgrade the system configurations…not really possible in a Kaggle notebook!</p>

<p style="text-align: justify">.</p>
<p style="text-align: justify">.</p>
<p style="text-align: justify">.</p>

<p style="text-align: justify">However, even if we <em>could</em> run at 1024 x 1024, we would be unlikely to see much improvement. Why? Remember that we saw that the median image size was 437, while the mean was 470.5. Increasing the size much beyond this would likely be unhelpful, since, as noted <a href="https://blog.roboflow.com/you-might-be-resizing-your-images-incorrectly/">here</a>, increasing image sizes beyond their original dimensions stretches image pixels, impacting the ability of models to learn key features like object boundaries. Therefore, ‘the larger the better’ is only true for image size up to a point, with accuracies <a href="https://pubs.rsna.org/doi/full/10.1148/ryai.2019190015">often dropping</a> when image size is further increased. And finally, even if the accuracy doesn’t drop, one still needs to be consider if the <a href="https://www.quora.com/Does-the-input-image-size-affect-CNNs-performance-For-instance-is-a-CNN-trained-with-512-512-input-perform-better-than-being-trained-with-128-128">64-fold increase</a> in memory requirement and processing time caused by an 8-fold increase in image size (from 128 x 128 to 1024 x 1024) will really be worth it…</p>

<p style="text-align: justify">So, in short, 512 x 512 seems like a good image size, giving high validation accuracy without crashing our system. This image size does take considerably longer to train, though, than 128 x 128 - a 10 minute run for 128 x 128 may become a 160 minute run for 512 x 512. Given this, <a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/">progressive resizing</a>, a concept introduced by Jeremy Howard at FastAI, may be a good way to get great accuracies while keeping training times reasonable. As mentioned in that link,</p>

<blockquote style="text-align: justify">
  <p>One of our main advances in DAWNBench was to introduce progressive image resizing for classification – using small images at the start of training, and gradually increasing size as training progresses. That way, when the model is very inaccurate early on, it can quickly see lots of images and make rapid progress, and later in training it can see larger images to learn about more fine-grained distinctions.</p>
</blockquote>

<p style="text-align: justify">For now, we will park this idea, since the aim of this post is to keep things simple, and revisit the concept in a later post.</p>
<p><br /></p>
<h2 id="the-elephant-in-the-room-">The elephant in the room <a id="elephant"></a></h2>

<p style="text-align: justify">All right, we have talked quite a bit about validation accuracies and image sizes, but haven’t really mentioned one thing that really is important for a dataset as unbalanced as this - how good is the model at identifying each character? In other words, is the high accuracy just because it has learnt to recognise the main characters, or is it more well-rounded? Let’s find out!</p>

<p style="text-align: justify">Since we saw that using the Imagenet weights gives smoother training curves and using an image size of 512 x 512 increases validation accuracy, I made <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/tf-acmc-simple-imagenet-512.ipynb">yet another notebook</a>, this time using these parameters. The maximum validation accuracy obtained? 0.63! Yay! But that’s not what we are really interested in here. We are more into the information presented in the <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html">‘confusion matrix’</a>. A confusion matrix is a great tool to see at a glance how a machine learning algorithm has performed on a class-level, i.e., how many examples of each class it has correctly classified, and into which classes the incorrect results fall. A simple example is given by Scikit-learn for the Iris dataset:</p>

<p><img src="/agneev-blog/assets/img/img_9_6.png?raw=true" alt="Image_6" width="500&quot;, height=&quot;250" /></p>

<p style="color:grey;font-size:100%;">
 Image 6: Scikit-learn's example of a confusion matrix on the Iris dataset
</p>

<p style="text-align: justify">We see here in the normalised confusion matrix that perfect results were obtained for the setosa and virginica irises, while for the versicolor, 62% were correctly classified, the rest all being misclassified as virginica.</p>

<p style="text-align: justify">For the final run on our dataset, the normalised confusion matrix (created using Scikit-learn) looks like the image below. <em>(Note: the image looks much better on Firefox than Chrome, due to a well-known problem Chrome has with downscaling images. If needed, you can open the image in a new tab and zoom it to make it easier to read.)</em></p>

<p><img src="/agneev-blog/assets/img/img_9_7.png?raw=true" alt="Image_7" width="800&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Image 7: Normalised confusion matrix
</p>

<p style="text-align: justify">Hmmm, we see that the performance is all over the place, and that for some classes, the results are really quite poor. We also see that the model has put some examples from almost every class into ‘Others’, which makes sense if you think about it - if the model is sure that an example is Grundy or Moose, it goes into that category, while any it is unsure of goes into the catch-all bin of ‘Others’.</p>

<p style="text-align: justify">To better understand the model performance, however, we may want to also have a look at the non-normalised confusion matrix. Here it is:</p>

<p><img src="/agneev-blog/assets/img/img_9_8.png?raw=true" alt="Image_8" width="800&quot;, height=&quot;400" /></p>

<p style="color:grey;font-size:100%;text-align: center;">
 Image 8: Non-normalised confusion matrix
</p>

<p style="text-align: justify">To get why this is important, have a look at the Harry Clayton class. The normalised confusion matrix was disappointing for this class, as it showed that the model did not make a single accurate prediction on this class. Image 8, however, shows that there were only two images of this class in the validation set! The 0% true positive rate for this class is not, therefore, as shocking as it might first seem. The random manner in which the validation set is selected means that several of the smaller classes may be under-represented even more severely in the validation set than they had been in the original dataset, which is clearly the case here for Harry Clayton. Other runs had given a 100% true positive rate for the same class! This means that for the minor character classes, it is not merely difficult to model them but also hard to quantify the model performance on them.</p>

<p style="text-align: justify">A picture may help demonstrate the effect of class size on the classification performance more vividly. The excellent <a href="https://stackoverflow.com/a/43331484">top-rated answer</a> (not the accepted answer) on a Stackoverflow question shows how to get the true positives (Archie predicted as Archie), true negatives (non-Archie predicted as non-Archie), false positives (non-Archie predicted as Archie) and false negatives (Archie predicted as non-Archie) from a confusion matrix. Let us just deal with the true positive rate (TPR) (also called <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity</a> or <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>) for now, i.e., what fraction of, say, Ethel pix were correctly predicted as Ethel. The code below calculates the TPR for each class, makes a Pandas dataframe out of these, and then uses that dataframe to create a Seaborn regression plot.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">FN</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>
<span class="n">TP</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>
<span class="c1"># from https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal
</span><span class="n">TPR</span> <span class="o">=</span> <span class="n">TP</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FN</span><span class="p">)</span>

<span class="n">df_from_arr</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">samples_per_class</span><span class="p">,</span> <span class="n">TPR</span><span class="p">]).</span><span class="n">T</span>
<span class="n">df_from_arr</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s">"No. of samples"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s">"True positive rate"</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df_from_arr</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">classes</span>
<span class="n">df_from_arr</span><span class="p">[</span><span class="s">"No. of samples"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_from_arr</span><span class="p">[</span><span class="s">"No. of samples"</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">df_from_arr</span><span class="p">[</span><span class="s">"True positive rate"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_from_arr</span><span class="p">[</span><span class="s">"True positive rate"</span><span class="p">].</span><span class="nb">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_from_arr</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"No. of samples"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"True positive rate"</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_9_9.png?raw=true" alt="Image_9" width="400&quot;, height=&quot;200" /></p>

<p style="color:grey;font-size:100%;">
 Image 9: True positive rate versus number of samples in class
</p>

<p style="text-align: justify">We see that there is plenty of scatter towards the left-hand side of the plot, with some minor classes having a very low true positive rate and others very high values. We have already seen one possible cause of this - the fact that random selection means that there may be few examples of the classes in the training or validation sets. Another may be that the image quality or other factors could make certain classes easier to classify. And finally, some characters may simply be easier to recognise than others!</p>

<p style="text-align: justify">On the whole, though, the trend is clear - there is a clear upward trend, indicating that the characters with more images available are classified more accurately than those with fewer, which is what we would expect. For Archie pix, we have a TPR of 0.84, which is quite impressive, while Jughead and Betty also have a TPR of over 0.7, Veronica and Reggie getting 0.67 and 0.61 respectively.</p>

<h3 id="does-it-matter-">Does it matter? <a id="matter"></a></h3>

<p style="text-align: justify">We see that the major classes contribute heavily towards the overall accuracy of 0.63. Is that something we should be happy about? Well, that depends on what we want! If we are trying to build a classifier that will perform well on a randomly selected page from an Archie comics digest, we should be quite happy with what we have done so far. The odds of getting an Archie or a Jughead image, as opposed to a Beazly or Kleats image, in a random Archie comics panel is actually even higher than indicated by this dataset. So it makes sense for us to build a classifier that would work well on the major classes. Think of it this way - if you are building a dog image classifier, you would probably want your model to work well on German Shephard or Golden Retriever images, even if it gets the occasional Azawakh or Lagotto Romagnolo wrong.</p>

<p style="text-align: justify">If, on the other hand, we want a classifier that will work roughly equally well on every class, then our current approach is clearly not working. What can we do to sort this out? We shall see next time, when we discuss the different approaches for handling unbalanced datasets. For now, ciao!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Archie-comics-multiclass-dataset-intro/">&#8672;&nbsp;8. Introducing the Archie Comics Multiclass dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-undersampling/">10. ACMC - Undersampling and F-scores&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;]" /><category term="TensorFlow 2" /><category term="Keras" /><category term="Matplotlib" /><category term="Seaborn" /><category term="PIL" /><category term="Pandas" /><category term="Scikit-learn" /><category term="Confusion Matrix" /><summary type="html"><![CDATA[&#8672;&nbsp;8. Introducing the Archie Comics Multiclass dataset 10. ACMC - Undersampling and F-scores&nbsp;&#8674;]]></summary></entry><entry><title type="html">8. Introducing the Archie Comics Multiclass dataset</title><link href="https://agneevmukherjee.github.io/agneev-blog/Archie-comics-multiclass-dataset-intro/" rel="alternate" type="text/html" title="8. Introducing the Archie Comics Multiclass dataset" /><published>2022-04-04T13:00:00+02:00</published><updated>2022-04-04T13:00:00+02:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Archie-comics-multiclass-dataset-intro</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Archie-comics-multiclass-dataset-intro/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-4/">&#8672;&nbsp;7. Modelling water bodies - 4</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-simple-model/">9. Diving into the Archie Comics Multiclass dataset&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
Created dataset
</p>
<p><br /></p>

<p style="text-align: justify">Hello again! Today marks the start of an exciting new series, where we look at image classification and other computer vision (CV)-related tasks using the Archie Comics Multi-class (ACMC) dataset that I created.</p>

<p style="text-align: justify">Now, right at the start, let me address one issue that some of you may have - why comics? Why not something with more real world applicability? Well, firstly, because comics are inherently a sketched representation of the real world, almost any CV-related task that can be done on photo datasets can be done on comics datasets - image classification, object detection, image segmentation, image captioning, image reconstruction, you name it. Indeed, we shall look at several of these applications in later posts. The only real difference is that instead of say, training our model on photos of faces, cars, animals, etc., we will train on drawings of these instead. The second reason is that we can often obtain a large number of comics sketches much more easily than we could obtain photographs of a particular subject. In the ACMC dataset, for instance, we shall see that we have hundreds of sketches of certain characters, a number that may not be easy to obtain for any real life subjects. Thirdly, while comics may be representative of reality, they are often an <em>exaggerated</em> version of reality, providing us with poses and expressions that are not common in real life. Depending on your point of view, this may or may not be a plus, but I see it as increasing the diversity of the dataset. Fourth, and probably the main reason I chose to go down this path - it’s fun! Look, doing machine learning (ML) stuff is often hard and frustrating, and so I preferred to choose a subject that is at least interesting to me, rather than some dataset I couldn’t care less about!</p>

<p style="text-align: justify">OK, so then comes the next question - why Archie comics in particular? Once again, we have a list of reasons…starting with the fact that I have been reading them for almost three decades now, and so both have an extensive amount of material, and an intimate knowledge of the characters. The second reason would be that they have been published for over 80 years now. Why does that matter? Well, aside from the abundance of available material, this leads to a point that may be better appreciated by those familiar with the comics than those who are not. You see, dozens of artists have drawn these characters over the decades, bringing their own drawing style to the table. At the same time, dressing styles and other aspects of daily life have also changed tremendously over this period. Despite this, the artists are required to maintain a continuity with past representations of the characters, so that readers can read a 2020 Archie story as easily as they can a 1950 story (and stories from different decades often appear together in the same issue). In other words, the characters have to look similar enough for human readers to be able to readily recognise them. This, then, is a great CV challenge - can the model learn the character feature representations well enough to tell that, though they look <em>somewhat</em> different, this sketch from 1962 and this from 2008 both show Reggie?</p>

<p style="text-align: justify">Thirdly, they have a large cast of characters - over two dozen recurring characters, easily. This makes it especially interesting for classification-type tasks. Fourthly, the frequency of appearance of these characters is wildly different, with those of the lead characters often orders of magnitude higher than some of the niche characters. This makes the dataset very imbalanced…and even more intriguing!</p>

<p style="text-align: justify">So let’s look at the dataset, which can be found <a href="https://www.kaggle.com/datasets/agneev/archie-comics-multi-class">here</a>, in a little more detail. Before that, an obvious disclaimer - all rights to the images belong to Archie Comic Publications Inc., with the dataset only being used by me for educational purposes. I have created the dataset from clippings from various Archie comic books and newspaper strips, with some minor editing occasionally done to remove the lettering in dialogue boxes, etc.</p>

<h2 id="multi-class-versus-multi-label-">Multi-class versus multi-label <a id="mc-ml"></a></h2>
<p style="text-align: justify">Before anything else, I should mention that I have actually created <em>two</em> datasets - one multi-class, and one multi-label. We will deal with the multi-label dataset later, but since people sometimes get confused about the difference between multi-class and multi-label classification, let me explain this here briefly. Multi-class is when every image has a single label, with the label being one of a number of possible classes. Multi-label is when every image has or can have multiple labels.</p>

<p>An example of multi-class image classification is this:</p>

<p><img src="/agneev-blog/assets/img/img_8_1.png?raw=true" alt="Image_1" width="200&quot;, height=&quot;100" /></p>
<p> &emsp;  &emsp;   &emsp; &emsp;  Cat</p>
<p><img src="/agneev-blog/assets/img/img_8_2.png?raw=true" alt="Image_2" width="200&quot;, height=&quot;100" /></p>
<p> &emsp;  &emsp;   &emsp; &emsp; Dog</p>
<p style="color:grey;font-size:80%;">
© 2022 Agneev Mukherjee
</p>
<p><br />
Simple, right? On the other hand, the images below, with the labels given below them, may be used for multi-<em>label</em> classification:</p>

<p><img src="/agneev-blog/assets/img/img_8_3.jpg?raw=true" alt="Image_3" width="400&quot;, height=&quot;300" />
<br />
[Grass; Sand; Sea; Boats; Sunny]</p>
<p style="color:grey;font-size:80%;">
© 2022 Agneev Mukherjee
</p>
<p><br /><br />
<img src="/agneev-blog/assets/img/img_8_4.jpg?raw=true" alt="Image_4" width="400&quot;, height=&quot;200" />
<br />
[Cars; Bicycle; Buildings; Trees; Lamp post; Street; Grass; Shrubs; Cloudy]</p>
<p style="color:grey;font-size:80%;">
© 2022 Agneev Mukherjee
</p>
<p><br /></p>

<p style="text-align: justify">Of course, the exact labels for the figures will depend on the application, but you get the drift. So the dataset we are dealing with now is multi-class, that is, each picture only has a single label - the name of a particular character - attached to it. Later we will deal with the multi-label dataset, with several characters in each image.</p>

<h2 id="brief-look-at-the-characters-">Brief look at the characters <a id="brief"></a></h2>

<p style="text-align: justify">As I said earlier, Archie comics has a venerable history stretching back over 80 years. While nowhere near as popular (or ubiquitous) as in its heyday, it continues to have legions of fans, with new ‘properties’ like the animated series <a href="https://en.wikipedia.org/wiki/Archie%27s_Weird_Mysteries"><em>Archie’s Weird Mysteries</em></a>, the zombie comics title <a href="https://en.wikipedia.org/wiki/Afterlife_with_Archie"><em>Afterlife with Archie</em></a>, and the TV series <a href="https://en.wikipedia.org/wiki/Riverdale_(2017_TV_series)"><em>Riverdale</em></a> coming out every now and then. There are also different versions of the characters like <a href="https://en.wikipedia.org/wiki/Little_Archie"><em>Little Archie</em></a>, <a href="https://en.wikipedia.org/wiki/The_New_Archies"><em>The New Archies</em></a>, the <a href="https://en.wikipedia.org/wiki/Bad_Boy_Trouble">‘new look’ series</a>, etc.  The ACMC, however, deals with the <em>classic</em> version of the Archie comics characters - as we shall see, there is more than enough varied content in this to satiate us.</p>

<p style="text-align: justify">I was originally planning to provide a fairly detailed description of each character in the dataset, but then realised that this would be superfluous and irrelevant to the task at hand. I therefore redirect you to <a href="https://en.wikipedia.org/wiki/List_of_Archie_Comics_characters">Wikipedia</a> if you would like to know more about them. Here, let’s just see a couple of pix of each character, one ‘old’ and one ‘new’.</p>

<p>Archie Andrews (<em>An archetypal average American teenager</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_5.png?raw=true" alt="Image_5" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_6.png?raw=true" alt="Image_6" width="180&quot;, height=&quot;100" /></p>

<p><br />
Jughead Jones (<em>Archie’s best friend, usually wears a beanie, smart but lazy, girl hater, has an insatiable appetite</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_7.png?raw=true" alt="Image_7" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_8.png?raw=true" alt="Image_8" width="170&quot;, height=&quot;90" /></p>

<p><br />
Betty Cooper (<em>Sweet girl, good at studies and sports, has crush on Archie</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_9.png?raw=true" alt="Image_9" width="200&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_8_10.png?raw=true" alt="Image_10" width="150&quot;, height=&quot;80" /></p>

<p><br />
Veronica Lodge (<em>Rich, spoilt girl, Archie’s crush</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_11.png?raw=true" alt="Image_11" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_12.png?raw=true" alt="Image_12" width="160&quot;, height=&quot;100" /></p>

<p><br />
Reggie Mantle (<em>Archie’s main rival</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_13.png?raw=true" alt="Image_13" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_14.png?raw=true" alt="Image_14" width="170&quot;, height=&quot;100" /></p>

<p style="text-align: justify"><br />
The above are, in my opinion, the 5 most important characters in the ‘Archies Universe’ - and, in fact, the members of the band <a href="https://en.wikipedia.org/wiki/The_Archies">‘The Archies’</a>! The ‘Riverdale gang’, on the other hand, has several other members, among whom the most notable are:</p>

<p>Dilton Doiley (<em>Stereotypical teenage geeky genius</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_15.png?raw=true" alt="Image_15" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_16.png?raw=true" alt="Image_16" width="180&quot;, height=&quot;100" /></p>

<p><br />
Moose Mason (<em>Stereotypical jock, with near-superhuman strength and a meagre intellect</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_17.png?raw=true" alt="Image_17" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_18.png?raw=true" alt="Image_18" width="120&quot;, height=&quot;60" /></p>

<p><br />
Midge Klump (<em>Moose’s girlfriend</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_19.png?raw=true" alt="Image_19" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_20.png?raw=true" alt="Image_20" width="120&quot;, height=&quot;60" /></p>

<p><br />
Ethel Muggs (<em>Chases Jughead</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_21.png?raw=true" alt="Image_21" width="180&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_22.png?raw=true" alt="Image_22" width="100&quot;, height=&quot;60" /></p>

<p><br />
Chuck Clayton (<em>Talented athlete and cartoonist</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_23.png?raw=true" alt="Image_23" width="180&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_24.png?raw=true" alt="Image_24" width="150&quot;, height=&quot;80" /></p>

<p><br />
Nancy Woods (<em>Chuck’s girlfriend</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_25.png?raw=true" alt="Image_25" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_26.png?raw=true" alt="Image_26" width="100&quot;, height=&quot;50" /></p>

<p><br />
The Riverdale gang studies at Riverdale high, whose most important staff members are:</p>

<p>Waldo Weatherbee (<em>The school principal</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_27.png?raw=true" alt="Image_27" width="200&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_8_28.png?raw=true" alt="Image_28" width="150&quot;, height=&quot;80" /></p>

<p><br />
Geraldine Grundy (<em>Usually an English teacher, although often also shown teaching Maths, History and other subjects</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_29.png?raw=true" alt="Image_29" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_30.png?raw=true" alt="Image_30" width="150&quot;, height=&quot;80" /></p>

<p><br />
Mr. Flutesnoot (<em>Usually a science, especially chemistry, teacher</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_31.png?raw=true" alt="Image_31" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_32.png?raw=true" alt="Image_32" width="140&quot;, height=&quot;70" /></p>

<p><br />
Coach Kleats (<em>Head physical education teacher</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_33.png?raw=true" alt="Image_33" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_34.png?raw=true" alt="Image_34" width="120&quot;, height=&quot;60" /></p>

<p><br />
Coach Clayton (<em>Chuck’s father, physical education and history teacher</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_35.png?raw=true" alt="Image_35" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_36.png?raw=true" alt="Image_36" width="120&quot;, height=&quot;60" /></p>

<p><br />
Mr. Svenson (<em>School janitor</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_37.png?raw=true" alt="Image_37" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_38.png?raw=true" alt="Image_38" width="150&quot;, height=&quot;80" /></p>

<p><br />
Ms. Beazley (<em>School cafeteria cook</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_39.png?raw=true" alt="Image_39" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_40.png?raw=true" alt="Image_40" width="150&quot;, height=&quot;80" /></p>

<p><br />
Of the parents of the Riverdale gang, two characters make appearances far more frequently than others:</p>

<p>Hiram Lodge (<em>Veronica’s father</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_41.png?raw=true" alt="Image_41" width="120&quot;, height=&quot;60" />        
<img src="/agneev-blog/assets/img/img_8_42.png?raw=true" alt="Image_42" width="150&quot;, height=&quot;80" /></p>

<p><br />
Fred Andrews (<em>Archie’s father</em>):</p>

<p><img src="/agneev-blog/assets/img/img_8_43.png?raw=true" alt="Image_43" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_44.png?raw=true" alt="Image_44" width="140&quot;, height=&quot;70" /></p>

<p><br />
Being as rich as they are, it is no surprise that the Lodges have a butler, Smithers, who makes semi-regular appearances:</p>

<p><img src="/agneev-blog/assets/img/img_8_45.png?raw=true" alt="Image_45" width="150&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_46.png?raw=true" alt="Image_46" width="140&quot;, height=&quot;70" /></p>

<p><br />
And what is the gang’s favourite hangout? Pop Tate’s Chok’lit Shoppe, of course. Here’s Pop:</p>

<p><img src="/agneev-blog/assets/img/img_8_47.png?raw=true" alt="Image_47" width="160&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_8_48.png?raw=true" alt="Image_48" width="130&quot;, height=&quot;70" /></p>

<p style="text-align: justify"><br />
So that was a round-up of all the major characters in the ACMC dataset. However, the dataset actually has one more class - ‘Others’. This, as you can guess, is a medley of images of random characters, and the aim is for a model to put any images that it cannot classify as a member of any of the other classes into this category. Let’s have a look at some of the images under this heading:</p>

<p><img src="/agneev-blog/assets/img/img_8_49.png?raw=true" alt="Image_49" width="130&quot;, height=&quot;100" />
<img src="/agneev-blog/assets/img/img_8_50.png?raw=true" alt="Image_50" width="130&quot;, height=&quot;100" />
<img src="/agneev-blog/assets/img/img_8_51.png?raw=true" alt="Image_51" width="130&quot;, height=&quot;100" />
<img src="/agneev-blog/assets/img/img_8_52.png?raw=true" alt="Image_52" width="130&quot;, height=&quot;100" />
<img src="/agneev-blog/assets/img/img_8_53.png?raw=true" alt="Image_53" width="120&quot;, height=&quot;100" /></p>

<p><br />
<img src="/agneev-blog/assets/img/img_8_54.png?raw=true" alt="Image_54" width="110&quot;, height=&quot;100" />
<img src="/agneev-blog/assets/img/img_8_55.png?raw=true" alt="Image_55" width="140&quot;, height=&quot;120" />
<img src="/agneev-blog/assets/img/img_8_56.png?raw=true" alt="Image_56" width="140&quot;, height=&quot;120" />
<img src="/agneev-blog/assets/img/img_8_57.png?raw=true" alt="Image_57" width="140&quot;, height=&quot;120" />
<img src="/agneev-blog/assets/img/img_8_58.png?raw=true" alt="Image_58" width="140&quot;, height=&quot;120" /></p>

<p><br />
<img src="/agneev-blog/assets/img/img_8_59.png?raw=true" alt="Image_59" width="160&quot;, height=&quot;120" />
<img src="/agneev-blog/assets/img/img_8_60.png?raw=true" alt="Image_60" width="90&quot;, height=&quot;60" />
<img src="/agneev-blog/assets/img/img_8_61.png?raw=true" alt="Image_61" width="160&quot;, height=&quot;120" />
<img src="/agneev-blog/assets/img/img_8_62.png?raw=true" alt="Image_62" width="110&quot;, height=&quot;80" />
<img src="/agneev-blog/assets/img/img_8_63.png?raw=true" alt="Image_63" width="140&quot;, height=&quot;120" /></p>

<p style="text-align: justify"><br />
Archie comics fans will recognise several familiar faces in that gallery: Jughead’s father, Gaston, Archie’s mother, Cheryl Blossom, Betty’s father, Jellybean, and Ms. Haggly. The other images are of non-recurring characters.</p>

<h2 id="conclusion-">Conclusion <a id="conc"></a></h2>

<p style="text-align: justify">Looking at the pictures above serves to highlight some of the challenges of working with this dataset. I used a range of materials to make the dataset, collected over a long period, and hence the images vary widely in size and image quality. How did I decide whether an image belongs in the dataset or not? The primary criterion was that a ‘human expert’, in this case an Archie comics fan, should be able to look at the image in question and identify it without any further cues. In some cases, this is difficult - without prior knowledge, for example, it is hard to categorise the Midge images above as belonging to the same person. Still, this is better than if I had put Jughead’s mother as a category - believe it or not, both the images below are of her. These images actually appear in the ‘Others’ category, meaning that their belonging to the same character is moot.</p>

<p><img src="/agneev-blog/assets/img/img_8_64.png?raw=true" alt="Image_64" width="180&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_8_65.png?raw=true" alt="Image_65" width="120&quot;, height=&quot;60" /></p>

<p style="text-align: justify"><br />
Another reason for the dataset being challenging - well, a look at some Jughead images below should suffice…</p>

<p><br />
<img src="/agneev-blog/assets/img/img_8_66.png?raw=true" alt="Image_66" width="100&quot;, height=&quot;100" />  
<img src="/agneev-blog/assets/img/img_8_67.png?raw=true" alt="Image_67" width="100&quot;, height=&quot;100" />  
<img src="/agneev-blog/assets/img/img_8_68.png?raw=true" alt="Image_68" width="100&quot;, height=&quot;100" />  
<img src="/agneev-blog/assets/img/img_8_69.png?raw=true" alt="Image_69" width="100&quot;, height=&quot;100" />  
<img src="/agneev-blog/assets/img/img_8_70.png?raw=true" alt="Image_70" width="100&quot;, height=&quot;100" />  
<img src="/agneev-blog/assets/img/img_8_71.png?raw=true" alt="Image_71" width="100&quot;, height=&quot;100" /></p>

<p style="text-align: justify"><br />
The images above can all easily be identified by any Jughead fan, but for a ML model, it might not be as straightforward.</p>

<p style="text-align: justify">And finally, as I said at the start, the dataset is also very imbalanced, which introduces its own challenges - and provides the opportunity to test some novel techniques. We shall look at this and other aspects in detail next time, so goodbye for now!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-4/">&#8672;&nbsp;7. Modelling water bodies - 4</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/ACMC-simple-model/">9. Diving into the Archie Comics Multiclass dataset&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;]" /><category term="Created dataset" /><summary type="html"><![CDATA[&#8672;&nbsp;7. Modelling water bodies - 4 9. Diving into the Archie Comics Multiclass dataset&nbsp;&#8674;]]></summary></entry><entry><title type="html">7. Modelling water bodies - 4</title><link href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-4/" rel="alternate" type="text/html" title="7. Modelling water bodies - 4" /><published>2022-03-10T12:00:00+01:00</published><updated>2022-03-10T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-4</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-4/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-3/">&#8672;&nbsp;6. Modelling water bodies - 3</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Archie-comics-multiclass-dataset-intro/">8. Introducing the Archie Comics Multiclass dataset&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Data Science">Data Science</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Time series">Time series</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Tabular">Tabular</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
Environment, Water, Kaggle, Competition, Matplotlib
</p>
<p><br /></p>

<p style="text-align: justify">Welcome to the last post in the water body modelling series! The <a href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-3/">last time</a> we saw the modelling of the Aquifer Petrignano using a Random Forest, a LightGBM and an LSTM model, for two different forecast periods. We also saw how to analyse their feature importance via permutation importance and SHAP. It would be redundant (and boring!) to look at all that again for each of the nine water bodies, and so in this post I will just highlight some of the interesting points that arose for the other water bodies. As I say each time, you can find all the code <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/acea-submission-code.ipynb">here</a>.</p>

<h2 id="aquifer-auser--">Aquifer Auser  <a id="Auser"></a></h2>

<p>As per the <a href="https://www.kaggle.com/c/acea-water-prediction/data">competition page</a>,</p>

<blockquote style="text-align: justify">
  <p>This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.</p>
</blockquote>
<blockquote style="text-align: justify">

  <p>The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.</p>
</blockquote>

<p style="text-align: justify">So what is interesting about the modelling of this particular water body? Let us examine the hydrometry chart:</p>

<p><img src="/agneev-blog/assets/img/img_7_1.png?raw=true" alt="Image_1" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">Here we see one aspect that differs from the Petrignano dataset. It appears that the following a long break, the Piaggione hydrometry level has been moved down, in relation to both its pre-break levels and Monte_S_Quirico (the black series) levels. This most likely indicates an error. It would probably be sensible to move it up by the difference in the minima of the pre-and post-break values. The other option would be to move the first part down, but since all the other hydrometry terms lie in the positive region, I went with the moving up option. In any case, it’s the inconsistency in the before and after value that is important; the models will be able to handle the absolute values.</p>

<p style="text-align: justify">The post-break appears to start around early 2011, so first let’s verify this. Let’s get the row number:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2011-01-01'</span><span class="p">]</span></code></pre></figure>

<font size="2">
<div style="overflow:auto;">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
       <th>Date</th>
       <th>Rainfall_Gallicano</th>
       <th>Rainfall_Pontetetto</th>
       <th>Rainfall_Monte_Serra</th>
       <th>Rainfall_Orentano</th>
       <th>Rainfall_Borgo_a_Mozzano</th>
       <th>Rainfall_Piaggione</th>
       <th>Rainfall_Calavorno</th>
       <th>Rainfall_Croce_Arcana</th>
       <th>Rainfall_Tereglio_Coreglia_Antelminelli</th>
       <th>Temperature_Monte_Serra</th>
       <th>Temperature_Ponte_a_Moriano</th>
       <th>Temperature_Lucca_Orto_Botanico</th>
       <th>Volume_POL</th>
       <th>Volume_CC1</th>
       <th>Volume_CC2</th>
       <th>Volume_CSA</th>
       <th>Volume_CSAL</th>
       <th>Hydrometry_Monte_S_Quirico</th>
       <th>Hydrometry_Piaggione</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <th>1363</th>
        <td>2011-01-01</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.2</td>
        <td>3.6</td>
        <td>6.4</td>
        <td>4.9</td>
        <td>-10080.64516</td>
        <td>-16485.10963</td>
        <td>-13875.84</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.5</td>
        <td>-0.33</td>
      </tr>
    </tbody>
  </table>
</div>
</font>

<p><br />
‘Hydrometry_Piaggione’ is the last column in the table, so:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1355</span><span class="p">:</span><span class="mi">1370</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_7_2.png?raw=true" alt="Image_2" width="400&quot;, height=&quot;400" /></p>

<p style="text-align: justify">We can see that there is indeed a bunch of NaNs here, so let us now move up the 2nd part by the difference between the minima of the 1st and 2nd parts, and then re-plot.</p>

<p><img src="/agneev-blog/assets/img/img_7_3.png?raw=true" alt="Image_3" width="400&quot;, height=&quot;200" /></p>

<p>This looks much better, and after imputation we get:</p>

<p><img src="/agneev-blog/assets/img/img_7_4.png?raw=true" alt="Image_4" width="400&quot;, height=&quot;200" /></p>

<p>This looks just fine.</p>

<p style="text-align: justify">Another important aspect is the ensembling. Let us look at the results of the three individual models and the ensemble:</p>

<p><img src="/agneev-blog/assets/img/img_7_5.png?raw=true" alt="Image_5" width="800&quot;, height=&quot;400" /></p>

<p style="text-align: justify">Here we see the benefit of using a range of models. The LSTM, which had given the best predictions for Petrignano, gives the worst results for Auser, but ensembling enables us to mitigate the inaccuracy, even if the final predictions are still worse than those given by the tree-based methods.</p>

<p><br /></p>
<h2 id="aquifer-doganella--">Aquifer Doganella  <a id="Doganella"></a></h2>

<blockquote style="text-align: justify">
  <p>The wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, …, Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes.</p>
</blockquote>

<p>The benefits of ensembling are demonstrated even better in this dataset. First let us see the results for each term, using code like:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lgbm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lstm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">avg_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_7_6.png?raw=true" alt="Image_6" width="500&quot;, height=&quot;200" /></p>

<p style="text-align: justify">We see that each model gives the best prediction for 3 targets out of 9, and the ensemble is able to give more accurate predictions on the whole than is the case for any individual model. This can be crosschecked by looking at the uniform average errors below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'uniform_average'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lgbm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'uniform_average'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lstm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'uniform_average'</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">avg_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'uniform_average'</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_7_7.png?raw=true" alt="Image_7" width="150&quot;, height=&quot;100" /></p>

<p><br /></p>
<h2 id="aquifer-luco--">Aquifer Luco  <a id="Luco"></a></h2>

<blockquote style="text-align: justify">
  <p>The Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes.</p>
</blockquote>

<p style="text-align: justify">The main thing I want to show about aquifer Luco is the poor quality of some of the data. Have a look at the ‘Depth to Groundwater’ data:</p>

<p><img src="/agneev-blog/assets/img/img_7_8.png?raw=true" alt="Image_8" width="400&quot;, height=&quot;300" /></p>

<p>Imputing with MissForest gives:</p>

<p><img src="/agneev-blog/assets/img/img_7_9.png?raw=true" alt="Image_9" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">This looks fairly reasonable, but is it? The large chunks of missing data means there is no way to tell. Imputing over such a large span of time is unwise, and should be avoided unless there is no alternative. In either case, the lack of data for these terms made forecasting for this water body difficult, with the best error obtained (0.466 for the ensemble) amounting to well over 5% error.</p>

<p><br /></p>
<h2 id="water-spring-amiata--">Water spring Amiata  <a id="Amiata"></a></h2>

<blockquote style="text-align: justify">
  <p>The Amiata waterbody is composed of a volcanic aquifer not fed by rivers or lakes but fed by meteoric infiltration. This aquifer is accessed through Ermicciolo, Arbure, Bugnano and Galleria Alta water springs. The levels and volumes of the four sources are influenced by the parameters: rainfall, depth to groundwater, hydrometry, temperatures and drainage volumes.</p>
</blockquote>

<p style="text-align: justify">Modelling the water springs follows essentially the same format as the aquifers, except that the type of input variables is a little different (flow rate in place of volume and hydrometry). Let us just look at the flow rate term.</p>

<p><img src="/agneev-blog/assets/img/img_7_10.png?raw=true" alt="Image_10" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">The flow rates refer to, as expected, the water flow rate of the spring. Looking at the above graphs, two things stand out - the 0s are likely to be errors, and since the data are in the form of a continuous series, rolling means are an appropriate interpolation method. Imputation was therefore simply done by:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">flows</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">flows</span><span class="p">):</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">flows</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># 0s in flows are errors, so replace them with Nans</span></code></pre></figure>

<p>There isn’t much else that is different here, so let’s move on to the next water body.</p>

<p><br /></p>
<h2 id="water-spring-lupa--">Water spring Lupa  <a id="Lupa"></a></h2>

<blockquote style="text-align: justify">
  <p>This water spring is located in the Rosciano Valley, on the left side of the Nera river. The waters emerge at an altitude of about 375 meters above sea level through a long draining tunnel that crosses, in its final section, lithotypes and essentially calcareous rocks. It provides drinking water to the city of Terni and the towns around it.</p>
</blockquote>

<p>A cursory look at this dataset is enough to show that it is different:</p>

<p><img src="/agneev-blog/assets/img/img_7_11.png?raw=true" alt="Image_11" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify"><br />
That’s right, a single ‘X’ or exogenous term - ‘Rainfall_Terni’, and a single ‘Y’ or target term - ‘Flow_Rate_Lupa’. As if that wasn’t enough, look at the rainfall term:</p>

<p><img src="/agneev-blog/assets/img/img_7_12.png?raw=true" alt="Image_12" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">The values of Rainfall_Terni are given as monthly averages for the period from 2009 to 2019, but as daily data in 2020. This means that the test data is considerably different from the training data, further compounded by one day in 2020 receiving 76 mm of rainfall, by far the highest value. To be able to model this dataset, I had to replace the 2020 daily rainfall data by the monthly average. First I located the row at which 2020 started:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rain_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3965</span><span class="p">:</span><span class="mi">3970</span><span class="p">]</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_7_13.png?raw=true" alt="Image_13" width="200&quot;, height=&quot;100" /></p>

<p>Then I made a new dataframe for the 2020 rainfall:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rains_2020</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3966</span><span class="p">:]</span></code></pre></figure>

<p>Replace by the monthly average:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rains_2020</span><span class="p">[</span><span class="s">'Rainfall_Terni'</span><span class="p">]</span> <span class="o">=</span> <span class="n">rains_2020</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s">'Date'</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s">'M'</span><span class="p">)).</span><span class="n">transform</span><span class="p">(</span><span class="s">'mean'</span><span class="p">)</span></code></pre></figure>

<p>Insert the newly averaged terms back into the original dataframes and plot the new data:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rain_df</span><span class="p">.</span><span class="n">Rainfall_Terni</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3966</span><span class="p">:]</span> <span class="o">=</span> <span class="n">rains_2020</span><span class="p">.</span><span class="n">Rainfall_Terni</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:]</span>

<span class="n">train</span><span class="p">.</span><span class="n">Rainfall_Terni</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3966</span><span class="p">:]</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">Rainfall_Terni</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3966</span><span class="p">:]</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Rainfall (mm)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_7_14.png?raw=true" alt="Image_14" width="400&quot;, height=&quot;200" /></p>

<p>This now looks quite reasonable…</p>

<p>What about the other term, the one we are supposed to predict for?</p>

<p><img src="/agneev-blog/assets/img/img_7_15.png?raw=true" alt="Image_15" width="400&quot;, height=&quot;200" /></p>

<p>I think you can see why this waterbody was the cause of a lot of anguished comments on the competition forums…</p>

<p><br /></p>
<h2 id="water-spring-madonna-di-canneto--">Water spring Madonna di Canneto  <a id="Madonna"></a></h2>

<blockquote style="text-align: justify">
  <p>The Madonna di Canneto spring is situated at an altitude of 1010m above sea level in the Canneto valley. It does not consist of an aquifer and its source is supplied by the water catchment area of the river Melfa.</p>
</blockquote>

<p style="text-align: justify">Speaking of the data causing anguish, I will just show a couple of graphs for this water spring and leave it at that:</p>

<p><img src="/agneev-blog/assets/img/img_7_16.png?raw=true" alt="Image_16" width="400&quot;, height=&quot;200" /></p>

<p>That’s not too bad, but this…</p>

<p><img src="/agneev-blog/assets/img/img_7_17.png?raw=true" alt="Image_17" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">I am actually quite fond of this graph, as it is something I like to present as an ‘Exhibit A’ of bad data…</p>

<p><br /></p>
<h2 id="river-arno--">River Arno  <a id="Arno"></a></h2>

<blockquote style="text-align: justify">
  <p>Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Arno results to be the main source of water supply of the metropolitan area of Florence-Prato-Pistoia. The availability of water for this waterbody is evaluated by checking the hydrometric level of the river at the section of Nave di Rosano.</p>
</blockquote>

<p style="text-align: justify">This is the only river in the competition, and is actually entirely unremarkable, so much so that I did not make a single specific comment on it in my competition submission notebook. Perhaps the most noteworthy thing would be the rainfall graph below, showing the different rainfall terms having different years of missing data…</p>

<p><img src="/agneev-blog/assets/img/img_7_18.png?raw=true" alt="Image_18" width="400&quot;, height=&quot;200" /></p>

<p><br /></p>
<h2 id="lake-bilancino--">Lake Bilancino  <a id="Bilancino"></a></h2>

<blockquote style="text-align: justify">
  <p>Bilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river.</p>
</blockquote>

<p style="text-align: justify">This is the final water body in the competition dataset, and the only lake. Its modelling is largely similar to the rest, except that it has a ‘Lake_Level’ term, which is one of the targets, and the ‘Flow_Rate’, the other target, differs in meaning from the water spring flow rates.</p>

<p>First, the lake level looks like this:</p>

<p><img src="/agneev-blog/assets/img/img_7_19.png?raw=true" alt="Image_19" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">It exhibits a distinct degree of seasonality, and hence I used seasonal interpolation for data imputation, similarly to temperature.</p>

<p><img src="/agneev-blog/assets/img/img_7_20.png?raw=true" alt="Image_20" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">The above shows the flow rate, which, as I said, is different from what it meant in the water springs. Here it refers to the flow rate at the water withdrawal station, and hence is comparable with the volume term in the aquifers. In other words, it can be 0, and therefore imputation can only be done by filling with 0s.</p>

<p><br /></p>
<h2 id="conclusion--">Conclusion  <a id="Conc"></a></h2>

<p style="text-align: justify">And that’s it! It’s been a long trip through the different water bodies and the quirks associated with their modelling, but worthwhile, I hope. One of the frustrations of this competition was that the winning submissions were never released, preventing people from learning from these. I quite liked my own submission, except that it was very long (like these posts…), and in hindsight, I would have skipped some of the figures for the latter waterbodies to reduce the length and the monotony. Oh well, I at least hope the organisers got what they wanted from the competition. I will leave you with my final words from my competition entry, which I think still sum up my feelings well:</p>

<blockquote style="text-align: justify">
  <p>This notebook presents models for forecasting the target terms for four different types of waterbodies, with the ensembling of random forest, gradient boosting and neural networks providing accurate and reliable predictions for any desired time period. Overall, this competition was a great learning experience, and I would like to thank the organisers for this. One issue is regarding the quality of data, which for some waterbodies, especially water springs Lupa and Madonna di Canneto, were scarce and of poor quality, severely hampering the quality of the model predictions. Ultimately, no amount of sophistication in data imputation and modelling can compensate for a lack of appropriate and sufficient data, and therefore addressing this should be the first step taken towards being able to better forecast water availability. Overall, though, this analytics competition was a great initiative, and I hope that the outcome of the competition will not only help Acea better manage its waterbodies, but will also spur a general interest in applying such techniques to water management.</p>
</blockquote>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-3/">&#8672;&nbsp;6. Modelling water bodies - 3</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Archie-comics-multiclass-dataset-intro/">8. Introducing the Archie Comics Multiclass dataset&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Data Science&quot;, &quot;Machine Learning&quot;, &quot;Python&quot;, &quot;Time series&quot;, &quot;Tabular&quot;]" /><category term="Environment" /><category term="Water" /><category term="Kaggle" /><category term="Competition" /><category term="Matplotlib" /><summary type="html"><![CDATA[&#8672;&nbsp;6. Modelling water bodies - 3 8. Introducing the Archie Comics Multiclass dataset&nbsp;&#8674;]]></summary></entry><entry><title type="html">5. Modelling water bodies - 2</title><link href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-2/" rel="alternate" type="text/html" title="5. Modelling water bodies - 2" /><published>2022-03-07T12:00:00+01:00</published><updated>2022-03-07T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-2</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-2/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-1/">&#8672;&nbsp;4. Modelling water bodies - 1</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-3/">6. Modelling water bodies - 3&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Data Science">Data Science</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Time series">Time series</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Tabular">Tabular</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
Environment, Water, Pandas, Kaggle, Competition, Imputation, Matplotlib
</p>
<p><br /></p>

<p><em>[Edit: This and the next post were originally published as one, but I have now split them to improve readability.]</em></p>

<p style="text-align: justify">Welcome back. The <a href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-1/">last time</a> we saw the details of the methodology that I used to tackle the <a href="https://www.kaggle.com/c/acea-water-prediction/overview">Acea Smart Water Analytics</a> competition. Now, let’s see the actual modelling of the water bodies. While the notebook containing all the code (available <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/acea-submission-code.ipynb">here</a>) deals with all the waterbodies, the procedure is repetitive, and so we will look in detail in this post at only one representative water body. As you will see, there’s plenty to unpack for that one!</p>

<p style="text-align: justify">Before we start, the competition’s <a href="https://www.kaggle.com/c/acea-water-prediction/data">data page</a> helpfully points out one of the challenges involved in the modelling:</p>

<blockquote style="text-align: justify">
  <p>It is of the utmost importance to notice that some features like rainfall and temperature, which are present in each dataset, don’t go alongside the date. Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. This means, for instance, that rain fell on 1st January doesn’t affect the mentioned features right the same day but some time later. As we don’t know how many days/weeks/months later rainfall affects these features, this is another aspect to keep into consideration when analyzing the dataset.</p>
</blockquote>

<p style="text-align: justify">OK, keeping that in mind, let us get to the modelling. This post will deal with the <a href="https://en.wikipedia.org/wiki/Data_wrangling">data wrangling/munging</a> aspects, with the actual modelling handled next time.</p>

<h2 id="aquifer-petrignano--">Aquifer Petrignano  <a id="Petrignano"></a></h2>

<p style="text-align: justify">The water body I have selected to demonstrate the modelling here is Aquifer Petrignano, one of the four aquifers present in the dataset. As per the stated goal of the competition, this ‘aquifer’ model I built is applicable to each aquifer. Only the initial <a href="https://expressanalytics.com/blog/what-is-data-wrangling-what-are-the-steps-in-data-wrangling/">data wrangling</a> portion requires some manual intervention, while the rest of the process is automated and identical for each aquifer.</p>

<p style="text-align: justify">The competition <a href="https://www.kaggle.com/c/acea-water-prediction/data">data page</a> describes the Petrignano aquifer as follows:</p>

<blockquote style="text-align: justify">
  <p>The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.</p>
</blockquote>

<p style="text-align: justify"><br />
The reason I chose this aquifer to demonstrate is since it has all the column types associated with aquifers (rainfall, depth to groundwater, temperature, volume and hydrometry), and only two target variables, unlike some of the other water bodies. The methods for loading and visualising the data, filling in the missing values, and the various modelling steps are largely similar for all the water bodies, and so I will take the time to explain these in detail.</p>

<p><br /></p>
<h2 id="data-loading-and-visualisation--">Data loading and visualisation  <a id="visual"></a></h2>
<p style="text-align: justify">The first thing to do is to load the dataset. For every water body I loaded the csv file in an identical fashion, to keep things general. As is customary for dealing with tabular data in Python, the <a href="https://pandas.pydata.org/">pandas</a> library has been used for all the data loading.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Load the dataset
</span><span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../input/acea-water-prediction/Aquifer_Petrignano.csv'</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">dayfirst</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Create a copy of the original data, since the 'train' dataframe will be modified
</span><span class="n">train_orig</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span></code></pre></figure>

<p><br />
Let us now look at the start and end of the data, as well as the columns in the dataframe.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">train</span><span class="p">.</span><span class="n">tail</span><span class="p">()</span>
<span class="n">train</span><span class="p">.</span><span class="n">info</span><span class="p">()</span></code></pre></figure>

<font size="2">
<div style="overflow:auto;">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
       <th>Date</th>
       <th>Rainfall_Bastia_Umbra</th>
       <th>Depth_to_Groundwater_P24</th>
       <th>Depth_to_Groundwater_P25</th>
       <th>Temperature_Bastia_Umbra</th>
       <th>Temperature_Petrignano</th>
       <th>Volume_C10_Petrignano</th>
       <th>Hydrometry_Fiume_Chiascio_Petrignano</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <th>0</th>
        <td>2006-03-14</td>
        <td>NaN</td>
        <td>-22.48</td>
        <td>-22.18</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
      </tr>
      <tr>
        <th>1</th>
        <td>2006-03-15</td>
        <td>NaN</td>
        <td>-22.38</td>
        <td>-22.14</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
      </tr>
      <tr>
        <th>2</th>
        <td>2006-03-16</td>
        <td>NaN</td>
        <td>-22.25</td>
        <td>-22.04</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
      </tr>
      <tr>
        <th>3</th>
        <td>2006-03-17</td>
        <td>NaN</td>
        <td>-22.38</td>
        <td>-22.04</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
      </tr>
      <tr>
        <th>4</th>
        <td>2006-03-18</td>
        <td>NaN</td>
        <td>-22.60</td>
        <td>-22.04</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
        <td>NaN</td>
      </tr>
    </tbody>
  </table>
</div>
</font>

<p><br /></p>

<font size="2">
<div style="overflow:auto;">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
       <th>Date</th>
       <th>Rainfall_Bastia_Umbra</th>
       <th>Depth_to_Groundwater_P24</th>
       <th>Depth_to_Groundwater_P25</th>
       <th>Temperature_Bastia_Umbra</th>
       <th>Temperature_Petrignano</th>
       <th>Volume_C10_Petrignano</th>
       <th>Hydrometry_Fiume_Chiascio_Petrignano</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <th>5218</th>
        <td>2020-06-26</td>
        <td>0.0</td>
        <td>-25.68</td>
        <td>-25.07</td>
        <td>25.7</td>
        <td>24.5</td>
        <td>-29930.688</td>
        <td>2.5</td>
      </tr>
      <tr>
        <th>5219</th>
        <td>2020-06-27</td>
        <td>0.0</td>
        <td>-25.80</td>
        <td>-25.11</td>
        <td>26.2</td>
        <td>25.0</td>
        <td>-31332.960</td>
        <td>2.4</td>
      </tr>
      <tr>
        <th>5220</th>
        <td>2020-06-28</td>
        <td>0.0</td>
        <td>-25.80</td>
        <td>-25.19</td>
        <td>26.9</td>
        <td>25.7</td>
        <td>-32120.928</td>
        <td>2.4</td>
      </tr>
      <tr>
        <th>5221</th>
        <td>2020-06-29</td>
        <td>0.0</td>
        <td>-25.78</td>
        <td>-25.18</td>
        <td>26.9</td>
        <td>26.0</td>
        <td>-30602.880</td>
        <td>2.4</td>
      </tr>
      <tr>
        <th>5222</th>
        <td>2020-06-30</td>
        <td>0.0</td>
        <td>-25.91</td>
        <td>-25.25</td>
        <td>27.3</td>
        <td>26.5</td>
        <td>-31878.144</td>
        <td>2.4</td>
      </tr>
    </tbody>
  </table>
</div>
</font>

<p><br />
<img src="/agneev-blog/assets/img/img_5_1.png?raw=true" alt="Image_1" width="500&quot;, height=&quot;100" /></p>

<p style="text-align: justify">We see that the data starts in March 2006 and ends in June 2020, and that there are 7 columns in all: 1 Date, 1 Rainfall, 2 Depth to Groundwater, 2 Temperature, 1 Volume and 1 Hydrometry. We also see that there are lots of missing values, especially at the start of the data - the Date column, which has an entry for each row, has 5223 non-null values, while some of the columns have only 4199. It is clear that missing value imputation will play a key part - we will discuss that in detail later. Before that though, let us make a list of the target columns - a step that obviously must be done manually.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Depth_to_Groundwater_P24'</span><span class="p">,</span> <span class="s">'Depth_to_Groundwater_P25'</span><span class="p">]</span></code></pre></figure>

<p style="text-align: justify"><br />
Next, I decided to remove all the February 29 rows from the data, since otherwise there are issues later on in the modelling process, all years other than leap years having 365 days. Since a leap day only occurs once every 1461 days, there is minimal loss of data. The following cell removes the leap days via masking.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">is_leap_and_29Feb</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">Date</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">year</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> \
           <span class="p">((</span><span class="n">s</span><span class="p">.</span><span class="n">Date</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">year</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">Date</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">year</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span> <span class="o">&amp;</span> \
           <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">Date</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">month</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">Date</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">day</span> <span class="o">==</span> <span class="mi">29</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">is_leap_and_29Feb</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span></code></pre></figure>

<p style="text-align: justify"><br />
OK, the next step is removing those rows at the start of the dataframe that do not have values for one or more of the target variables. Why is this needed? While it is not the case for Aquifer Petrignano, there are some datasets where target values are not available for several years at the beginning. Imputing values and using these rows for model training is incorrect, as the model will learn to predict the imputed data instead of fulfilling the aim of matching the actual data. I therefore wrote the cell below to find out the maximum length of NaNs before a valid value for any of the target columns, and remove all the dataframe rows before this value. Any missing values in the targets that occur later in the dataframe will be imputed. Doesn’t this contradict what I just said about imputation being inappropriate? Well, the gaps in data in the middle are a lot less numerous than the large gaps in the beginning, and so imputing these is an acceptable trade-off here.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nan_len</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">):</span>
    <span class="n">nan_len</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">notna</span><span class="p">().</span><span class="n">idxmax</span><span class="p">())</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">nan_len</span><span class="p">):].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">Let us now see how the data looks visually, which will help determine if any data wrangling other than missing value imputation is needed. I should note here that I used a light touch to data wrangling, only making any changes to the data where I felt it was both necessary and justified.</p>

<p style="text-align: justify">The cell below creates separate dataframes for each type of variable (e.g. rainfall, volume, etc.), both for ease of plotting and because they come in handy later in the modelling process. A list of the columns corresponding to that variable type is also created for later use.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rain_df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Rainfall.*"</span><span class="p">))</span>
<span class="n">rains</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">rain_df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span>

<span class="n">vol_df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Volume.*"</span><span class="p">))</span>
<span class="n">vols</span> <span class="o">=</span> <span class="n">vol_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">vol_df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span>

<span class="n">temp_df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Temperature.*"</span><span class="p">))</span>
<span class="n">temps</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">temp_df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span>

<span class="n">depth_df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Depth_to_Groundwater.*"</span><span class="p">))</span>
<span class="n">depths</span> <span class="o">=</span> <span class="n">depth_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">depth_df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span>

<span class="n">hydro_df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Hydrometry.*"</span><span class="p">))</span>
<span class="n">hydros</span> <span class="o">=</span> <span class="n">hydro_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">hydro_df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span></code></pre></figure>

<p style="text-align: justify"><br />
Now let us plot the different variables (using <a href="https://matplotlib.org/">Matplotlib</a>, as ever). You will notice that all the plot codes are contained inside a if statement that checks if the particular type of variable exists. If, for instance, there is no hydrometry term, the length of the hydrometry list created earlier will be 0, and the code will not run. This takes care of different datasets not having all the different variable types (a recurrent problem), ensuring that the model runs smoothly and automatically for every dataset.</p>

<p>First, the rainfall:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">rain_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Rainfall (mm)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_5_2.png?raw=true" alt="Image_2" width="400&quot;, height=&quot;200" /></p>

<p style="text-align: justify">We can see that for the rainfall, the graph is as expected. Most of the days receive no rainfall, the average rainy day sees perhaps 10-20 mm of rain, while there are a few exceptional days that have values greater than 50 mm. Note the X-axis; the rainfall data is only available from 2009.</p>

<p>Next, the ‘volume’:
<br /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vols</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">vols</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">vol_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">vols</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">vol_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">vols</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Volume (m$^3$)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify"><img src="/agneev-blog/assets/img/img_5_3.png?raw=true" alt="Image_3" width="400&quot;, height=&quot;200" />
<br />
The volume term refers to the amount of water taken from the drinking water treatment plant, a critical parameter in determining its health. Here, we see that most of the days had water extraction of roughly 30000 m<sup>3</sup>, with no clear trends visible.</p>

<p><br /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Temperature ($^o$C)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_5_4.png?raw=true" alt="Image_4" width="400&quot;, height=&quot;200" />
<br /></p>

<p style="text-align: justify">The two temperature terms show a seasonal trend, with maximum temperatures of around 30 <sup>o</sup>C towards the middle of the year, and a minimum value of around 0 <sup>o</sup>C around the turn of the year. This is as expected; after all seasons are virtually defined by the rise and fall in temperatures. This seasonality will be used for imputing missing temperature data.</p>

<p><br /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">depth_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">depth_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Depth to groundwater (m)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify"><img src="/agneev-blog/assets/img/img_5_5.png?raw=true" alt="Image_5" width="400&quot;, height=&quot;200" />
<br />
‘Depth to groundwater’ refers to the groundwater level measured in terms of distance from the ground floor as detected by a piezometer. As the target variables for aquifers are the depth to groundwater terms, this is obviously a very important variable category. The two depth to groundwater terms in this dataset follow each other very closely, and both show a highly haphazard pattern, with the values dropping and rising by large amounts at seemingly random intervals.</p>

<p><br /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hydros</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">hydros</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">hydro_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">hydros</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">hydro_df</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">hydros</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Hydrometry (m)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify"><img src="/agneev-blog/assets/img/img_5_6.png?raw=true" alt="Image_6" width="400&quot;, height=&quot;200" />
<br />
Finally, we come to the hydrometry term, which is also a measure of the groundwater level, but as determined by a hydrometric station rather than using a piezometer. We see from the figure that this variable is more stable than the depth to groundwater terms, and it exhibits a mild seasonal pattern, far less pronounced than for temperatures.</p>

<p><br /></p>
<h2 id="data-imputation--">Data imputation  <a id="impute"></a></h2>

<p style="text-align: justify">As I said earlier, data imputation plays a major part in tackling this competition dataset. Different imputation methods are suitable for the different variable types, as will be seen below. An additional point is that the imputation should occur not just for missing values, but also to replace erroneous data, if these can be definitively identified.</p>

<p style="text-align: justify">Let us start with rainfall. For rainfalls, since 0 is a legitimate value, and is in fact the <a href="https://en.wikipedia.org/wiki/Mode_(statistics)">mode</a> of the data, and since there is no theoretical upper limit to the amount of rain received, the only way any data can be identified as wrong is in the case of negative values or values perhaps a hundred times greater than the rest. This is not the case for any of the datasets, and hence we can focus simply on imputing the missing data. Using the monthly average of the rainfall may be one possible method, but as we saw in the rainfall graph above, there is no seasonality in the data, and hence using monthly averages to impute long stretches of time will probably bias the data. As per the <a href="https://en.wikipedia.org/wiki/KISS_principle">KISS principle</a>, complexity should be avoided unless necessary. In the case of rainfall, inserting the mode value of 0 (i.e. no rainfall on that day) in place of the missing data is the simplest solution, and in the absence of any better alternatives, I decided to use this.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">):</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">rains</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify">Next, the volume. As mentioned above, the volume term refers to the amount of water taken from the drinking water treatment plant. It can therefore have a value of 0 if no water is taken on that day. The same points as made for rainfall also hold for volume, and therefore I did the imputation the same way too.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vols</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">vols</span><span class="p">):</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">vols</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">vols</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify">We have seen that the temperature terms show a seasonal trend as expected, and I used this for imputation. The invalid data is a little hard to identify here, given that the data correctly has positive, negative and zero values. Again, values far outside the range of the rest of the data may be erroneous, and a statistical measure like the <a href="https://en.wikipedia.org/wiki/Standard_score">z-score</a> may be used to identify these outliers. An inspection of the minimum and maximum values of the temperature data for all the datasets did not turn up anything untoward, and hence I decided to skip further statistical inspection. Another source of erroneous data, however, does appear to exist - long stretches of time with values of 0, likely indicating corrupt data. Somewhat arbitrarily, I decided that 7 or more consecutive values of 0 were errors, and decided to mask and replace these with NaNs for imputation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">()</span><span class="o">-</span><span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">().</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">a</span><span class="p">).</span><span class="n">ffill</span><span class="p">().</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">7</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">mask</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">mask</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Replacing 7 or more 0 temperature values with Nans
# Taken from
#https://stackoverflow.com/questions/42946226/replacing-more-than-n-consecutive-values-in-pandas-dataframe-column   </span></code></pre></figure>

<p><br />
That done, we can now turn to the actual data imputation, which is done in the cell below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">temp_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">X</span><span class="p">[</span><span class="s">'dayofyear'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">].</span><span class="n">dt</span><span class="p">.</span><span class="n">dayofyear</span>
        <span class="n">X2</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'dayofyear'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">X2</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">True</span><span class="p">,</span> <span class="s">'same_day'</span><span class="p">]</span> <span class="o">=</span>\
        <span class="n">X</span><span class="p">[</span><span class="s">'dayofyear'</span><span class="p">].</span><span class="nb">map</span><span class="p">(</span><span class="n">X2</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'dayofyear'</span><span class="p">)[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> <span class="s">'same_day'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">True</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s">'same_day'</span><span class="p">]</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span>

        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">])</span>

        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">temps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Temperature ($^o$C)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify">Briefly, the average temperature for each day of the year is calculated, and this average is used to fill in missing data for that date. For example, if the temperature value for June 29, 2018 is missing, then the average temperature of June 29 calculated using the rest of the data will be filled in. As we can see from the figures showing the original (left) and the imputed data (right) below, this method works very well. In the original temperature data, you see that the ‘red’ graph has a big stretch of zeroes around 2015, and the updated graph looks much more reasonable in this timespan.</p>

<p><img src="/agneev-blog/assets/img/img_5_4.png?raw=true" alt="Image_4" width="350&quot;, height=&quot;200" />
<img src="/agneev-blog/assets/img/img_5_7.png?raw=true" alt="Image_7" width="350&quot;, height=&quot;200" /></p>

<p style="text-align: justify"><br />
Next, let us turn to the hydrometry term, which also has a weak seasonal pattern, and hence can be imputed similarly to temperature. Unlike all the previous items, however, the 0 values here <em>can</em> be considered as errors - the values cannot suddenly depart from their usual 2-4 m range and become 0. Therefore, I replaced all the 0 values by NaNs. I made a fresh dataframe for the hydro terms from the imputed data, for reasons that I will explain when I touch upon Aquifer Auser. The rest of the procedure is the same as for temperature. The figure below shows that this imputation method gives adequate results for hydrometry, though less realistic than was the case for temperature.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hydros</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">hydros</span><span class="p">):</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">hydros</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># 0s in hydros are errors, so replace them with Nans
</span>    <span class="n">hydro_df2</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="p">(</span><span class="s">"Hydrometry.*"</span><span class="p">))</span>
    <span class="n">hydros</span> <span class="o">=</span> <span class="n">hydro_df2</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">hydro_df2</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">column</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_5_6.png?raw=true" alt="Image_6" width="350&quot;, height=&quot;200" />
<img src="/agneev-blog/assets/img/img_5_8.png?raw=true" alt="Image_8" width="350&quot;, height=&quot;200" /></p>

<p style="text-align: justify">That leaves only one more term to deal with. I have kept the depth of groundwater term for the last, since this is easily the most difficult to impute. Going over all the different datasets, no one method, e.g. mean, rolling mean, fill forward, K-nearest neighbours, seasonal interpolation, etc. worked well for every depth of groundwater series. This left me with two alternatives - either use a bespoke imputation method for each depth of groundwater after manual inspection and analysis, or use a <a href="https://www.bmj.com/content/338/bmj.b2393">multiple imputation method</a>. Multiple imputation methods compensate for the uncertainty in the missing data by creating and combining predictions from several plausible imputed datasets. Their use permits automatic processing, which was one of my aims, and hence I decided to use these for imputing the depth of groundwater data. The question, though, was: which of the several different multiple imputation methods should I use?</p>

<p style="text-align: justify">Again, there is no perfect solution, but some <a href="https://bmjopen.bmj.com/content/3/8/e002847">studies</a>/<a href="https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/">opinions</a> show that the RF-based MissForest algorithm (details can be found <a href="https://towardsdatascience.com/missforest-the-best-missing-data-imputation-algorithm-4d01182aed3">here</a> or <a href="https://www.kaggle.com/lmorgan95/missforest-the-best-imputation-algorithm">here</a>) can outperform alternatives like MICE and Hmisc. We will therefore use this method. The original MissForest algorithm was implemented in R, and sklearns’ IterativeImputer, which <a href="https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html">has an ExtraTreesRegressor estimator</a> similar to MissForest is <a href="https://scikit-learn.org/stable/modules/impute.html#iterative-imputer">currently experimental</a> and not stable. I therefore used the <a href="https://pypi.org/project/missingpy/">missingpy module</a>. I carried out the imputation on a copy of the original dataset to avoid the fitting being influenced by the imputations of the other variables carried out above. Like with the hydrometry term, I treated 0 values as missing data. As the algorithm cannot handle ‘Date’ terms, and they are not necessary for the imputation anyway, I dropped the Date column prior to the run, and then added it back to the imputed dataframe.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">imputer</span> <span class="o">=</span> <span class="n">MissForest</span><span class="p">()</span>
<span class="n">train_copy</span> <span class="o">=</span> <span class="n">train_orig</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">):</span>
        <span class="n">train_copy</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># 0s in depths are errors, so replace them with Nans
</span>    <span class="n">train_copy</span> <span class="o">=</span> <span class="n">train_copy</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_copy</span><span class="p">)</span>
    <span class="n">imputs</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_copy</span><span class="p">)</span>
    <span class="n">imputs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">imputs</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col_list</span><span class="p">)</span>
    <span class="n">imputs</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span><span class="o">=</span> <span class="n">train_orig</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span></code></pre></figure>

<p>The imputed values are used to replace the missing values in the cell below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">depth_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">True</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">]</span> <span class="o">=</span>\
        <span class="n">X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">].</span><span class="nb">map</span><span class="p">(</span><span class="n">imputs</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Date'</span><span class="p">)[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span>
        <span class="n">train</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span> <span class="o">+</span> <span class="s">'_new'</span><span class="p">]</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax1</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'Date'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">depths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colours</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">)</span>
        <span class="n">y_axis</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="n">yaxis</span>
        <span class="n">y_axis</span><span class="p">.</span><span class="n">set_label_text</span><span class="p">(</span><span class="s">'Depth to groundwater (m)'</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p style="text-align: justify"><br />
It must be said that the plot showing the imputed data (below, right) is not the best advertisement for MissForest, as there are some fitted points above and below the actual values. Overall, though, I found MissForest to be the most reliable and generalisable of all the imputation methods I tried on all the different depth of groundwater series, and hence decided to stick with it.</p>

<p><img src="/agneev-blog/assets/img/img_5_5.png?raw=true" alt="Image_5" width="350&quot;, height=&quot;200" />
<img src="/agneev-blog/assets/img/img_5_9.png?raw=true" alt="Image_9" width="350&quot;, height=&quot;200" /></p>

<p><br /></p>
<h2 id="forecast-period--">Forecast period  <a id="Period"></a></h2>

<p style="text-align: justify">After concluding the data inspection and imputation, we are finally ready to look at the actual models. The first thing I had to decide is the time frame for which we want the predictions, i.e. how many days ahead do we want to predict. There was plenty of discussion on this topic in the competition forums, and it was stated by the organisers that the prediction period is difficult to predetermine, but needs to be decided on a case-by-case basis depending on the water body type and other factors. I therefore created a variable called days_ahead that allows the user to specify the desired time period, the model automatically forecasting for that period. The default value I used is 7 days, but I also ran it for 30 days for this waterbody to show how it works and how the predictions are affected by the forecast period.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">days_ahead</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">target_cols</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">train</span><span class="p">[</span><span class="n">target</span> <span class="o">+</span> <span class="s">'_target'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">target</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="n">days_ahead</span><span class="p">)</span>
    <span class="n">target_cols</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span> <span class="o">+</span> <span class="s">'_target'</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># Creating new columns by shifting the target columns by the forecast period
</span><span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="c1"># Required as the shift in the cell above introduces some new NaNs</span></code></pre></figure>

<h2 id="conclusion--">Conclusion  <a id="conc"></a></h2>

<p>All the initial groundwork is finished now, and the actual modelling can be done. As this post is long enough already, we will look at the three different types of models I used in the next post. So long!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-1/">&#8672;&nbsp;4. Modelling water bodies - 1</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-3/">6. Modelling water bodies - 3&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Data Science&quot;, &quot;Machine Learning&quot;, &quot;Python&quot;, &quot;Time series&quot;, &quot;Tabular&quot;]" /><category term="Environment" /><category term="Water" /><category term="Pandas" /><category term="Kaggle" /><category term="Competition" /><category term="Imputation" /><category term="Matplotlib" /><summary type="html"><![CDATA[&#8672;&nbsp;4. Modelling water bodies - 1 6. Modelling water bodies - 3&nbsp;&#8674;]]></summary></entry><entry><title type="html">6. Modelling water bodies - 3</title><link href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-3/" rel="alternate" type="text/html" title="6. Modelling water bodies - 3" /><published>2022-03-07T12:00:00+01:00</published><updated>2022-03-07T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-3</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-3/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-2/">&#8672;&nbsp;5. Modelling water bodies - 2</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-4/">7. Modelling water bodies - 4&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Data Science">Data Science</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Time series">Time series</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Tabular">Tabular</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
Environment, Water, Pandas, Kaggle, Competition, Random Forest, LightGBM, LSTM, SHAP, Matplotlib, Ensemble
</p>
<p><br /></p>

<p><em>[Edit: This and the previous post were originally published as one, but I have now split them to improve readability.]</em></p>

<p style="text-align: justify">Hello! The <a href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-2/">last time</a> we looked at the data wrangling aspects on data for Aquifer Petrignano in the <a href="https://www.kaggle.com/c/acea-water-prediction/overview">Acea Smart Water Analytics</a> competition. Now, let’s tackle the modelling. As a reminder, the complete code is <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/acea-submission-code.ipynb">available on Github</a>, but we will only look at the main bits here. First, let’s look at a Random Forest (RF) model.</p>

<h2 id="random-forest-model--">Random forest model  <a id="RF"></a></h2>

<p style="text-align: justify">Before anything else, we should make a copy of the train dataframe that we have carried out all the data wrangling on, since that dataframe will also be used for the LGBM and LSTM models. The next thing is to carry out the feature engineering needed for a tree-based model to make predictions. This involves calculating the rolling mean for the variables where the cumulative sum is meaningful for modelling (i.e. rainfall and volume - the sum of the daily rainfall over a week can be an important input to the model) and the lag terms where this sum is meaningless (i.e. temperatures, hydrometry and depth of gorundwater - the ‘sum’ of the daily temperature over a week has no utility as such). Based on preliminary tests, separate rolling mean terms for every 30 days till 180 days and four lag terms till 90 days work sufficiently well without being either an excessive or an insufficient number of terms, and hence these are calculated in the cell below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_rf</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rains</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rains</span><span class="p">:</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_030'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">30</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_060'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_090'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">90</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_120'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">120</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_150'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">150</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="s">'_roll_180'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">180</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vols</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vols</span><span class="p">:</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_030'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">30</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_060'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_090'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">90</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_120'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">120</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_150'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">150</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span> <span class="o">+</span> <span class="s">'_roll_180'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rolling</span><span class="p">(</span><span class="mi">180</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temps</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">temps</span><span class="p">:</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="s">'_week_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="s">'_month_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">30</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="s">'_bimonth_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="s">'_quarter_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">90</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hydros</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hydros</span><span class="p">:</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="s">'_week_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="s">'_month_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">30</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="s">'_bimonth_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="s">'_quarter_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">90</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">depths</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="s">'_week_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="s">'_month_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">30</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="s">'_bimonth_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
        <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="s">'_quarter_lag'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">d</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">90</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">If the non-shifted target terms are kept in the dataframe, then the RF focusses on using those to predict the shifted target terms, which is contrary to our aim of getting it to predict based on the input variables. These are therefore dropped in the next cell. Alongside, the lag terms introduce NaNs at the start of the dataframe which will also be removed in the following cell in the same way we saw last time. The dropna removes all rows containing NaN, and the index[0] gives the first row of the resulting dataframe.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">train_rf</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span><span class="mi">1</span>
<span class="c1"># If the target columns are not removed, the RF just focuses on them.
</span><span class="n">train_rf</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_rf</span><span class="p">.</span><span class="n">dropna</span><span class="p">().</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]:].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">Now we are ready to split the dataframe into the train and test sets. Since we are trying to see how good our model is at predicting the future, the final year of data, i.e. from 1/7/2019 to 30/6/2020 will be used as the test set, with all the earlier data being used for training. We will further split the data into X (input) and y (output) terms. All this is done in the following cell.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rf_X</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">train_rf</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">difference</span><span class="p">(</span><span class="n">target_cols</span><span class="p">)]</span>
<span class="n">rf_y</span> <span class="o">=</span> <span class="n">train_rf</span><span class="p">[</span><span class="n">target_cols</span><span class="p">]</span>
<span class="n">train_rf_X</span> <span class="o">=</span> <span class="n">rf_X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">rf_X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">()].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_rf_X</span> <span class="o">=</span> <span class="n">rf_X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">():].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">train_rf_y</span> <span class="o">=</span> <span class="n">rf_y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">rf_y</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">()].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_rf_y</span> <span class="o">=</span> <span class="n">rf_y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rf_y</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">():].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Note that rf_y does not have a Date column, but as it's the same length as rf_X,
# the Date column from that df can be used for the train-test split
</span><span class="n">col_list2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_rf</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">train_rf_X</span> <span class="o">=</span> <span class="n">train_rf_X</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_rf_X</span> <span class="o">=</span> <span class="n">test_rf_X</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># The Date column needs to be removed for the RF regressor to work</span></code></pre></figure>

<p style="text-align: justify"><br />
Now, I want to clarify one thing to prevent any misgivings on the issue. The rolling and lag terms for the first few items of the test set will be based on data from the end of the training set. Is this data leakage? No.</p>

<p style="text-align: justify">Data leakage refers to <a href="https://insidebigdata.com/2014/11/26/ask-data-scientist-data-leakage/">‘when the data you are using to train a machine learning algorithm happens to have the information you are trying to predict’</a>. This is not the case here, as we are not trying to predict any of the rolling or lag terms. Further, we can see in the above link that leakage occurs due to ‘leaking data from the test set into the training set’, ‘leaking the correct prediction or ground truth into the test data’, ‘leaking of information from the future into the past’ and ‘information from data samples outside of scope of the algorithm’s intended use’. None of these is the case here.</p>

<p style="text-align: justify">In short, leakage only occurs if information that will not be available at prediction time is used in model training. All the lag and rolling means terms are in the past and thus will be available at prediction time, and hence this is not a data leak.</p>

<p style="text-align: justify"><br />
Having clarified the above point, we can now run the RF regressor. I maintained the default number of estimators (100), but this can be increased or decreased depending on whether accuracy and robustness or processing time is of greater importance. The max_depth item I kept at 30 to prevent overfitting, while I passed a random_state int value of 2 to <a href="https://stackoverflow.com/questions/39158003/confused-about-random-state-in-decision-tree-of-scikit-learn">ensure consistency across calls</a>. All the other (hyper)parameters were kept at their default. Hyperparameter tuning may lead to somewhat improved accuracy at the cost of increased complexity, run time and perhaps overfitting, and if desired, can be investigated further as per online guides, such as <a href="https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/">this</a> or <a href="https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d">this</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">regr_rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">regr_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_rf_X</span><span class="p">,</span> <span class="n">train_rf_y</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify"><br />
After the initial fit carried out above, it is better to take the training features deemed most important as per permutation importance and refit the RF only on these. This will enable the RF to focus on these important features, rather than spreading its attention, so to speak, among a lot of different features of marginal importance. This is done below. The code is mostly self-explanatory, but <a href="https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb">this reference</a> may be used for better understanding certain portions if needed.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Calculating permutation importance based on the previous RF
</span><span class="n">train_val_rf</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">regr_rf</span><span class="p">,</span> <span class="n">train_rf_X</span><span class="p">,</span> <span class="n">train_rf_y</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sorted_idx_val</span> <span class="o">=</span> <span class="n">train_val_rf</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">.</span><span class="n">argsort</span><span class="p">()</span>

<span class="c1"># Taking the important features from the random forest above
</span><span class="n">var_names</span> <span class="o">=</span> <span class="n">train_rf_X</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sorted_idx_val</span><span class="p">].</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">imps</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">train_val_rf</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">)</span>
<span class="n">var_names</span> <span class="o">=</span> <span class="n">var_names</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">imps</span> <span class="o">=</span> <span class="n">imps</span><span class="p">)</span>
<span class="n">var_names</span> <span class="o">=</span> <span class="n">var_names</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s">"vars"</span><span class="p">})</span>

<span class="c1"># Making a list containing the most important variables. The feature importance value of 0.001
# is arbitrary and can be changed if needed
</span><span class="n">to_keep</span> <span class="o">=</span> <span class="n">var_names</span><span class="p">[</span><span class="n">var_names</span><span class="p">.</span><span class="n">imps</span><span class="o">&gt;</span><span class="mf">0.001</span><span class="p">].</span><span class="nb">vars</span><span class="p">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="c1"># Making new train and test dataframes for the X terms judged important enough to keep
</span><span class="n">train_rf_X</span> <span class="o">=</span> <span class="n">train_rf_X</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">train_rf_X</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">to_keep</span><span class="p">]]</span>
<span class="n">test_rf_X</span> <span class="o">=</span> <span class="n">test_rf_X</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">test_rf_X</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">to_keep</span><span class="p">]]</span></code></pre></figure>

<p style="text-align: justify"><br />
We can now rerun the RF on the new dataframe containing the most important features and obtain the predictions. We can then have a look at the features now deemed most important as per permutation importance, this time using the test data instead of the training data as above.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">regr_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_rf_X</span><span class="p">,</span> <span class="n">train_rf_y</span><span class="p">)</span>
<span class="n">rf_preds_1</span> <span class="o">=</span> <span class="n">regr_rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_rf_X</span><span class="p">)</span>

<span class="n">result_val2</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">regr_rf</span><span class="p">,</span> <span class="n">test_rf_X</span><span class="p">,</span> <span class="n">test_rf_y</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sorted_idx_val2</span> <span class="o">=</span> <span class="n">result_val2</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">.</span><span class="n">argsort</span><span class="p">()</span>

<span class="n">fig1</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">result_val2</span><span class="p">.</span><span class="n">importances</span><span class="p">[</span><span class="n">sorted_idx_val2</span><span class="p">].</span><span class="n">T</span><span class="p">,</span>
           <span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">test_rf_X</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sorted_idx_val2</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Permutation Importances (test set)"</span><span class="p">)</span>
<span class="n">fig1</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_1.png?raw=true" alt="Image_1" width="800&quot;, height=&quot;400" /></p>

<p style="text-align: justify">We can see that the weekly lag terms for the two targets are the most important predictors, which is unsurprising. Among the other inputs, the 30-day rolling mean of the volume term and the 120-day rolling mean of the rainfall term are of relatively major importance. Neither of the temperature terms appears in the list of important features at all, perhaps because they had been dropped when we narrowed down the list of features earlier.</p>

<p style="text-align: justify">Finally, we can calculate the MAE of the predictions. The predictions appear to be reasonably accurate - given that the depth to groundwater terms are in the range of 19 and 35 m, an average absolute error of around 0.3 m is a little over 1%. The RMSE values are also provided below for reference, and are slightly higher than the MAE numbers.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">,</span> <span class="n">test_rf_y</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">,</span> <span class="n">test_rf_y</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_2.png?raw=true" alt="Image_2" width="300&quot;, height=&quot;200" /></p>

<p style="text-align: justify"><br />
We have looked at the most important features in terms of permutation importance above. Let us now see what the SHAP values calculated using SHAP’s TreeExplainer say. First a look at the summary plot for all the target columns combined:</p>

<p><img src="/agneev-blog/assets/img/img_6_3.png?raw=true" alt="Image_3" width="600&quot;, height=&quot;300" /></p>

<p style="text-align: justify">We see that the weekly lag of the two depth to groundwater terms are important predictors for both the target terms, in line with both what the permutation importances showed and our expectations.The 120-day rolling mean of the rainfall is also an important predictor, more for the Depth_to_Groundwater_P24 target than Depth_to_Groundwater_P25 (the blue portion of the bar is longer than the red portion for the Rainfall_Bastia_Umbra_roll_120 term). Three different rolling means of the volume term are of moderate importance, while the weekly lag of the hydrometry term comes much lower down than was the case for the permutation importances. Overall, there is relatively good agreement, but the SHAP plots can provide greater depth of information, as shown below.</p>

<p><br />
Now let us see what the individual SHAP summary plots show.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="c1"># To ensure the code works if only target variable is present
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">shap</span><span class="p">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">test_rf_X</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shap</span><span class="p">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_rf_X</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">+=</span><span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_4.png?raw=true" alt="Image_4" width="600&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_6_5.png?raw=true" alt="Image_5" width="600&quot;, height=&quot;300" /></p>

<p style="text-align: justify">A detailed explanation of how to interpret SHAP summary plots can be found <a href="https://christophm.github.io/interpretable-ml-book/shap.html#shap-summary-plot">here</a>, but looking at the above, it is clear that they present some interesting insights. Consider the Depth_to_Groundwater_P24 figure. We see that the Depth_to_Groundwater_P24_week_lag term has a positive impact on the model output (except for very low values of the feature), while the Depth_to_Groundwater_P25_week_lag has a negative impact (except for very high values of the feature. Thus, we see that very high values of the Depth_to_Groundwater_P24_week_lag (bright red colour) leads to the model output increasing by around 1.2, while for the Depth_to_Groundwater_P25_week_lag term, very high values affect the model output by -0.2 to +0.1. Very low values of this feature (bright blue) reduce the model output by around 1.5. We also see that the Rainfall_Bastia_Umbra_roll_120 term has a small positive effect, but this effect is a bit inconsistent, as a mixture of blue and red values are seen in the 0.1 to 0.5 region. Finally, all the other terms are centred around 0 with limited spread on either side, indicating that it does not much matter to the model predictions whether these terms have a high or low value - in other words, that they are of low importance in prediction.</p>

<p style="text-align: justify">The above thus helps us not only see which features are important, but also how they actually impact the model - whether this impact is positive or negative, and whether high values and low values of the feature affect predictions differently. While the above can be discussed in further depth, in the interests of brevity, let us move on to the LGBM model.</p>

<p><br /></p>
<h2 id="lightgbm-model--">LightGBM model  <a id="LGBM"></a></h2>

<p style="text-align: justify">First things first - the LightGBM algorithm <a href="https://github.com/microsoft/LightGBM/issues/524">does not support multi-output regression</a>, and <a href="https://github.com/dmlc/xgboost/issues/2087">neither does the competing library XGBoost</a>, for that matter. This hurdle can be tackled in two ways. One is to use sklearns’ <a href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html">MultiOutputRegressor</a> as a wrapper. This works very well indeed, but in our case presents one hitch - SHAP does not support MultiOutputRegressor. This brings us to the second alternative - run LGBM for each target term. This would normally be an inferior alternative, as the interactions between the target terms would not be captured. In this case, for instance, if we run the LGBM for the Depth_to_Groundwater_P24 term, dropping the Depth_to_Groundwater_P25 term, and then the other way round, the effect of the Depth_to_Groundwater_P24 term on the Depth_to_Groundwater_P25 term and vice versa would not be accounted for in the model. Here, though, we are using the lag terms of the targets, which are present during the modelling step, and hence the target variables’ interactions <em>are</em> captured in the model. We will therefore use the LGBMRegressor in a for-loop for each of the target variables. We will stick to the default LightGBM parameters in the interest of time, simplicity, and robustness, but more information about parameter tuning can be found <a href="https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html">here</a> and <a href="https://neptune.ai/blog/lightgbm-parameters-guide">here</a>.</p>

<p style="text-align: justify">The train and test sets are prepared in the same way as for RF, reusing the lag and rolling mean terms we had calculated for using in the RF. As a Gradient Boosting method works sequentially, <a href="https://www.displayr.com/gradient-boosting-the-coolest-kid-on-the-machine-learning-block/">progressively improving the model to minimise prediction error</a>, a feature selection step as was carried out for the RF is less important - less important features will anyway be dropped as the algorithm finetunes the predictions. In theory, a lower number of features should allow for lower run time, but the feature selection itself is time consuming, and so I did not carry this out for the LGBM model. Therefore, all we now need to do before running the regressor is to drop the Date columns, and create an empty numpy array to hold the model predictions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">regr_lgbm</span> <span class="o">=</span> <span class="n">LGBMRegressor</span><span class="p">()</span>

<span class="n">train_lgbm_X</span> <span class="o">=</span> <span class="n">rf_X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">rf_X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">()].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_lgbm_X</span> <span class="o">=</span> <span class="n">rf_X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">():].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">train_lgbm_y</span> <span class="o">=</span> <span class="n">rf_y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">rf_y</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">()].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">test_lgbm_y</span> <span class="o">=</span> <span class="n">rf_y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rf_y</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">rf_X</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">():].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Note that rf_y does not have a Date column, but as it's the same length as rf_X,
# the Date column from that df can be used for the train-test split
</span><span class="n">train_lgbm_X</span> <span class="o">=</span> <span class="n">train_lgbm_X</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_lgbm_X</span> <span class="o">=</span> <span class="n">test_lgbm_X</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lgbm_preds_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">test_lgbm_y</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">m1</span> <span class="o">=</span> <span class="n">regr_lgbm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_lgbm_X</span><span class="p">,</span> <span class="n">train_lgbm_y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
    <span class="n">lgbm_preds_1</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">m1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_lgbm_X</span><span class="p">)</span>
    <span class="n">explainer2</span> <span class="o">=</span> <span class="n">shap</span><span class="p">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">regr_lgbm</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer2</span><span class="p">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">test_lgbm_X</span><span class="p">)</span>
    <span class="n">shap</span><span class="p">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">test_lgbm_X</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">+=</span><span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_6.png?raw=true" alt="Image_6" width="600&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_6_7.png?raw=true" alt="Image_7" width="600&quot;, height=&quot;300" /></p>

<p style="text-align: justify">The permutation importances can be calculated for the LGBM similarly to RF, but let us focus on the SHAP summary plots. We see that these are broadly similar to the RF plots, with some differences. In the Depth_to_Groundwater_P24 plot, we see that the impact of the Depth_to_Groundwater_P24 weekly lag term is less pronounced than was the case for RF, with high values increasing model output by less than 1.0. We also see the individual rainfall terms making less of an impact on the model output. These differences are partly a result of the differences in the algorithm, and partly simply due to the greater number of features available to the LGBM regressor, since we did not carry out a feature selection for it. The broad picture, though, remains the same - the weekly lag terms for the depths to groundwater are the most important, the volume and rainfall terms have a part to play, and the hydrometry and temperature terms have a negligible impact on the model output.</p>

<p style="text-align: justify">Looking at the accuracy of the predictions, we see numbers fairly similar to those obtained for RF.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lgbm_preds_1</span><span class="p">,</span> <span class="n">test_lgbm_y</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">lgbm_preds_1</span><span class="p">,</span> <span class="n">test_lgbm_y</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_8.png?raw=true" alt="Image_8" width="300&quot;, height=&quot;200" /></p>

<p><br /></p>
<h2 id="lstm-model--">LSTM model  <a id="LSTM"></a></h2>

<p style="text-align: justify">Finally, we come to the LSTM model. The train-test split I did similarly to the RF and LGBM, but the split into the X and y arrays I did as per the code given <a href="https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/">here</a>. In fact, the LSTM model I used is itself adapted from the ‘LSTM Model With Univariate Input and Vector Output’ section of that page, and hence the explanations given there are applicable here. The only item that needs separate explanation is the scaling step. Scaling is <a href="https://stackoverflow.com/questions/46686924/why-scaling-data-is-very-important-in-neural-networklstm/46688787">very useful</a> in helping NNs like LSTM make quicker and more accurate predictions, but needs to be carried out <a href="https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data">post the train-test split</a> to avoid data leakage, which is what has been done below. Sklearn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">MinMaxScaler</a> has been used for scaling.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Make a copy of the train dataframe
</span><span class="n">train_lstm</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">train_lstm</span> <span class="o">=</span> <span class="n">train_lstm</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_lstm</span><span class="p">.</span><span class="n">dropna</span><span class="p">().</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]:].</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Create train-test split, then drop Date columns
</span><span class="n">train_train_lstm</span> <span class="o">=</span> <span class="n">train_lstm</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">train_lstm</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_lstm</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">train_test_lstm</span> <span class="o">=</span> <span class="n">train_lstm</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_lstm</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_lstm</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2019-07-01'</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">():]</span>
<span class="n">train_train_lstm</span> <span class="o">=</span> <span class="n">train_train_lstm</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_test_lstm</span> <span class="o">=</span> <span class="n">train_test_lstm</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Create a copy of the train test dataframes and perform scaling on thiese dataframes
# Separate scalers used for the train and test sets to prevent the possibility of data leakage
</span><span class="n">train_train_lstm2</span> <span class="o">=</span> <span class="n">train_train_lstm</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">train_test_lstm2</span> <span class="o">=</span> <span class="n">train_test_lstm</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">scaler1</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler2</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">train_train_lstm2</span> <span class="o">=</span> <span class="n">scaler1</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_train_lstm2</span><span class="p">)</span>
<span class="n">train_test_lstm2</span> <span class="o">=</span> <span class="n">scaler2</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_test_lstm2</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">The function below splits the train and test sets into separate arrays containing the X and y terms, and it is run to obtain the inputs and outputs in the shape required for the LSTM. As mentioned above, the portion below is based on the code <a href="https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/">here</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Split a multivariate sequence into samples
</span><span class="k">def</span> <span class="nf">split_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)):</span>
        <span class="c1"># find the end of this pattern
</span>        <span class="n">end_ix</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n_steps</span>
        <span class="c1"># check if we are beyond the dataset
</span>        <span class="k">if</span> <span class="n">end_ix</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="c1"># gather input and output parts of the pattern
</span>        <span class="n">seq_x</span><span class="p">,</span> <span class="n">seq_y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">end_ix</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">target_cols</span><span class="p">)],</span> <span class="n">sequences</span><span class="p">[</span><span class="n">end_ix</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">target_cols</span><span class="p">):]</span>
        <span class="n">X</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_x</span><span class="p">)</span>
        <span class="n">y</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Choose number of time steps used for training. Arbitrary, 7 chosen here.
</span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">7</span>

<span class="c1"># Convert into input/output
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">split_sequences</span><span class="p">(</span><span class="n">train_train_lstm2</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">split_sequences</span><span class="p">(</span><span class="n">train_test_lstm2</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># The number of features
</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_9.png?raw=true" alt="Image_9" width="200&quot;, height=&quot;100" /></p>

<p style="text-align: justify">After the preprocessing above, we are ready to define and run the LSTM model. I fit the model for 200 epochs, as I found this to give good results while taking a reasonable amount of time. If necessary the ideal number of epochs can be figured out as shown <a href="https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/">here</a>, but let’s leave that for now.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_cols</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>

<span class="c1"># Fit model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Demonstrate prediction
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Change X test array dimensions and concatenate with y test and y prediction arrays
</span><span class="n">X_test_new</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span>
<span class="n">test_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X_test_new</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_new2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X_test_new</span><span class="p">,</span> <span class="n">yhat</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Scale back the test arrays
</span><span class="n">test_new</span> <span class="o">=</span> <span class="n">scaler2</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">test_new</span><span class="p">)</span>
<span class="n">test_new2</span> <span class="o">=</span> <span class="n">scaler2</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">test_new2</span><span class="p">)</span>

<span class="c1"># Create target and prediction arrays containing only the target terms, to compare the predictions
</span><span class="n">y_targets</span> <span class="o">=</span> <span class="n">test_new</span><span class="p">[:,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">target_cols</span><span class="p">):]</span>
<span class="n">lstm_preds_1</span> <span class="o">=</span> <span class="n">test_new2</span><span class="p">[:,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">target_cols</span><span class="p">):]</span></code></pre></figure>

<p style="text-align: justify"><br />
Looking at the MAE values, we see that the LSTM model is more accurate for this waterbody than the RF and LGBM models.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">lstm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">lstm_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_10.png?raw=true" alt="Image_10" width="300&quot;, height=&quot;200" /></p>

<p style="text-align: justify"><br />
The SHAP analysis of the LSTM cannot be done by the TreeExplainer like for the tree-based models, and the DeepExplainer available for NNs gives errors like <a href="https://github.com/slundberg/shap/issues/1490">this</a> for LSTMs, and so I used the <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.GradientExplainer.html">GradientExplainer</a> instead. The difference in the SHAP summary plots for LSTM as compared to the earlier plots is obvious - the Depth_to_Groundwater terms are used directly in the LSTM for predictions, and hence high values of these variables give high values for the model output, and the other way around. This shows that the influence of a feature on the prediction depends not only on the variable but also on the type of model used.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="p">.</span><span class="n">GradientExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
<span class="n">shap_values1</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">Xtests</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">shaps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">shap_values1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">n_steps</span><span class="p">)</span>
    <span class="n">shap</span><span class="p">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">shaps</span><span class="p">[</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">Xtests</span><span class="p">[</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                      <span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">train_test_lstm</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">+=</span><span class="mi">1</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_11.png?raw=true" alt="Image_11" width="600&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_6_12.png?raw=true" alt="Image_12" width="600&quot;, height=&quot;300" /></p>

<p><br /></p>
<h2 id="ensembling--">Ensembling  <a id="Ensemble"></a></h2>

<p style="text-align: justify">We have seen that all the models gave quite good results on the Petrignano dataset. Let us now see how an ensemble of these models fares. First we need to reshape the different model prediction arrays to make these all the same shape. Then we take an average of the predicted values, and find the MAE.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">rf_preds_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">delete</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">s_</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lgbm_preds_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">delete</span><span class="p">(</span><span class="n">lgbm_preds_1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">s_</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lstm_preds_1</span> <span class="o">=</span> <span class="n">lstm_preds_1</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rf_preds_1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">avg_preds_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">rf_preds_1</span> <span class="o">+</span> <span class="n">lgbm_preds_1</span> <span class="o">+</span> <span class="n">lstm_preds_1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">avg_preds_1</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s">'raw_values'</span><span class="p">)</span></code></pre></figure>

<p><img src="/agneev-blog/assets/img/img_6_13.png?raw=true" alt="Image_13" width="300&quot;, height=&quot;100" /></p>

<p style="text-align: justify">We see that the ensemble produces results that are slightly inferior to the best model here, the LSTM. Two comments can be made on this. Firstly, we have used a simple average of the predictions here. Using a <a href="https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/">meta model to stack these predictions</a> will <a href="https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/">almost certainly result</a> in more accurate predictions. The downside is that stacked models are <a href="https://towardsdatascience.com/a-practical-guide-to-stacking-using-scikit-learn-91e8d021863d">significantly slower and more computationally expensive</a>. As I had already developed some very elaborate models that took a fair amount of time to run, I decided that a simple average would suffice here instead of adding further complications. The other point is that the benefit of ensembling will be more clearly brought out when I touch upon some of the other water bodies next time.</p>

<p><br /></p>
<h2 id="forecast-period-of-30-days--">Forecast period of 30 days  <a id="Thirty"></a></h2>

<p style="text-align: justify">I reran all the code for a forecast period of 30 days - everything remaining identical, except putting days_ahead = 30. The model ran smoothly and automatically, producing predictions that are slightly less accurate, understandable given that the prediction period is further away:</p>

<p><img src="/agneev-blog/assets/img/img_6_14.png?raw=true" alt="Image_14" width="800&quot;, height=&quot;800" /></p>

<p><br /></p>
<h2 id="conclusion--">Conclusion  <a id="Conc"></a></h2>

<p style="text-align: justify">That pretty much covers everything I had to say about Aquifer Petrignano. The next time, we will take a brief look at some of the interesting points in the modelling of the other water bodies to wind up this series. So long!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-2/">&#8672;&nbsp;5. Modelling water bodies - 2</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-4/">7. Modelling water bodies - 4&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Data Science&quot;, &quot;Machine Learning&quot;, &quot;Python&quot;, &quot;Time series&quot;, &quot;Tabular&quot;]" /><category term="Environment" /><category term="Water" /><category term="Pandas" /><category term="Kaggle" /><category term="Competition" /><category term="Random Forest" /><category term="LightGBM" /><category term="LSTM" /><category term="SHAP" /><category term="Matplotlib" /><category term="Ensemble" /><summary type="html"><![CDATA[&#8672;&nbsp;5. Modelling water bodies - 2 7. Modelling water bodies - 4&nbsp;&#8674;]]></summary></entry><entry><title type="html">4. Modelling water bodies - 1</title><link href="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-1/" rel="alternate" type="text/html" title="4. Modelling water bodies - 1" /><published>2022-02-28T12:00:00+01:00</published><updated>2022-02-28T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-1</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Water-modelling-part-1/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-2/">&#8672;&nbsp;3. Evaluating handwritten Roman numerals datasets - 2</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-2/">5. Modelling water bodies - 2&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Data Science">Data Science</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Time series">Time series</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
Environment, Water, Kaggle, Competition
</p>
<p><br /></p>

<p style="text-align: justify">Hello again! The last few weeks, we have seen the creation and evaluation of handwritten Roman numerals datasets. This time, let us turn our attention to something different - the modelling of water levels in different water bodies.</p>

<p style="text-align: justify">One thing in common between this post and the previous ones is their origin in competitions I participated in. While the Roman numerals datasets were an output of my participation in the <a href="https://https-deeplearning-ai.github.io/data-centric-comp/">Data-Centric AI competition</a>, the present post is based on my writeup for the <a href="https://www.kaggle.com/c/acea-water-prediction/overview">Acea Smart Water Analytics</a> competition organised on Kaggle from December 2020 - February 2021. Unlike the vast majority of Kaggle competitions, this particular competition was an ‘Analytics Competition’, which means that it had no leaderboard and no metric to fit to. Instead, competitors were asked to provide a notebook containing both, the models, and an explanation of the models. These notebooks were judged according to the criteria defined <a href="https://www.kaggle.com/c/acea-water-prediction/overview/evaluation">here</a> - essentially, the soundness of the methodology, the quality of the presentation, and the applicability of the methods.</p>

<p style="text-align: justify">How did I do in this competition? Well, I at least made it to the list of <a href="https://www.kaggle.com/c/acea-water-prediction/discussion/229764">finalists</a>, which was something…then again, finishing in the top 17 out of 103 isn’t <em>that</em> great… In any case, in this series of posts, we will look at how I approached the problem. This post will describe the problem, why it matters, and the methodology I used, while the actual modelling and the results will be covered in future posts. The notebook containing all the code is available <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/acea-submission-code.ipynb">here</a>, while the competition dataset is <a href="https://www.kaggle.com/c/acea-water-prediction/data">here</a>. Please note that if you want to download the dataset, you will need to create a Kaggle account and accept the competition rules.</p>

<p style="text-align: justify">With that said, let’s go!</p>
<p><br /></p>
<h2 id="the-problem--">The problem  <a id="prob"></a></h2>

<p style="text-align: justify">The competition was hosted by the Italian multiutility <a href="https://en.wikipedia.org/wiki/Acea_(company)">Acea SpA</a>, which is involved in the water, energy and environmental sectors. As a water utility, one of the challenges they face is forecasting water body levels, which is important both in terms of ensuring water body health and adequately meeting water demand. This is made harder by the fact that they are in charge of different types of water bodies, with each type having unique characteristics, and so making generalisable models for predicting water levels is very difficult.</p>

<p style="text-align: justify">What the organisers wanted from the competitors therefore were four models that could be applied to one of the four categories of water bodies presented. A total of nine water bodies were present in the data - four aquifers, three water springs, one river and one lake. The aim was to determine how the particular features of a water body category influence the water availability. The organisers helpfully provided the following figure to explain what they were looking for:</p>

<p><br />
<img src="/agneev-blog/assets/img/img_4_1.png?raw=true" alt="Image_1" width="600&quot;, height=&quot;400" /></p>

<p style="text-align: justify"><br />
In reality, the modelling procedure follows a similar pattern for all the water bodies. We will look at that procedure next time - this post, as I said, will focus instead on the methodology used for the modelling.</p>
<p><br /></p>
<h2 id="why-it-matters--">Why it matters  <a id="matter"></a></h2>

<p style="text-align: justify">Before heading into the methodological details, however, I think it might be worth explaining why I decided to enter this competition. Firstly, an analytics competition was appealing since it avoids my pet peeve about Kaggle competitions - a horde of competitors doing whatever it takes to get a 0.01% improvement on the target metric. I might rant on that some other time, but suffice to say that this competition allowed the rather unusual prospect of models being judged on their elegance and real-world applicability. The flip side, of course, was the inherent subjectivity of the evaluation, but then again, this is certainly the case in real life as well.</p>

<p style="text-align: justify">The second reason was that I felt the competition tackled a very important and somewhat underappreciated problem - water scarcity. We have all heard the phrase ‘water is life’, to the point that it has become something of a cliché. And yet, that statement hardly overstates the critical role played by water in our existence. Indeed, it is because water is so ubiquitous, something we all need and use on a daily basis, be it for drinking, bathing, cooking, cleaning, or watering our plants, that we take it for granted, neglecting to think of how we would fare in its absence.</p>

<p style="text-align: justify">Actually, the previous sentence is probably only applicable to a relatively privileged portion of the global population. While precise numbers are hard to calculate, as per some estimates, as many as <a href="https://www.science.org/doi/10.1126/sciadv.1500323">four billion people</a> may be said to be living under conditions of severe water scarcity, a situation that is likely to grow worse if <a href="https://apo.org.au/node/276976">present trends continue</a>. Growing population, combined with an ever-increasing water footprint, means that water demand is likely to continue to rise till at least the middle of the century, which prompts the question - how will this demand be met?</p>

<p style="text-align: justify">Globally, the answer to this question thus far has usually been to simply overexploit water resources. The results of this approach are now plain to see in many parts of the world - <a href="https://www.smithsonianmag.com/science-nature/world-vanishing-lakes-180949645/">disappearing lakes</a>, <a href="https://www.nationalgeographic.com/environment/photos/rivers-run-dry/">drying rivers</a>, <a href="https://www.wearewater.org/en/the-inside-of-india-is-drying-out_288691">depleted aquifers</a>, <a href="https://nationalpost.com/news/world/the-water-table-is-dropping-all-over-the-world-new-nasa-study-reveals-global-drought">dropping water tables</a>… Climate change is unsurprisingly expected to <a href="https://www.nationalgeographic.com/magazine/article/water-crisis-looms-for-270-million-people-south-asia-perpetual-planet-feature/">make matters worse</a> still. An aspect that is less appreciated, though, is how water scarcity itself begets further overexploitation of water. An example may be seen in California, USA, where a prolonged drought <a href="https://alumni.berkeley.edu/california-magazine/just-in/2018-04-02/deep-water-deep-trouble-can-we-save-californias-drying">caused by over-damming of rivers</a> caused a huge <a href="https://www.nationalgeographic.com/news/2014/8/140819-groundwater-california-drought-aquifers-hidden-crisis/">increase in groundwater pumping</a>. Of course, groundwater depletion in turn causes a <a href="https://theprint.in/environment/man-made-disaster-under-your-feet-river-flows-dry-up/300451/">drop in river flows</a>, creating a vicious cycle. This is in addition to the <a href="https://link.springer.com/article/10.1007/s00254-004-1164-3">other adverse impacts</a> of groundwater depletion, such as reduction in vegetation, land subsidence and saltwater intrusion.</p>

<p style="text-align: justify">I think the above paragraph, though depressing, has made its point - sustainable use of our water resources is no trifling matter, but ranks among the biggest challenges facing us today. While water conservation measures will no doubt have a huge role to play, proper management on the supply side is no less important. Another less obvious point from the above is that management of water resources is a complex subject not just because of the associated socio-economic considerations, but also because the dynamics of water extraction, replenishment, and the interplay between the various water resources is very complex. Consider an aquifer. The aquifer may be replenished by rainfall, but depending on the geology, there may be a time lag before its water levels rise due to the rain. The aquifer may feed or be fed by surface water sources like rivers or lakes - perhaps both. It may be connected to other underground aquifers, with the flows through this system determined by the levels of the components. The aquifer system may also change with time, due to environmental changes or other factors. This means that proper water resource management requires both an understanding of the factors affecting the resources and a model that can employ these factors to tell us how to optimally utilise the resources. In other words, it’s a problem that calls for interpretable machine learning.</p>
<p><br /></p>
<h2 id="methodology--">Methodology  <a id="method"></a></h2>

<p style="text-align: justify">Based on the above, I decided to develop four interpretable machine learning models to forecast water levels for the nine different water bodies in the competition dataset. The guiding principles behind the model creation were simplicity, generalisability and robustness, while also attempting to predict as accurately as possible within these constraints.</p>

<p style="text-align: justify">The first thing to do when tackling any machine learning problem is to define the type of the problem. This problem was clearly an example of time series modelling - we were provided with features as a function of time, and asked to predict water volumes over a time interval (often multiple volumes, as we shall see next time). This brings us to the next question - what is the best approach to solve this problem?</p>

<h3 id="classical-methods--">Classical methods  <a id="classical"></a></h3>

<p style="text-align: justify">The traditional solution for time series forecasting has been a group of <a href="https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/">‘classical’ methods</a>. These range from simple methods like autoregression and moving average to progressively more complicated methods like Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) and Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX). These methods are time-tested, and despite recent hype about machine learning (ML), <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889"> often still outperform </a> ML methods on a range of data. Naturally then, I started out by trying to apply these methods to the competition dataset. After about a week, though, I decided that I needed to change tack, as the classical methods were simply not working well enough, or at all in some cases.</p>

<p style="text-align: justify">The first problem was trying to figure out which method to use. This choice needed to be made based on the number and types of the inputs and outputs. Consider the aquifers. The temperature and hydrometry data showed strong seasonal trends, implying that a SARIMA/SARIMAX type model should be considered. However, these models are univariate, meaning that they tackle a single input variable, and hence the interaction between the different variables is lost. A VAR/VARMAX type model solves this issue by considering multivariate inputs, but is most suitable for time series without trends or seasonal components. To an extent, this can be addressed by repeatedly differencing each time series till it is judged as stationary by a test like the <a href="https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/"> Augmented Dickey Fuller Test (ADF Test)</a> or <a href="https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/">KPSS test</a>. Another concern is the way VAR-type models make their predictions. A VAR-type model works as follows: you feed in the data for all the series from, say, 2006-2021, and the model then issues a forecast based on this for all the series for 2022. What is desirable, though, is a model to which the values for the non-target items for 2022 can be supplied, with the model predicting the target items. That is to say, we provide the rainfall, humidity, temperature, etc. for 2022, and get the expected water flow as output. This needs the use of a different class of methods, which I will discuss later.</p>

<p style="text-align: justify">Leaving the above shortcomings aside, I found that the classical methods were less suitable for this task simply because their inherent shortcomings, nicely summed up <a href="https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/">here</a>: to perform well, they require complete and ‘clean’ data, which was not the case here; they generally assume a linear relationship between the variables; they focus on a fixed temporal dependence, and on one-step forecasts. Rather than trying to patch over each of these shortcomings, I decided to see if there are alternative methods that are easier to use and more accurate, while also being sufficiently robust. This search brought me to two different ways of tackling the problem.</p>

<h3 id="neural-networks--">Neural networks  <a id="NN"></a></h3>

<p style="text-align: justify">The term ‘neural networks’ has become so popular in recent years as to become almost synonymous with ‘artificial intelligence’. Quite simply, a neural network (NN) is a model that attempts to replicate the structure of the brain via a system of mathematical functions called ‘neurons’. These ‘neurons’ can be trained to learn the relationship between input information and the desired outputs, and a properly trained NN can then be deployed on unseen data. If fed sufficient training data, then large NNs, with dozens of layers and thousands of neurons, can significantly outperform traditional ML algorithms or statistical methods.</p>

<p><br />
<img src="/agneev-blog/assets/img/img_4_2.jpg?raw=true" alt="Image_2" width="500&quot;, height=&quot;100" /></p>

<p>Advantage of neural networks (based on Andrew Ng’s talk available <a href="https://www.youtube.com/watch?v=F1ka6a13S9I&amp;feature=youtu.be">here</a>)</p>

<p style="text-align: justify">The downside of NNs is that they require a large amount of data to outperform, and are computationally expensive. Anyway, among the different types of NNs, Recurrent Neural Networks (RNNs) are better suited to time series analysis, as their neurons <a href="https://www.guru99.com/rnn-tutorial.html">contain a ‘memory state’ </a>that helps them ‘remember’ the previous inputs and thus model sequential data. A traditional RNN, however, cannot model long-term dependencies in the data owing to the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">‘vanishing gradient problem’</a> - simply put, it has a relatively short-term memory. <a href="https://databricks.com/blog/2019/09/10/doing-multivariate-time-series-forecasting-with-recurrent-neural-networks.html">Long-Short Term Memory networks (LSTMs)</a> are specialised RNNs developed to tackle this issue, and these are therefore well-suited to time series modelling where long-term trends form an important part of the modelling inputs, as was the case here. A detailed explanation of these systems is beyond the scope of this post (later maybe…), and I refer you instead to the many excellent explanations found online (such as <a href="https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/">here</a>, <a href="https://heartbeat.fritz.ai/a-beginners-guide-to-implementing-long-short-term-memory-networks-lstm-eb7a2ff09a27">here</a> or <a href="https://adventuresinmachinelearning.com/keras-lstm-tutorial/">here</a>). All that I will say here is that their robustness to noisy input data, ability to learn non-linear relationships in the data, and inherent support for multivariate, multi-step forecasts certainly make LSTMs worthy of further consideration for this competition dataset. At the same time, LSTMs need a large quantity of data to perform properly, which is not the case for several of the data series here. They also ideally require a large number of layers and training epochs, which is computationally unattractive. It is therefore pertinent to also look at another class of deep learning models.</p>

<h3 id="decision-tree-based-methods--">Decision tree-based methods  <a id="DT"></a></h3>

<p style="text-align: justify">A <a href="https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53">decision tree</a> is an ML algorithm that takes decisions based on the responses to a series of yes or no questions about the data. A simple decision tree for predicting the possibility of the survival of passengers on the Titanic is shown below as an example.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg" width="400" height="400" /></p>

<p>Example of decision tree based on Titanic passengers data, <a href="https://commons.wikimedia.org/wiki/File:Decision_Tree.jpg">taken from Wikimedia</a></p>

<p style="text-align: justify">While simple, a decision tree is not very useful by itself as a machine learning model, as it will <a href="https://towardsdatascience.com/decision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704">overfit to the training data</a>, and therefore be virtually useless for any new data. However, if numerous decision trees are trained on subsets of the data, the average of these predictions is much more accurate and robust than those of any single tree trained on the entire data. Two methods used for this are <a href="https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9">bagging and boosting</a>. Random forest (RF) is a bagging method that is <a href="https://builtin.com/data-science/random-forest-algorithm">simple, easy to use, and resistant to overfitting</a>, due to which it is one of the most widely used ML algorithms. On the flipside, making an accurate RF model requires lots of trees, and training these can be a slow process in the presence of large quantities of data. Boosting methods, if tuned properly, <a href="https://www.datasciencecentral.com/profiles/blogs/decision-tree-vs-random-forest-vs-boosted-trees-explained">give better results than RFs.</a> However, they are more sensitive to noisy data and require more careful tuning, as they are prone to overfitting.</p>

<p style="text-align: justify">With respect to the competition dataset, decision tree-based methods have some pros and cons. On the negative side, these methods have no awareness of time, as they assume that observations are independent and identically distributed. This limitation can be solved by feature engineering, particularly <a href="https://www.statworx.com/en/content-hub/blog/time-series-forecasting-with-random-forest/">‘time delay embedding’</a>, in other words providing the model with time lags of the modelled data. Another concern is that these methods cannot extrapolate, i.e., predict values outside the range of the trained data. This can also be dealt with using feature engineering, such as differencing and statistical transforms, but for this competition, the predicted values are in any case unlikely to be outside the training data band, meaning that this disadvantage is not a major handicap.</p>

<p style="text-align: justify">While the above explains why decision tree-based methods <em>can</em> be used for time series modelling, it does not explain why they are a <em>good</em> option. The first reason they are a good option is that, unlike classical methods or LSTM, one can simply provide these models with the non-target item values and obtain the target predictions. This is useful since, at the end of the day, the aim of the exercise is to see how variables like rainfall and temperature influence the output variables like depth of groundwater. Secondly, it has been noted that interpretability is very important in making an effective model, and in this regard, decision tree-based methods are far superior to NN methods. As noted <a href="https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb">here</a>, these methods help easily answer questions such as which inputs are the most important for the predictions, how they are related to the dependent variable and interact with each other, which particular features are most important for some particular observation, etc. Finally, the very fact that they work in a manner very different from NNs makes them useful in an ‘ensemble’.</p>

<h3 id="ensembling--">Ensembling  <a id="Ensemble"></a></h3>

<p style="text-align: justify">I mentioned earlier that the reason a combination of the predictions of decision trees trained on subsets of the data outperforms that of a decision tree trained on the entire data is because their uncorrelated errors tend to cancel each other out. This is the basis of the ensembling principle, wherein several models making uncorrelated errors on different parts of the data can be combined to give a model with a much lower error rate and higher generalisability. Decion tree-based ensembles are an ensemble of ‘weak learners’, as the decision trees are not individually very good at making predictions. The ensembling principle can however be extended to ‘strong learners’, such as a combination of RFs and NNs. By offsetting each other’s weaknesses, they give more robust and accurate predictions, explaining why they are a staple of <a href="https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/#:~:text=Why%20do%20stacked%20ensemble%20models%20win%20data%20science%20competitions%3F,-0&amp;text=Ensemble%20methods%20are%20commonly%20used,of%20multiple%20machine%20learning%20models.&amp;text=In%20an%20unweighted%20average%2C%20each,an%20ensemble%20model%20is%20built.">winning ML competition submissions.</a> The added complexity is an undoubted disadvantage, and so it needs to be checked that the ensemble predictions are indeed better enough than those of the individual models to justify this extra complexity.</p>

<h3 id="models-used--">Models used  <a id="models"></a></h3>

<p style="text-align: justify">I performed some preliminary runs on the competition datasets, which showed that random forests, LightGBM (<a href="https://lightgbm.readthedocs.io/en/latest/">Light Gradient Boosting Machine</a>, a decision tree boosting method) and long short-term memory neural networks were the best options, with each giving the best results for a certain portion of the data. I therefore decided to use these three methods, hereafter be referred to as RF, LGBM and LSTM. In addition, I also evaluated the results of an ensemble of these methods.</p>

<h3 id="metrics-used--">Metrics used  <a id="metrics"></a></h3>

<p style="text-align: justify">Before we can judge whether a model is ‘good’ or not, the metrics for determining the quality of the predictions need to first be fixed. Here, the competition organisers asked us to provide ‘a way of assessing the performance and accuracy of the solution’, in other words to provide a metric with justification. A range of error metrics are available, with <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean Absolute Error (MAE)</a>, <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">Root Mean Square Error (RMSE)</a>, <a href="https://en.wikipedia.org/wiki/Mean_percentage_error">Mean Percentage Error (MPE)</a>, <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">Mean Absolute Percentage Error (MAPE)</a>, <a href="https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error">Symmetric mean absolute percentage error (SMAPE)</a>, and <a href="https://en.wikipedia.org/wiki/WMAPE">Weighted Mean Absolute Percentage Error (WMAPE)</a> among the proposed options for regression problems. Since all of these have shortcomings, and trying them all out was impractical, the first two options, which are the most widely used of the lot, were the only two I seriously considered. Unless there is a specific reason to penalise larger errors much more than smaller errors (i.e. an error of 10 is more than twice as bad as an error of 5), MAE is a better metric due to its <a href="https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d">better interpretability</a> as compared to RMSE, which is actually a function of <a href="https://www.jstor.org/stable/24869236">three different characteristics of errors</a>, rather than of just the average error, making its interpretation ambiguous. However, regression algorithms such as sklearn’s RandomForestRegressor tend to be <a href="https://stackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to"> far slower</a> if MAE is used as a splitting criterion rather than RMSE. This was not a handicap, though - I was interested in using MAE as a <a href="https://datascience.stackexchange.com/questions/43542/mean-absolute-error-in-random-forest-regression"><em>model evaluation criterion</em></a>, so the split criterion used in the algorithm is irrelevant. In other words, the RMSE was used earlier in the modelling process to train the model, and then the performance of the model was later determined using MAE. I anyway provided the RMSE of all the models as well, but only as a reference, not for deciding model performance.</p>

<h3 id="model-interpretation--">Model interpretation  <a id="interpret"></a></h3>

<p style="text-align: justify">I have already stated that model interpretability is very important in ML, and that decision tree ensembles are superior in this regard to NNs. For RFs, for example, the decision on how to split a tree is based on an ‘impurity’ measure like Gini impurity, and the <a href="https://blog.datadive.net/selecting-good-features-part-iii-random-forests/">Mean Decrease Impurity (MDI)</a> can be itself be used as a gauge of feature importance. MDI, however, <a href="https://explained.ai/rf-importance/#7">tends to inflate the importance</a> of continuous or high cardinality variables compared to lower cardinality categorical variables, meaning that a variable than has a 1000 levels is likely to be ranked higher than one with 3 levels regardless of the actual contribution to the target prediction. Besides, as the MDI is calculated on the training set, it <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html"> can give high importances to variables that are not actually predictive of the target</a> but which could potentially be used to overfit. The solution is to use permutation importance - further details can be found <a href="https://explained.ai/rf-importance/#7">here</a> and <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html">here</a>.</p>

<p style="text-align: justify">While permutation importance can be and has been used here for the tree-based methods, the LSTM is a <a href="https://www.kdnuggets.com/2017/04/ai-machine-learning-black-boxes-transparency-accountability.html">‘black box model’</a>, and therefore requires a different interpretation mechanism. Several interpretation methods have been proposed in recent years, of which SHAP (SHapely Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) are perhaps the two most prominent (see <a href="https://www.kdnuggets.com/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html">here</a> and <a href="https://www.kdnuggets.com/2019/12/interpretability-part-3-lime-shap.html">here</a> for detailed explanations). While neither is perfect, the main advantage of LIME <a href="https://medium.com/analytics-vidhya/explain-your-model-with-lime-5a1a5867b423">appears to be speed</a>, while SHAP appears to be a better overall <a href="https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612">choice for model interpretation</a>, especially if one is trying to <a href="https://python-bloggers.com/2020/12/lime-vs-shap-which-is-better-for-explaining-machine-learning-models/">explain the entire model rather than a single prediction</a>. While I initially tried using both simultaneously, this became unwieldy and hard to work with when applied to all the models, and hence decided to stick with SHAP, which I applied to all the models, including the tree-based ones. The details of the SHAP mechanisms can be better understood when we look at the actual runs, and will therefore be provided there.</p>
<p><br /></p>
<h2 id="conclusion--">Conclusion  <a id="conc"></a></h2>

<p style="text-align: justify">OK, so this was a text-heavy post where I covered the methodology in plenty of depth. I think, though, that this detailed background was necessary to understand why I made the modelling choices I did. With that out of the way, next time we will see the actual modelling of the water bodies. So long!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-2/">&#8672;&nbsp;3. Evaluating handwritten Roman numerals datasets - 2</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-2/">5. Modelling water bodies - 2&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>]]></content><author><name></name></author><category term="[&quot;Data Science&quot;, &quot;Machine Learning&quot;, &quot;Python&quot;, &quot;Time series&quot;]" /><category term="Environment" /><category term="Water" /><category term="Kaggle" /><category term="Competition" /><summary type="html"><![CDATA[&#8672;&nbsp;3. Evaluating handwritten Roman numerals datasets - 2 5. Modelling water bodies - 2&nbsp;&#8674;]]></summary></entry><entry><title type="html">3. Evaluating handwritten Roman numerals datasets - 2</title><link href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-2/" rel="alternate" type="text/html" title="3. Evaluating handwritten Roman numerals datasets - 2" /><published>2022-02-21T12:00:00+01:00</published><updated>2022-02-21T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-2</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-2/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-1/">&#8672;&nbsp;2. Evaluating handwritten Roman numerals datasets - 1</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-1/">4. Modelling water bodies - 1&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
TensorFlow 2, Keras, Matplotlib, Competition
</p>
<p><br /></p>

<p style="text-align: justify">Welcome to part 2 of evaluating the Roman numerals datasets - you can read the background about the reason behind creating this dataset <a href="https://agneevmukherjee.github.io/agneev-blog/preparing-a-Roman-MNIST/">here</a>. In the <a href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1/">previous part</a>, we saw that a cut-off ResNet50 overfit on the three datasets we created and tested it on. In this post, let’s see how a full ResNet and a simple CNN perform on these datasets, before the winner is tested on a dataset combining samples from the three datasets. As a reminder, we will only look at running the models on CPU here - GPU and TPU runs will be looked at in future posts. As in the previous post, we will be dealing with <a href="https://www.tensorflow.org/tutorials/quickstart/beginner">TensorFlow 2</a> and <a href="https://keras.io/">Keras</a>-based models here.</p>

<h2 id="early-stopping--">Early stopping  <a id="stop"></a></h2>

<p style="text-align: justify">The second part of each notebook I linked to in the previous post (<a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-1.ipynb">this</a>, <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-2.ipynb">this</a> and <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-3.ipynb">this</a>) have the full ResNet50 operating. Before we get to looking at that, however, we might recollect one point from all the graphs seen in the previous post- the accuracy values reach a particular level pretty quickly, and then plateau. In the competition organisers’ code that I used, however, the model continues running until the 100 epochs asked for have finished. It would be nice if we could stop the training once no further progress is being made - this would surely be a timesaver! We can accomplish this using an <a href="https://keras.io/api/callbacks/early_stopping/">early stopping callback</a>, which is implemented in the code below. Alongside, we have another callback saving the best model as a checkpoint – this had been implemented in the organisers’ code as well.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">stopping</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s">"val_accuracy"</span><span class="p">,</span>
        <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"auto"</span><span class="p">,</span>
        <span class="n">baseline</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
        <span class="s">"best_model"</span><span class="p">,</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s">"val_accuracy"</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"max"</span><span class="p">,</span>
        <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span></code></pre></figure>

<p style="text-align: justify"><br />
The two important parameters to note in the early stopping callback are ‘min_delta’ and ‘patience’. Min_delta refers to the minimum change in the monitored quantity required for it to qualify as an improvement. For example, if we are monitoring validation accuracy, we can specify ‘min_delta = 0.01’, which would mean that the validation accuracy would have to improve by at least 0.01 for it to count. Here I have just kept it at the default value of 0 for simplicity. ‘Patience’ is the number of epochs of no improvement after which training will be stopped. The default for this is also 0, which means that the instant no improvement is observed, training will stop. In practice, this is usually sub-optimal, as the accuracy fluctuates, and hence one bad round does not imply that no further improvement is possible. We should therefore be ‘patient’ for a few epochs to see if the results improve before terminating the model. Here I have set the ‘patience’ parameter at 10, which is a very conservative value - I think it is safe to say that if no further improvement is obtained even after 10 epochs, then it is very unlikely that any further rounds will be helpful.</p>

<h2 id="full-resnet50--">Full ResNet50  <a id="full"></a></h2>

<p>OK, so then let’s run the full ResNet50, as per the code below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">start_2</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="n">base_model_2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">ResNet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">inputs_2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">resnet</span><span class="p">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">inputs_2</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">base_model_2</span><span class="p">(</span><span class="n">x_2</span><span class="p">)</span>
<span class="n">model_2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs_2</span><span class="p">,</span> <span class="n">x_2</span><span class="p">)</span>

<span class="n">model_2</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(),</span><span class="c1">#from_logits=True),
</span>        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">]</span>
    <span class="p">)</span>

<span class="n">loss_0</span><span class="p">,</span> <span class="n">acc_0</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"loss </span><span class="si">{</span><span class="n">loss_0</span><span class="si">}</span><span class="s">, acc </span><span class="si">{</span><span class="n">acc_0</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">history_2</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">train</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="n">valid</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stopping</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">model_2</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">"best_model"</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"final loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s">, final acc </span><span class="si">{</span><span class="n">acc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"test loss </span><span class="si">{</span><span class="n">test_loss</span><span class="si">}</span><span class="s">, test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">end_2</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Time taken = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">end_2</span> <span class="o">-</span> <span class="n">start_2</span><span class="p">)</span> <span class="o">+</span> <span class="s">' s'</span><span class="p">)</span></code></pre></figure>

<p><br />
Without further ado, let use see the results. First, for the raw images dataset:</p>

<p><img src="/agneev-blog/assets/img/img_3_1.png?raw=true" alt="Image_1" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_2.png?raw=true" alt="Image_2" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_3.png?raw=true" alt="Image_3" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify"><br />
For the raw images dataset, our high ‘Patience’ value means that all the 100 epochs have been run, and yet the test accuracy obtained is considerably lower than <a href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1/#raw">had been accomplished</a> by the cut-off ResNet50 (~81% instead of ~87%). The validation loss is much jumpier, and worst of all, though not unexpected, the run took almost 8 times longer. In short, there was no advantage to using the full ResNet at all on this dataset.</p>

<p><br />
For the EMNIST-based dataset, we have:</p>

<p><img src="/agneev-blog/assets/img/img_3_4.png?raw=true" alt="Image_4" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_5.png?raw=true" alt="Image_5" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_6.png?raw=true" alt="Image_6" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">Here we at least see a reduction in the total number of epochs, although the time taken is again several times higher than even the 100 epochs that the cut-off ResNet had taken. This was the easiest dataset to fit <a href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1/#syn">the last time</a>, and it’s no surprise that we again obtained almost a perfect accuracy.</p>
<p><br />
Finally we come to the Chars74K-based dataset:</p>

<p><img src="/agneev-blog/assets/img/img_3_7.png?raw=true" alt="Image_7" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_8.png?raw=true" alt="Image_8" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_9.png?raw=true" alt="Image_9" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">Arguably the worst results are obtained for this dataset, with test accuracy being half that obtained by the cut-off ResNet. The problem of overfitting seen last time has obviously been magnified by applying a bigger model.</p>

<p style="text-align: justify">We see that the results are different for the three datasets, but are rather discouraging overall. Now, we can undoubtedly improve the performance of the full ResNet - we have not applied any regularisation, BatchNormalization, Dropout, transfer learning weights, etc., etc. As a first pass, though, we can conclude that using the full ResNet50 on what are at the end of the day are fairly simple images is unlikely to lead to accuracy improvements that will be worth the added complexity and run times.</p>

<h2 id="simple-cnn--">Simple CNN  <a id="simple"></a></h2>

<p>What about a simpler network? Let us build a simple, no-frills CNN from scratch and see how it performs.</p>

<p>First the CNN itself is built as per the following code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model_3</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
<span class="p">])</span></code></pre></figure>

<p><br />
Now for the results. For the raw images dataset we have:</p>

<p><img src="/agneev-blog/assets/img/img_3_10.png?raw=true" alt="Image_10" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_11.png?raw=true" alt="Image_11" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_12.png?raw=true" alt="Image_12" width="400&quot;, height=&quot;300" /></p>

<p>Well, the good news is that it was quick - only 342 s! The bad news is that the test accuracy is only 66%…</p>

<p>Let us see what the EMIST-dataset results are like:</p>

<p><img src="/agneev-blog/assets/img/img_3_13.png?raw=true" alt="Image_13" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_14.png?raw=true" alt="Image_14" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_15.png?raw=true" alt="Image_15" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">If you have followed the previous runs on this dataset, you might have already guessed the results obtained - very quick run, and 98% test accuracy. Moving on to the Chars74K-dataset, we have:</p>

<p><img src="/agneev-blog/assets/img/img_3_16.png?raw=true" alt="Image_16" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_17.png?raw=true" alt="Image_17" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_18.png?raw=true" alt="Image_18" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">In one sense, the results obtained are better than that which had been obtained for the cut-off ResNet - the validation loss curve is much smoother than what we saw last time. Although a good thing in itself, as too much fluctuation in the loss values is a sign of instability in training, ultimately it does not in this case lead to a higher, or even comparable, test accuracy, which is what really counts.</p>

<p>So we can conclude that the cut-off ResNet50 used by the competition organisers is in fact the best choice for this problem.</p>

<p style="text-align: justify">But wait, I hear some of you say - what about run time? Isn’t the simple CNN much faster than the cut-off ResNet? Well yes, but remember that we did <em>not</em> use early stopping for the cut-off ResNet. We can <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/chars74k-cutoff-resnet50-early-stopping.ipynb">see what happens</a> if we apply early stopping and run the cut-off ResNet on the Chars74K-based dataset - we get both lower run time (154 s against 198 s) and higher test accuracy (~53% against ~47%) for the cut-off ResNet. So the organisers certainly knew what they were doing when they selected this particular network!</p>

<h2 id="combined-dataset--">Combined dataset  <a id="combo"></a></h2>

<p style="text-align: justify">All right, so we are now ready for the final part of this particular journey. I mentioned earlier that we will be testing the best performing network on a combined dataset. Now that we have selected the winning network, let us see how it does on the final dataset.</p>

<p style="text-align: justify">The combined dataset I used contains all the files in the raw images and Chars-74k based datasets, along with 100 capital and 100 small letters for each number from the EMNIST-based dataset. The reason for using a limited number of EMNIST-based images is simple - using all the images (~10,000) would have led to this dataset providing the overwhelming majority of images in the combined dataset. As it now stands, the combined dataset is relatively well balanced, with almost 6,500 images split in a 70:20:10 training:validation:test set ratio. You can find this dataset <a href="https://www.kaggle.com/agneev/combined-handwritten-roman-numerals-dataset">here</a>, while the evaluation code is <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/combined-ds-cutoff-resnet50-early-stopping.ipynb">here</a>.</p>

<p>As per the usual procedure, let us see the accuracy values and the training curves:</p>

<p><img src="/agneev-blog/assets/img/img_3_19.png?raw=true" alt="Image_19" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_20.png?raw=true" alt="Image_20" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_21.png?raw=true" alt="Image_21" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">Not bad! We got a test accuracy of ~85%, while the training curves are also reasonably smooth, although some evidence of overfitting is present.</p>

<h2 id="data-augmentation--">Data augmentation  <a id="aug"></a></h2>

<p style="text-align: justify">Now, as the final touch, let us see if we can improve the results a little further by using <a href="https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/">image augmentation</a>. Image augmentation is an easy way to generate subtly modified images on the fly, enhancing the number of images available for training. It also makes the model more robust against overfitting by teaching the model to recognise images despite changes such as distortions or orientation shifts. We will look at image augmentation in greater depth in the future, but for now let us just <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/augment-combined-ds-cutoff-resnet50-early-stopping.ipynb">dip our toes</a>.</p>

<p>The code used for the data augmentation is:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">train_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
        <span class="n">rotation_range</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">fill_mode</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>

<span class="n">val_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">()</span>

<span class="n">test_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">()</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="p">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">user_data</span><span class="p">,</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">valid</span> <span class="o">=</span> <span class="n">val_datagen</span><span class="p">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">valid_data</span><span class="p">,</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">test_datagen</span><span class="p">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">test_data</span><span class="p">,</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">You can read about the various parameters used in Keras’ ImageDataGenerator <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">here</a>. Although there are a large number of arguments that can be provided, I am only using rotation_range (the degrees by which the images can be rotated randomly), width_shift_range and height_shift_range (the fraction of the width and height by which to randomly move the images horizontally or vertically), shear_range (for random shear transformations) and zoom_range (for randomly zooming inside the images). Flipping, horizontally and/or vertically, is a commonly applied transformation, but a little thought will convince us that it is inappropriate here - a flipped ‘vii’ is no longer a 7…</p>

<p style="text-align: justify">It is important to remember that data augmentation, as a method to combat overfitting on the training set, is only applied to the training data, not the validation or test data. We therefore create two additional data generators for the validation and test sets without passing any arguments to these.</p>

<p style="text-align: justify">A brief point here - rescaling the images is generally recommended as an argument for all the data generators. By supplying ‘rescale = 1./255’, we ensure that the original 0-255 RGB pixel coefficients are reduced to a 0-1 range, which is more manageable for our models. In this case, however, rescaling led to noticeably worse results. This might be because the images are simple enough for the model to handle as-is, while rescaling led to information loss that impaired training. This is purely speculative, of course, and perhaps merits a more detailed look. For now, though, let us move forward without rescaling.</p>

<p style="text-align: justify">Once we have created the data generators, we need to feed them the appropriate data. As we are getting our data directly from the relevant directory, we can use Keras’ <a href="https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720">flow_from_directory</a> for this purpose. As can be seen from the code above, all this means here is providing the folder name, the target image size, and the batch size.</p>

<p>Once the above code is run, we get the output:</p>

<p><img src="/agneev-blog/assets/img/img_3_22.png?raw=true" alt="Image_22" width="300&quot;, height=&quot;200" /></p>

<p style="text-align: justify">Perfect. We see that the training, validation and test images have been passed in, and the number of classes correctly detected from the number of folders (i-x) in each set.</p>

<p style="text-align: justify">Before running the model, let us have a look at the images. The code (rescaling is done here only to enable Matplotlib to plot the images, it has no effect on the modelling):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batch</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
<span class="k">print</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
<span class="c1"># batch[0] are the images, batch[1] are the labels
# batch[0][0] is the first image,  batch[0][1] the next image
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">img</span><span class="o">=</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>   
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s">'auto'</span><span class="p">)</span></code></pre></figure>

<p>And the output (random, so will differ from run-to-run):</p>

<p><img src="/agneev-blog/assets/img/img_3_23.png?raw=true" alt="Image_23" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_24.png?raw=true" alt="Image_24" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_25.png?raw=true" alt="Image_25" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_26.png?raw=true" alt="Image_26" width="100&quot;, height=&quot;100" />        </p>

<p><img src="/agneev-blog/assets/img/img_3_27.png?raw=true" alt="Image_27" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_28.png?raw=true" alt="Image_28" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_29.png?raw=true" alt="Image_29" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_30.png?raw=true" alt="Image_30" width="100&quot;, height=&quot;100" /></p>

<p><br />
For comparison, the validation images are:</p>

<p><img src="/agneev-blog/assets/img/img_3_31.png?raw=true" alt="Image_31" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_32.png?raw=true" alt="Image_32" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_33.png?raw=true" alt="Image_33" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_34.png?raw=true" alt="Image_34" width="100&quot;, height=&quot;100" />        </p>

<p><img src="/agneev-blog/assets/img/img_3_35.png?raw=true" alt="Image_35" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_36.png?raw=true" alt="Image_36" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_37.png?raw=true" alt="Image_37" width="100&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_3_38.png?raw=true" alt="Image_38" width="100&quot;, height=&quot;100" /></p>

<p style="text-align: justify"><br />
We can see that the augmented training images are slightly harder to read, and have been rotated or moved up or down a little in some cases. We can certainly make the images more distorted, but ultimately, our aim is to make training a little harder for the neural network, not change the images so much that they bear little resemblance to the validation or test images.</p>

<p>All right, so let us train our model the same way, and see the results:</p>

<p><img src="/agneev-blog/assets/img/img_3_39.png?raw=true" alt="Image_39" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_3_40.png?raw=true" alt="Image_40" width="400&quot;, height=&quot;300" /></p>

<p><img src="/agneev-blog/assets/img/img_3_41.png?raw=true" alt="Image_41" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">We see that the validation curve fluctuates more than earlier, but the overfitting appears to have been more or less eliminated. The test accuracy is now around 87.5% - not bad!</p>

<p style="text-align: justify">Is it possible to improve the accuracy further? Probably - for starters, we could look at <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter optimisation</a> to search for the ImageDataGenerator argument values that work best. We must be careful though - too much fine-tuning is a recipe for overfitting on the test set!</p>

<p style="text-align: justify">The above statement may confuse some. How can we overfit on the test set, when the model never sees it? Ah, the <em>model</em> doesn’t see it, but we do! We tinker with the parameters, run the model, and then look at the test accuracy. We then change the parameters some more, rerun the model, and see how the test accuracy was affected. At the end of it all, we feel that we have obtained the highest accuracy possible on the test set, which may be true, but we have ended up defeating the purpose of the test set, which is to provide a means to objectively assess the effectiveness of our model on unseen data. In other words, the test set is now simply a glorified extension to the training set. If tuned in this way, our model is unlikely to perform optimally on <em>real</em> unseen data.</p>

<p style="text-align: justify">Let us therefore be satisfied with what we have achieved - an accuracy in the late 80s on a fairly diverse and representative dataset, with a reasonable non-GPU run time of ~26 minutes. On to a new adventure next time!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-1/">&#8672;&nbsp;2. Evaluating handwritten Roman numerals datasets - 1</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Water-modelling-part-1/">4. Modelling water bodies - 1&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;]" /><category term="TensorFlow 2" /><category term="Keras" /><category term="Matplotlib" /><category term="Competition" /><summary type="html"><![CDATA[&#8672;&nbsp;2. Evaluating handwritten Roman numerals datasets - 1 4. Modelling water bodies - 1&nbsp;&#8674;]]></summary></entry><entry><title type="html">2. Evaluating handwritten Roman numerals datasets - 1</title><link href="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1/" rel="alternate" type="text/html" title="2. Evaluating handwritten Roman numerals datasets - 1" /><published>2022-02-14T12:00:00+01:00</published><updated>2022-02-14T12:00:00+01:00</updated><id>https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1</id><content type="html" xml:base="https://agneevmukherjee.github.io/agneev-blog/Roman-numerals-dataset-evaluation-part-1/"><![CDATA[<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/preparing-a-Roman-MNIST/">&#8672;&nbsp;1. Preparing a handwritten Roman numerals dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-2/">3. Evaluating handwritten Roman numerals datasets - 2&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>

<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Categories:</span></p>
<div class="post-categories">
<p style="font-size:20px">
  
  
  <a href="/agneev-blog/categories/#Machine Learning">Machine Learning</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Computer Vision">Computer Vision</a>
  &nbsp;
  
  <a href="/agneev-blog/categories/#Python">Python</a>
  
  
  </p>
</div>
<p><br /></p>

<p><span style="font-family:Helvetica; font-size:1.5em;">Tags:</span><br /></p>
<p style="font-size:18px">
TensorFlow 2, Keras, Matplotlib, Competition
</p>
<p><br /></p>

<h2 id="evaluating-the-datasets-">Evaluating the datasets <a id="eval"></a></h2>

<p style="text-align: justify"><a href="https://agneevmukherjee.github.io/agneev-blog/preparing-a-Roman-MNIST/">Last time</a> we saw the methods I used to create my submissions for the <a href="https://https-deeplearning-ai.github.io/data-centric-comp/">Data-Centric AI competition</a>. You may have noticed that at the end I mentioned ‘we will look at evaluating some of my created datasets’, and wondered, why are there multiple datasets?</p>

<p style="text-align: justify">Well, like I explained, I used several data augmentation methods to increase the number of images. The images obtained by these methods could be combined in any ratio, obtaining different datasets. This begs the question - what is the ‘best’ ratio in which to combine the different types of images? There is no one answer to that question. The best training dataset for any task is one that will replicate the test or real-life data that the model trained on it will be used on. My submissions to the competition were judged based on the test set found <a href="https://worksheets.codalab.org/bundles/0x0b792c7dbeb14c82bf98978389a8eb85">here</a>. As you can see, the images in question were black ink on white background, and optimising for recognising these images will differ from recognising colourful images taken in real life settings, for example.</p>

<p style="text-align: justify">So an <em>ideal</em> handwritten Roman numerals dataset does not exist. What we will do here is look at three different datasets and how a cut-off ResNet50 performs on these.
In Part 2, we will apply both a full ResNet50 and a simple convolutional neural network (CNN) on the datasets and compare their performance with the cut-off ResNet50. Afterwards we will apply the best model on a combined dataset.</p>

<p style="text-align: justify">One small additional note: for simplicity, I will only be using CPU here to run all the models. We will look at running models on GPUs and TPUs in later posts.</p>

<h2 id="the-raw-images-dataset-">The raw images dataset <a id="raw"></a></h2>

<p style="text-align: justify">The first dataset (available <a href="https://www.kaggle.com/agneev/yaromnist-dataset">here</a>) we can look at is that containing all the raw images that I collected. As a reminder, these images look like this:</p>
<p><img src="/agneev-blog/assets/img/img_1_9.png?raw=true" alt="Image_1_9" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_10.png?raw=true" alt="Image_1_10" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_11.png?raw=true" alt="Image_1_11" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_12.png?raw=true" alt="Image_1_12" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_13.png?raw=true" alt="Image_1_13" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_14.png?raw=true" alt="Image_1_14" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_15.png?raw=true" alt="Image_1_15" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_16.png?raw=true" alt="Image_1_16" width="125&quot;, height=&quot;100" /></p>

<p style="text-align: justify"><br />
You can see the entire notebook <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-1.ipynb">here</a>, but let us see the important bits individually below. The code has been built in <a href="https://www.tensorflow.org/tutorials/quickstart/beginner">TensorFlow 2</a> and <a href="https://keras.io/">Keras</a>.</p>

<p style="text-align: justify">The first thing to note is that the notebooks dealing with the different datasets are actually the same, and if run interactively, the user has a choice of selecting which dataset to run on.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ds</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Which dataset do you want to run on? Enter 1 for </span><span class="se">\'</span><span class="s">Raw</span><span class="se">\'</span><span class="s">, 2 for </span><span class="se">\'</span><span class="s">EMNIST</span><span class="se">\'</span><span class="s"> </span><span class="se">\
</span><span class="s">or 3 for </span><span class="se">\'</span><span class="s">Chars74K'</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify">However, as the notebook is saved in a static fashion, the above code asking for user input cannot be run. I have therefore commented out the above code  and saved three separate versions of the notebook, with each version having a different value of ‘ds’ hardcoded for use in the code below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">while</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ds</span> <span class="o">==</span> <span class="s">'1'</span><span class="p">:</span>
        <span class="n">directory</span> <span class="o">=</span> <span class="s">"../input/yaromnist-dataset/YARoMNIST Dataset"</span>
        <span class="n">user_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/train"</span>
        <span class="n">valid_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/val"</span>
        <span class="n">test_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/test"</span> <span class="c1"># this can be the label book, or any other test set you create
</span>        <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">ds</span> <span class="o">==</span> <span class="s">'2'</span><span class="p">:</span>
        <span class="n">directory</span> <span class="o">=</span> <span class="s">"../input/emnistbased-handwritten-roman-numerals/500_each_EMNIST-based-Roman"</span>
        <span class="n">user_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/train"</span>
        <span class="n">valid_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/val"</span>
        <span class="n">test_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/test"</span> <span class="c1"># this can be the label book, or any other test set you create
</span>        <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">ds</span> <span class="o">==</span> <span class="s">'3'</span><span class="p">:</span>
        <span class="n">directory</span> <span class="o">=</span> <span class="s">"../input/basedonenglishhandwrittencharactersmodified/Based-Chars74K-image-dataset-Roman"</span>
        <span class="n">user_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/train"</span>
        <span class="n">valid_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/val"</span>
        <span class="n">test_data</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">+</span> <span class="s">"/test"</span> <span class="c1"># this can be the label book, or any other test set you create
</span>        <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span> <span class="n">ds</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Sorry, try again!'</span><span class="p">)</span></code></pre></figure>

<p style="text-align: justify"><br />
OK, with that out of the way, let us look at the evaluation code. First, I have used the code that was provided by the Data-Centric AI competition organisers, which was supposed to run unmodified on any dataset - after all, keeping the model fixed while changing the data was the whole point of the competition. In deference to this, I have left the code untouched, except for one addition - I have added lines for timing the model run, to compare this with the other models we will look at. The relevant lines have the comment ‘Not in original code’ written next to them:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">user_data</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="s">"inferred"</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
        <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"i"</span><span class="p">,</span> <span class="s">"ii"</span><span class="p">,</span> <span class="s">"iii"</span><span class="p">,</span> <span class="s">"iv"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">,</span> <span class="s">"vi"</span><span class="p">,</span> <span class="s">"vii"</span><span class="p">,</span> <span class="s">"viii"</span><span class="p">,</span> <span class="s">"ix"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">],</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">valid</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">valid_data</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="s">"inferred"</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
        <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"i"</span><span class="p">,</span> <span class="s">"ii"</span><span class="p">,</span> <span class="s">"iii"</span><span class="p">,</span> <span class="s">"iv"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">,</span> <span class="s">"vi"</span><span class="p">,</span> <span class="s">"vii"</span><span class="p">,</span> <span class="s">"viii"</span><span class="p">,</span> <span class="s">"ix"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">],</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">total_length</span> <span class="o">=</span> <span class="p">((</span><span class="n">train</span><span class="p">.</span><span class="n">cardinality</span><span class="p">()</span> <span class="o">+</span> <span class="n">valid</span><span class="p">.</span><span class="n">cardinality</span><span class="p">())</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">).</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">total_length</span> <span class="o">&gt;</span> <span class="mi">10_000</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Dataset size larger than 10,000. Got </span><span class="si">{</span><span class="n">total_length</span><span class="si">}</span><span class="s"> examples"</span><span class="p">)</span>
        <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span>

    <span class="n">test</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">test_data</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="s">"inferred"</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s">"categorical"</span><span class="p">,</span>
        <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"i"</span><span class="p">,</span> <span class="s">"ii"</span><span class="p">,</span> <span class="s">"iii"</span><span class="p">,</span> <span class="s">"iv"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">,</span> <span class="s">"vi"</span><span class="p">,</span> <span class="s">"vii"</span><span class="p">,</span> <span class="s">"viii"</span><span class="p">,</span> <span class="s">"ix"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">],</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span> <span class="c1"># Not in original code
</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">ResNet50</span><span class="p">(</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span>
        <span class="n">base_model</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">base_model</span><span class="p">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s">"conv2_block3_out"</span><span class="p">).</span><span class="n">output</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">resnet</span><span class="p">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="n">loss_0</span><span class="p">,</span> <span class="n">acc_0</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"loss </span><span class="si">{</span><span class="n">loss_0</span><span class="si">}</span><span class="s">, acc </span><span class="si">{</span><span class="n">acc_0</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
        <span class="s">"best_model"</span><span class="p">,</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s">"val_accuracy"</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"max"</span><span class="p">,</span>
        <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">train</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="n">valid</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">"best_model"</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"final loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s">, final acc </span><span class="si">{</span><span class="n">acc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"test loss </span><span class="si">{</span><span class="n">test_loss</span><span class="si">}</span><span class="s">, test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">end</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span> <span class="c1"># Not in original code
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Time taken = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">+</span> <span class="s">' s'</span><span class="p">)</span> <span class="c1"># Not in original code</span></code></pre></figure>

<p style="text-align: justify"><br />
If we go through the code above, we see that it builds a model based on a <a href="https://keras.io/api/applications/resnet/#resnet50-function">ResNet50</a>, but crucially, it only uses the ResNet up to the “conv2_block3_out” layer, and then uses Keras’ <a href="https://keras.io/guides/functional_api/">Functional API</a> to add a <a href="https://keras.io/api/layers/pooling_layers/global_average_pooling2d/">GlobalAveragePooling2D</a> and a <a href="https://keras.io/api/layers/core_layers/dense/">Dense</a> layer to complete the model. Details of these layers are beyond the scope of this blog post, but we will probably revisit them in later posts- for now, you can go through some of the great online resources like <a href="https://machinelearningknowledge.ai/keras-implementation-of-resnet-50-architecture-from-scratch/">this</a>, <a href="https://cv-tricks.com/keras/understand-implement-resnets/">this</a> or <a href="https://machinelearningmastery.com/how-to-implement-major-architecture-innovations-for-convolutional-neural-networks/">this</a> if you want a deeper understanding of ResNet architectures.</p>

<p>The above code also includes a model.summary(), which displays the model structure below:</p>

<p><img src="/agneev-blog/assets/img/img_2_1.png?raw=true" alt="Image_1" width="500&quot;, height=&quot;400" /></p>

<p style="text-align: justify">As you can see, the complexities of the ResNet50 architecture have been hidden away under the ‘model (Functional)’ heading. This is perhaps for the best, since the complete ResNet50 architecture looks like this:</p>

<figure class="highlight"><pre><code class="language-plain" data-lang="plain">Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           
__________________________________________________________________________________________________
predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   
==================================================================================================
Total params: 25,636,712
Trainable params: 25,583,592
Non-trainable params: 53,120
__________________________________________________________________________________________________</code></pre></figure>

<p><br />
If you scroll down the layers, around a quarter of the way down you will see:</p>

<p><img src="/agneev-blog/assets/img/img_2_2.png?raw=true" alt="Image_2" width="600&quot;, height=&quot;500" /></p>

<p>That’s the layer at which the ResNet50 is cut-off, and the GlobalAveragePooling2D is added after this.</p>

<p style="text-align: justify">OK, so on running the above code, the model iterates through the data, steadily providing us with the training loss, training accuracy, validation loss and validation accuracy for each of the 100 epochs we asked the model to run for. Towards the end of the run, the output looks something like this (the exact numbers will be different for each run):</p>

<p><img src="/agneev-blog/assets/img/img_2_3.png?raw=true" alt="Image_3" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">We see that because we specified a batch_size of 8, our dataset has been split into 291 batches. Every epoch takes around 12 seconds to run, and the overall run time for this model, seen at the bottom, is around 20 minutes. We also see that the final training accuracy is over 99%, that for the validation set is around 85%, and a similar accuracy is obtained for the test set as well.</p>

<p style="text-align: justify">To better understand the performance of the model, let us plot the training and validation accuracies over the entire model run:</p>

<p><img src="/agneev-blog/assets/img/img_2_4.png?raw=true" alt="Image_4" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">We can see that the training accuracy rises sharply for the first 15 epochs or so, to around the 95% mark, and then slowly but smoothly improves after that. The validation accuracy curve, on the other hand is very jagged, and plateaus at a lower level, though it does still rise until the end of the run. The loss curves show a similar picture, with the training loss falling smoothly almost to 0, while the validation loss curve is very irregular and averages around 0.7.</p>

<p style="text-align: justify">These curves are pretty much stereotypical <a href="https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/">overfitting curves</a>. This means that the model is memorising the training data, thereby obtaining almost perfect results on the training set, while the results on the validation and  test sets are considerably weaker. This is all the more concerning considering that the validation and test data in this case are very similar to the training data, which does not bode well for the generalisability of the model to real-world data. Methods for dealing with overfitting include getting more data and <a href="https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/">regularisation</a>, which we will look at in the future. For now, we can be fairly satisfied with the accuracy values obtained, and move on to the next dataset.</p>

<h2 id="the-synthesised-images-datasets--">The synthesised images datasets  <a id="syn"></a></h2>

<p style="text-align: justify">I described in <a href="https://agneevmukherjee.github.io/agneev-blog/preparing-a-Roman-MNIST/#syn">my previous post</a> how I created the synthetic data using the <a href="http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/">Chars74K</a> and <a href="https://arxiv.org/abs/1702.05373v1">EMNIST</a> datasets. Again as a reminder, the Chars74K-based figures looked like this:</p>

<p><img src="/agneev-blog/assets/img/img_1_55.png?raw=true" alt="Image_1_55" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_56.png?raw=true" alt="Image_1_56" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_57.png?raw=true" alt="Image_1_57" width="125&quot;, height=&quot;100" />        
<img src="/agneev-blog/assets/img/img_1_58.png?raw=true" alt="Image_1_58" width="125&quot;, height=&quot;100" /></p>

<p><br /><br />
while the EMNIST-based figures looked like:</p>

<p><img src="/agneev-blog/assets/img/img_1_82.png?raw=true" alt="Image_1_82" width="80&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_1_83.png?raw=true" alt="Image_1_83" width="80&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_1_84.png?raw=true" alt="Image_1_84" width="80&quot;, height=&quot;80" />        
<img src="/agneev-blog/assets/img/img_1_85.png?raw=true" alt="Image_1_85" width="80&quot;, height=&quot;80" /></p>

<p style="text-align: justify"><br />
We can evaluate these datasets (the EMNIST-based available <a href="https://www.kaggle.com/agneev/emnistbased-handwritten-roman-numerals">here</a> and the Chars74K-based available <a href="https://www.kaggle.com/agneev/basedonenglishhandwrittencharactersmodified">here</a>) in the same way as the raw images dataset, using the same basic notebook structure (<a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-2.ipynb">this</a> and <a href="https://github.com/AgneevMukherjee/agneev-blog/blob/main/roman-datasets-evaluation-3.ipynb">this</a>) hence let us directly look at the results.</p>

<p>For the EMNIST dataset, we have virtually perfect accuracy values, but the run time is almost 50 minutes:</p>

<p><img src="/agneev-blog/assets/img/img_2_5.png?raw=true" alt="Image_5" width="800&quot;, height=&quot;600" /></p>

<p style="text-align: justify">Neither the very high accuracy nor the long run time is actually surprising. The images in the EMNIST-based dataset are very similar to each other, and hence learning to recognise these is rather trivial, which explains the accuracy numbers. The long run time is because of the much greater number of images (almost 7,000 in the training set) necessitating 862 training batches.</p>

<p style="text-align: justify">If we look at the training and validation curves, we see they both reach very high accuracies within 5 epochs, with the main difference being that the validation curves are again much ‘jumpier’ than the training curves.</p>

<p><img src="/agneev-blog/assets/img/img_2_6.png?raw=true" alt="Image_6" width="400&quot;, height=&quot;300" /></p>

<p><br /><br />
The results for the Chars74K dataset are more interesting:</p>

<p><img src="/agneev-blog/assets/img/img_2_7.png?raw=true" alt="Image_7" width="800&quot;, height=&quot;600" /></p>

<p><img src="/agneev-blog/assets/img/img_2_8.png?raw=true" alt="Image_8" width="400&quot;, height=&quot;300" /></p>

<p style="text-align: justify">We see once again that the training accuracy jumps to almost 100% in about 15 epochs, but the validation accuracy stagnates at around the 60% mark, which is also the accuracy obtained for the test set. In other words, this dataset also exhibits a strong degree of overfitting. We also see that the validation loss is extremely volatile. These are both a result of the small size of the dataset (~800 training images), although at least the <a href="https://agneevmukherjee.github.io/agneev-blog/preparing-a-Roman-MNIST/#chars">transformations and insertion of noise</a> have made the validation set harder to recognise than the original black ink on white background images had been.</p>

<h2 id="conclusion-">Conclusion <a id="conc"></a></h2>

<p style="text-align: justify">We have now seen what the cut-off ResNet50 provided by the competition organisers achieved on our three datasets. As we know, the aim of the competition was to keep the model fixed and only change the data. However, we often have the feeling that bigger models do better than smaller ones, so how would a full ResNet50 perform on these datasets? Well, we can find out by simply running a full ResNet50! As this post is running too long, we will look at this next time, also examining how a simple CNN implemented from scratch will perform. Finally, we will pick the best of the three model options and run it on a ‘combined’ dataset, that will both represent a harder challenge and be more representative of real-life data. So long!</p>

<div class="post-nav">
  <p>
    
    <big><b>
    <a href="/agneev-blog/preparing-a-Roman-MNIST/">&#8672;&nbsp;1. Preparing a handwritten Roman numerals dataset</a></b></big>
    
  </p>
  <p style="text-align:right;">
    
    <big><b>
    <a href="/agneev-blog/Roman-numerals-dataset-evaluation-part-2/">3. Evaluating handwritten Roman numerals datasets - 2&nbsp;&#8674;</a>
    </b></big>
    
  </p>
</div>]]></content><author><name></name></author><category term="[&quot;Machine Learning&quot;, &quot;Computer Vision&quot;, &quot;Python&quot;]" /><category term="TensorFlow 2" /><category term="Keras" /><category term="Matplotlib" /><category term="Competition" /><summary type="html"><![CDATA[&#8672;&nbsp;1. Preparing a handwritten Roman numerals dataset 3. Evaluating handwritten Roman numerals datasets - 2&nbsp;&#8674;]]></summary></entry></feed>